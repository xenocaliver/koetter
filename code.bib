% This file was created with JabRef 2.9.2.
% Encoding: UTF8

@INPROCEEDINGS{5454095,
  author = {Abu-Surra, S. and Declercq, D. and Divsalar, D. and Ryan, W.E.},
  title = {Trapping set enumerators for specific LDPC codes},
  booktitle = {Information Theory and Applications Workshop (ITA), 2010},
  year = {2010},
  pages = {1-5},
  abstract = {In this paper, a method is presented for enumerating the trapping
	sets of a specific LDPC code given its Tanner graph. The technique
	involves augmenting the original Tanner graph with additional variable
	nodes, and then applying a weight-enumeration algorithm to the augmented
	Tanner graph. The proposed method is used to find trapping set enumerators
	for several LDPC codes in communication standards. The complexity
	of the proposed algorithm is discussed.},
  doi = {10.1109/ITA.2010.5454095},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\05454095.pdf:PDF},
  keywords = {graph theory;parity check codes;LDPC codes;augmented Tanner graph;communication
	standards;trapping set enumerator;weight-enumeration algorithm;Code
	standards;Communication standards;Iterative decoding;Laboratories;Parity
	check codes;Propulsion;Sections},
  timestamp = {2013.12.17}
}

@ARTICLE{raey,
  author = {Alon, Noga and Hoory, Shlomo and Linial, Nathan},
  title = {The Moore Bound for Irregular Graphs},
  journal = {Graphs and Combinatorics},
  year = {2002},
  volume = {18},
  pages = {53-57},
  number = {1},
  doi = {10.1007/s003730200002},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\A10.1007.003730200002.pdf:PDF},
  issn = {0911-0119},
  language = {English},
  publisher = {Springer-Verlag},
  timestamp = {2013.12.27},
  url = {http://dx.doi.org/10.1007/s003730200002}
}

@BOOK{alon:probabilistic,
  title = {The Probabilistic Method},
  publisher = {Wiley},
  year = {1992},
  author = {Alon, Noga and Spencer, Joel H.},
  address = {New York},
  added-at = {2008-02-26T11:58:58.000+0100},
  biburl = {http://www.bibsonomy.org/bibtex/2ec63f2bc7a58f88c650de0ea6194bfbb/schaul},
  citeulike-article-id = {2375612},
  description = {idsia},
  interhash = {3138bd2b231ea0afc1f1bed2197a7e0e},
  intrahash = {ec63f2bc7a58f88c650de0ea6194bfbb},
  keywords = {nn},
  timestamp = {2008-02-26T11:58:58.000+0100}
}

@INPROCEEDINGS{4641210,
  author = {Asao, Y. and Iwayama, M. and Tsuchida, K. and Nitayama, A. and Yoda,
	H. and Aikawa, H. and Ikegawa, S. and Kishi, T.},
  title = {A Statistical Model for Assessing the Fault Tolerance of Variable
	Switching Currents for a 1Gb Spin Transfer Torque Magnetoresistive
	Random Access Memory},
  booktitle = {Defect and Fault Tolerance of VLSI Systems, 2008. DFTVS '08. IEEE
	International Symposium on},
  year = {2008},
  pages = {507 -515},
  month = oct,
  abstract = {A comprehensive statistical model of the switching probability was
	proposed for a 1 Gb spin transfer torque magneto resistive random
	access memory (STT-MRAM). Since the switching current varies with
	every write cycle owing to the thermal instability, the read disturbance
	and the write error are critical issues in the STT-MRAM. In this
	paper, the operating condition of read and write was designed so
	as not to cause the read disturbance or the write error. The effect
	of an error correcting code (ECC) on the read disturbance was also
	calculated. Finally, it was demonstrated that the 1 Gb STT-MRAM could
	be realized with the optimal bit line voltages and the ECC.},
  doi = {10.1109/DFT.2008.18},
  file = {:PDF\\A_Statistical_Model_for_Assessing_the_Fault_Tolerance_of_Variable_Switching_Currents_for_a_1Gb_Spin_Transfer_Torque_Magnetoresistive_Random_Access_Memory.pdf:PDF},
  issn = {1550-5774},
  keywords = {error correcting code;fault tolerance;optimal bit line voltages;spin
	transfer torque magnetoresistive random access memory;statistical
	model;variable switching currents;error correction codes;fault tolerant
	computing;magnetoresistive devices;random-access storage;statistical
	analysis;}
}

@ARTICLE{4373388,
  author = {Barak, O. and Burshtein, D.},
  title = {Lower Bounds on the Error Rate of LDPC Code Ensembles},
  journal = {Information Theory, IEEE Transactions on},
  year = {2007},
  volume = {53},
  pages = {4225 -4236},
  number = {11},
  month = {nov. },
  abstract = {The ensemble of regular low-definition parity-check (LDPC) codes is
	considered. Using concentration results on the weight distribution,
	lower bounds on the error rate of a random code in the ensemble are
	derived. These bounds hold with some confidence level. Combining
	these results with known lower bounds on the error exponent, confidence
	intervals on the error exponent, under maximum-likelihood (ML) decoding,
	are obtained. Over a large range of channel parameter and transmission
	rate values, when the graph connectivity is sufficiently large, the
	upper bound of the interval approaches the lower bound, and the probability
	that the error exponent is within the interval can be arbitrarily
	close to one. In fact, in this case the true error exponent approaches
	the maximum between the random coding and the expurgated random coding
	exponents, with probability that approaches one.},
  doi = {10.1109/TIT.2007.907448},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\04373388.pdf:PDF},
  issn = {0018-9448},
  keywords = {LDPC code ensembles;error exponent;error rate;graph connectivity;low-definition
	parity-check codes;lower bounds;maximum-likelihood decoding;probability;random
	code;weight distribution;error statistics;maximum likelihood decoding;parity
	check codes;random codes;},
  timestamp = {2011.11.15}
}

@INPROCEEDINGS{1200390,
  author = {Belkoura, Z. and de B Naviner, L.A.},
  title = {Hardware implementation issues of a BMS decoding approach for AG
	based codes},
  booktitle = {Wireless Communications and Networking, 2003. WCNC 2003. 2003 IEEE},
  year = {2003},
  volume = {1},
  pages = {448-453 vol.1},
  month = {March},
  abstract = {Algebraic-geometry (AG) family of codes contains sequences with excellent
	asymptotic behaviour, but only few pieces of work have treated their
	hardware implementation. In this paper, we investigate an algorithm
	for decoding AG codes from the hardware feasibility point of view.
	We modify the original strategy in order to obtain a new structure
	more suitable for hardware implementation.},
  doi = {10.1109/WCNC.2003.1200390},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\01200390.pdf:PDF},
  issn = {1525-3511},
  keywords = {algebraic geometric codes;codecs;decoding;error correction codes;AG
	based codes;BMS decoding approach;Berlekamp-Massey-Sakata algorithm;algebraic
	geometry;code family;error correcting codes;hardware implementation
	issues;Algebra;Communication channels;Concatenated codes;Decoding;Error
	correction;Galois fields;Geometry;Hardware;Linear code;Publishing},
  timestamp = {2014.03.25}
}

@ARTICLE{1624642,
  author = {Ben-Haim, Y. and Litsyn, S.},
  title = {Upper bounds on the rate of LDPC codes as a function of minimum distance},
  journal = {Information Theory, IEEE Transactions on},
  year = {2006},
  volume = {52},
  pages = { 2092 - 2100},
  number = {5},
  month = {may},
  abstract = { New upper bounds on the rate of low-density parity-check (LDPC) codes
	as a function of the minimum distance of the code are derived. The
	bounds apply to regular LDPC codes, and sometimes also to right-regular
	LDPC codes. Their derivation is based on combinatorial arguments
	and linear programming. The new bounds improve upon the previous
	bounds due to Burshtein et al. It is proved that at least for high
	rates, regular LDPC codes with full-rank parity-check matrices have
	worse relative minimum distance than the one guaranteed by the Gilbert-Varshamov
	bound.},
  doi = {10.1109/TIT.2006.872972},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\01624642.pdf:PDF},
  issn = {0018-9448},
  keywords = { LDPC; combinatorial argument; linear programming; low-density parity-check
	code; minimum distance; combinatorial mathematics; linear programming;
	minimum principle; parity check codes;},
  timestamp = {2011.11.15}
}

@ARTICLE{springerlink:10.1007/BF02189233,
  author = {Berg, Bernd},
  title = {Multicanonical recursions},
  journal = {Journal of Statistical Physics},
  year = {1996},
  volume = {82},
  pages = {323-342},
  note = {10.1007/BF02189233},
  abstract = {The problem of calculating multicanonical parameters recursively is
	discussed. I describe in detail a computational implementation which
	has worked reasonably well in practice.},
  affiliation = {Florida State University Department of Physics 32306 Tallahassee FL
	32306 Tallahassee FL},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\10.1007_BF02189233.pdf:PDF},
  issn = {0022-4715},
  issue = {1},
  keyword = {Physics and Astronomy},
  publisher = {Springer Netherlands},
  timestamp = {2012.03.08},
  url = {http://dx.doi.org/10.1007/BF02189233}
}

@ARTICLE{Berg1998982,
  author = {Bernd A. Berg},
  title = {Algorithmic aspects of multicanonical simulations},
  journal = {Nuclear Physics B - Proceedings Supplements},
  year = {1998},
  volume = {63},
  pages = {982 - 984},
  number = {3},
  note = {Proceedings of the XVth International Symposium on Lattice Field
	Theory},
  abstract = {Monte Carlo (MC) simulations of many systems, can be considerably
	speeded up by using multicanonical or related methods. I shall focus
	on two aspects: (i) Opinions about the optimal choice of weights.
	(ii) Recursive weight factor estimates.},
  doi = {10.1016/S0920-5632(97)00962-6},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\Berg1998982.pdf:PDF},
  issn = {0920-5632},
  timestamp = {2012.03.23},
  url = {http://www.sciencedirect.com/science/article/pii/S0920563297009626}
}

@ARTICLE{PhysRevLett.69.2292,
  author = {Berg, Bernd A. and Celik, Tarik},
  title = {New approach to spin-glass simulations},
  journal = {Phys. Rev. Lett.},
  year = {1992},
  volume = {69},
  pages = {2292--2295},
  month = {Oct},
  doi = {10.1103/PhysRevLett.69.2292},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\PhysRevLett.69.2292.pdf:PDF},
  issue = {15},
  publisher = {American Physical Society},
  timestamp = {2012.03.09},
  url = {http://link.aps.org/doi/10.1103/PhysRevLett.69.2292}
}

@ARTICLE{PhysRevB.47.497,
  author = {Berg, B. A. and Hansmann, U. and Neuhaus, T.},
  title = {Simulation of an ensemble with varying magnetic field: A numerical
	determination of the order-order interface tension in the \textit{D}
	=2 Ising model},
  journal = {Phys. Rev. B},
  year = {1993},
  volume = {47},
  pages = {497--500},
  month = {Jan},
  doi = {10.1103/PhysRevB.47.497},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\simulation_ensemble_varying_magnetic_field_numerical_determination_order_order_interface_tension_d2_Ising_model_1993.pdf:PDF},
  issue = {1},
  publisher = {American Physical Society},
  timestamp = {2012.03.26},
  url = {http://link.aps.org/doi/10.1103/PhysRevB.47.497}
}

@ARTICLE{Berg1991249,
  author = {Bernd A. Berg and Thomas Neuhaus},
  title = {Multicanonical algorithms for first order phase transitions},
  journal = {Physics Letters B},
  year = {1991},
  volume = {267},
  pages = {249 - 253},
  number = {2},
  abstract = {Monte Carlo simulations are discussed for systems of volume V = Ld
	which undergo a first order phase transition in the finite volume
	limit. Conventional canonical, local Monte Carlo algorithms suffer
	from exponentially fast slowing down 竕・2 exp (cLd竏・). Here we present
	a class of multicanonical Monte Carlo algorithms which can reduce
	the slowing down to a quadratic power law 竕・2.},
  doi = {10.1016/0370-2693(91)91256-U},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\Berg1991249.pdf:PDF},
  issn = {0370-2693},
  url = {http://www.sciencedirect.com/science/article/pii/037026939191256U}
}

@ARTICLE{1055088,
  author = { Berlekamp, E.},
  title = {Goppa codes},
  journal = {Information Theory, IEEE Transactions on},
  year = {1973},
  volume = {19},
  pages = { 590 - 592},
  number = {5},
  month = sep,
  abstract = { Goppa described a new class of linear noncyclic error-correcting
	codes in [1] and [2]. This paper is a summary of Goppa's work, which
	is not yet available in English.^1We prove the four most important
	properties of Goppa codes. 1) There existq-ary Goppa codes with lengths
	and redundancies comparable to BCH codes. For the same redundancy,
	the Goppa code is typically one digit longer. 2) All Goppa codes
	have an algebraic decoding algorithm which will correct up to a certain
	number of errors, comparable to half the designed distance of BCH
	codes. 3) For binary Goppa codes, the algebraic decoding algorithm
	assumes a special form. 4) Unlike primitive BCH codes, which are
	known to have actual distances asymptotically equal to their designed
	distances, long Goppa codes have actual minimum distances much greater
	than twice the number of errors, which are guaranteed to be correctable
	by the algebraic decoding algorithm. In fact, long irreducible Goppa
	codes asymptotically meet the Gilbert bound.},
  doi = {10.1109/TIT.1973.1055088},
  file = {:PDF\\Goppa_Codes.pdf:PDF},
  issn = {0018-9448},
  keywords = { Goppa codes;}
}

@INPROCEEDINGS{Berlekamp_NONBINARY_BCH_DECODING,
  author = {E. R. Berlekamp},
  title = {NONBINARY BCH DECODING},
  booktitle = {1967 International Symposium on Information Theory, San Remeo, Italy},
  year = {1967},
  file = {:PDF\\Berlekamp_ISMS_1966_502.pdf:PDF}
}

@BOOK{biggs:1993,
  title = {Algebraic Graph Theory},
  publisher = {Cambridge University Press},
  year = {1993},
  author = {Biggs, N.},
  edition = {2nd},
  added-at = {2011-08-05T18:21:23.000+0200},
  biburl = {http://www.bibsonomy.org/bibtex/24f5faaac03d4009ed8b489ef250ac235/ulpsch},
  interhash = {4c87ee829665dbb45d227f7643cc632a},
  intrahash = {4f5faaac03d4009ed8b489ef250ac235},
  keywords = {d3 graphtheory},
  timestamp = {2011-08-05T18:21:23.000+0200}
}

@BOOK{Monte_Carlo_Simulation_in_Statistical_Physics,
  title = {Monte Carlo Simulation in Statistical Physics: An Introduction},
  publisher = {Springer Verlag},
  year = {2010},
  author = {Kurt Binder and Dieter W. Heermann},
  edition = {5th},
  month = {aug},
  abstract = {Monte Carlo Simulation in Statistical Physics deals with the computer
	simulation of many-body systems in condensed-matter physics and related
	fields of physics, chemistry and beyond, to traffic flows, stock
	market fluctuations, etc.). Using random numbers generated by a computer,
	probability distributions are calculated, allowing the estimation
	of the thermodynamic properties of various systems. This book describes
	the theoretical background to several variants of these Monte Carlo
	methods and gives a systematic presentation from which newcomers
	can learn to perform such simulations and to analyze their results.
	The fifth edition covers Classical as well as Quantum Monte Carlo
	methods. Furthermore a new chapter on the sampling of free-energy
	landscapes has been added. To help students in their work a special
	web server has been installed to host programs and discussion groups
	(http://wwwcp.tphys.uni-heidelberg.de). Prof. Binder was awarded
	the Berni J. Alder CECAM Award for Computational Physics 2001 as
	well as the Boltzmann Medal in 2007.},
  isbn = {978-3-642-03162-5},
  keywords = {Computational algorithms, Computer simulation and modeling of many-body
	systems, Sampling methods, Textbook on Monte Carlo Method, Textbook
	on statistical physics},
  timestamp = {2012.03.22},
  url = {http://www.springer.com/physics/theoretical%2C+mathematical+%26+computational+physics/book/978-3-642-03162-5}
}

@ARTICLE{720550,
  author = {Blake, I. and Heegard, C. and Hoholdt, T. and Wei, V.},
  title = {Algebraic-geometry codes},
  journal = {Information Theory, IEEE Transactions on},
  year = {1998},
  volume = {44},
  pages = {2596 -2618},
  number = {6},
  month = oct,
  abstract = {The theory of error-correcting codes derived from curves in an algebraic
	geometry was initiated by the work of Goppa as generalizations of
	Bose-Chaudhuri-Hocquenghem (BCH), Reed-Solomon (RS), and Goppa codes.
	The development of the theory has received intense consideration
	since that time and the purpose of the paper is to review this work.
	Elements of the theory of algebraic curves, at a level sufficient
	to understand the code constructions and decoding algorithms, are
	introduced. Code constructions from particular classes of curves,
	including the Klein quartic, elliptic, and hyperelliptic curves,
	and Hermitian curves, are presented. Decoding algorithms for these
	classes of codes, and others, are considered. The construction of
	classes of asymptotically good codes using modular curves is also
	discussed},
  doi = {10.1109/18.720550},
  file = {:NMP\\ユニット管理\\refereces\\PDF\\00720550.pdf:PDF},
  issn = {0018-9448},
  keywords = {BCH code;Bose-Chaudhuri-Hocquenghem codes;Goppa codes;Hermitian curves;Klein
	quartic curves;RS codes;Reed-Solomon codes;algebraic curves;algebraic-geometry
	codes;asymptotically good codes;code constructions;decoding algorithms;elliptic
	curves;error-correcting codes;hyperelliptic curves;modular curves;review;BCH
	codes;Goppa codes;Reed-Solomon codes;algebraic geometric codes;decoding;error
	correction codes;reviews;}
}

@ARTICLE{987093,
  author = {Blanksby, A.J. and Howland, C.J.},
  title = {A 690-mW 1-Gb/s 1024-b, rate-1/2 low-density parity-check code decoder},
  journal = {Solid-State Circuits, IEEE Journal of},
  year = {2002},
  volume = {37},
  pages = {404 -412},
  number = {3},
  month = {mar},
  abstract = {A 1024-b, rate-1/2, soft decision low-density parity-check (LDPC)
	code decoder has been implemented that matches the coding gain of
	equivalent turbo codes. The decoder features a parallel architecture
	that supports a maximum throughput of 1 Gb/s while performing 64
	decoder iterations. The parallel architecture enables rapid convergence
	in the decoding algorithm to be translated into low decoder switching
	activity resulting in a power dissipation of only 690 mW from a 1.5-V
	supply},
  doi = {10.1109/4.987093},
  file = {:PDF\\00987093.pdf:PDF},
  issn = {0018-9200},
  keywords = {1 Gbit/s;1.5 V;1024 bit;690 mW;CMOS digital integrated circuits;coding
	gain;decoder iterations;decoder switching activity;decoding algorithm;low-density
	parity-check code decoder;parallel architecture;power dissipation;soft
	decision;throughput;CMOS digital integrated circuits;iterative decoding;parallel
	architectures;},
  timestamp = {2011.04.22}
}

@INPROCEEDINGS{4035991,
  author = {Burshtein, D. and Barak, O.},
  title = {Upper Bounds on the Error Exponents of LDPC Code Ensembles},
  booktitle = {Information Theory, 2006 IEEE International Symposium on},
  year = {2006},
  pages = {401 -405},
  month = {july},
  abstract = {We consider the ensemble of regular LDPC codes and use recent concentration
	results on the distance spectrum to derive upper bounds on the error
	exponent of a randomly chosen code from the ensemble. These bounds
	hold with some confidence level that approaches one as the connectivity
	of the graph increases. We show that the bounds can be used to obtain
	the true error exponent over some range of channel parameter values,
	with the above confidence level},
  doi = {10.1109/ISIT.2006.261699},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\04035991.pdf:PDF},
  keywords = {LDPC code ensembles;channel parameter values;distance spectrum;error
	exponents;channel coding;parity check codes;random codes;},
  timestamp = {2011.11.17}
}

@INPROCEEDINGS{5592887,
  author = {Burshtein, D. and Goldenberg, I.},
  title = {Improved linear programming decoding and bounds on the minimum distance
	of LDPC codes},
  booktitle = {Information Theory Workshop (ITW), 2010 IEEE},
  year = {2010},
  pages = {1 -5},
  month = {302010-sept.3},
  abstract = {We propose a technique for improving LP decoding, based on the merging
	of check nodes. This technique can be applied to standard as well
	as generalized LDPC codes. Furthermore, we show how a recently-discovered
	linear-complexity LP decoder can be used to derive non-trivial lower
	bounds on the minimum distance of specific LDPC codes, with complexity
	that exhibits quadratic growth with respect to the block length.
	This bound can be refined using the check node merging technique.
	The lower bound on the minimum distance is shown to be an upper bound
	on the fractional distance of the code.},
  doi = {10.1109/CIG.2010.5592887},
  file = {:PDF\\Improved_Linear_Programming_Decoding_and_Bounds_on_the_Minimum_Distance_of_LDPC_Codes.pdf:PDF},
  keywords = {LDPC codes;check node merging technique;improved linear programming
	decoding;linear complexity LP decoder;low-density parity check codes;linear
	codes;linear programming;parity check codes;}
}

@INPROCEEDINGS{1216751,
  author = {Caire, G. and Shamai, S. and Verdu, S.},
  title = {A new data compression algorithm for sources with memory based on
	error correcting codes},
  booktitle = {Information Theory Workshop, 2003. Proceedings. 2003 IEEE},
  year = {2003},
  pages = { 291 - 295},
  month = {march-4 april},
  abstract = { A new fixed-length asymptotically optimal scheme for lossless compression
	of stationary ergodic tree sources with memory is proposed. Our scheme
	is based on the concatenation of the Burrows-Wheeler block sorting
	transform with the syndrome former of a linear error correcting code.
	Low-density parity-check (LDPC) codes together with belief propagation
	decoding lead to linear compression and decompression times, and
	to natural universal implementation of the algorithm.},
  doi = {10.1109/ITW.2003.1216751},
  file = {:PDF\\01216751.pdf:PDF},
  keywords = { Burrows-Wheeler block sorting transform; LDPC codes; belief propagation
	decoding; block error rate; concatenation; data compression; linear
	code; linear error correcting codes; low-density parity-check codes;
	memory; stationary ergodic tree sources; syndrome former; decoding;
	error correction codes; linear codes; parity check codes; source
	coding; transform coding; trees (mathematics);},
  timestamp = {2011.06.08}
}

@INPROCEEDINGS{1651870,
  author = {Cavus, E. and Daneshrad, B.},
  title = {A performance improvement and error floor avoidance technique for
	belief propagation decoding of LDPC codes},
  booktitle = {Personal, Indoor and Mobile Radio Communications, 2005. PIMRC 2005.
	IEEE 16th International Symposium on},
  year = {2005},
  volume = {4},
  pages = {2386-2390 Vol. 4},
  abstract = {In this work, we introduce a unique technique that improves the performance
	of the BP decoding in waterfall and error-floor regions by reversing
	the decoder failures. Based on the short cycles existing in the bipartite
	graph, an importance sampling simulation technique is used to identify
	the bit and check node combinations that are the dominant sources
	of error events, called trapping sets. Then, the identified trapping
	sets are used in the decoding process to avoid the pre-known failures
	and to converge to the transmitted codeword. With a minimal additional
	decoding complexity, the proposed technique is able to provide performance
	improvements for short-length LDPC codes and push or avoid error-floor
	behaviors of longer codes},
  doi = {10.1109/PIMRC.2005.1651870},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\01651870.pdf:PDF},
  keywords = {decoding;graph theory;importance sampling;parity check codes;belief
	propagation decoding;bipartite graph;bit-check node combination;error
	floor avoidance technique;importance sampling simulation technique;low
	density parity check;short-length LDPC code;trapping set;Belief propagation;Bipartite
	graph;Bridges;Discrete event simulation;Floors;Maximum likelihood
	decoding;Monte Carlo methods;Parity check codes;Performance loss;Signal
	processing algorithms},
  timestamp = {2013.12.20}
}

@ARTICLE{0605051,
  author = {Chad A. Cole, Stephen G. Wilson, Eric. K. Hall, Thomas R. Giallorenzi},
  title = {A General Method for Finding Low Error Rates of LDPC Codes},
  journal = {arxiv:cs/0605051},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\0605051.pdf:PDF},
  timestamp = {2014.04.07}
}

@INPROCEEDINGS{1031604,
  author = {Hsie-Chia Chang and Chien-Ching Lin and Chen-Yi Lee},
  title = {A low-power Reed-Solomon decoder for STM-16 optical communications},
  year = {2002},
  pages = { 351 - 354},
  doi = {10.1109/APASIC.2002.1031604},
  issn = { },
  journal = {ASIC, 2002. Proceedings. 2002 IEEE Asia-Pacific Conference on},
  keywords = { 0.25 micron; 2 kbit; 2.5 Gbit/s; CMOS 1P5M standard cells; Chien
	search circuit terminated mechanism; RS decoder; STM-16 optical communications;
	data rate; embedded memory; gate counts; key equation solver; low-power
	Reed-Solomon decoder; modified Berlekamp-Massey algorithm; power
	dissipation; received codewords; simulation; syndrome calculator;
	syndrome computation; CMOS integrated circuits; Reed-Solomon codes;
	circuit simulation; decoding; error correction codes; low-power electronics;
	optical communication equipment;}
}

@ARTICLE{new_serial_BM,
  author = {Hsic-Chia Chang and Bernard Shung},
  title = {New Serial Architecture for the Berlekamp-Massey Algorithm},
  journal = {IEEE Transactions on Communications},
  year = {1999},
  volume = {47},
  pages = {481-483},
  number = {4},
  month = {April},
  file = {:PDF\\New_Serial_Architecture_for_the_Berlekamp-Massey_Algorithm.pdf:PDF}
}

@INPROCEEDINGS{5872253,
  author = {Huan-Lin Chang and Hsuan-Chih Li and Liu, C.W. and Chen, F. and Tsai,
	M.-J.},
  title = {Physical mechanism of HfO2-based bipolar resistive random access
	memory},
  booktitle = {VLSI Technology, Systems and Applications (VLSI-TSA), 2011 International
	Symposium on},
  year = {2011},
  pages = {1 -2},
  month = {april},
  abstract = {The (anode) TiN/Ti/HfO2/TiN (cathode) resistive random access memory
	(RRAM) has shown yield ~100%. Its simple metal-insulator-metal (MIM)
	structure exhibits great potential for an embedded BEOL memory compatible
	with the high-k/metal gate CMOS process. There have been many theories
	of RRAM physical mechanism in the literature. This paper focuses
	on HfO2-based RRAM and describes a complete physical mechanism from
	forming, SET/RESET, current conduction, to explanations of various
	observed phenomena including multilevel, cell size scaling, resistance
	fluctuation, soft error, and non-abrupt RESRT process. Finally, suggestions
	for device optimization are given based on the physical model.},
  doi = {10.1109/VTSA.2011.5872253},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05872253.pdf:PDF},
  issn = {1930-8868},
  keywords = {HfO2-based bipolar resistive random access memory;RRAM;SET/RESET;TiN-Ti-HfO2-TiN;current
	conduction;device optimization;embedded BEOL memory;high-k/metal
	gate CMOS process;metal-insulator-metal structure;CMOS integrated
	circuits;MIM structures;bipolar memory circuits;hafnium compounds;random-access
	storage;titanium compounds;},
  timestamp = {2011.08.28}
}

@ARTICLE{5483292,
  author = {Yuan-Hao Chang and Tei-Wei Kuo},
  title = {A Management Strategy for the Reliability and Performance Improvement
	of MLC-Based Flash-Memory Storage Systems},
  journal = {Computers, IEEE Transactions on},
  year = {2011},
  volume = {60},
  pages = {305 -320},
  number = {3},
  month = march,
  abstract = {Cost has been a major driving force in the development of the flash-memory
	technology. Because of this, serious challenges are now faced for
	future products on reliability and performance requirements. In this
	work, we propose a management strategy to resolve the reliability
	and performance problems of many flash-memory products. A three-level
	address translation architecture with an adaptive block mapping mechanism
	is proposed to accelerate the address translation process with a
	limited amount of the RAM usage. Parallelism of operations over multiple
	chips is also explored with the considerations of the write constraints
	of advanced multilevel cell flash-memory chips. The capability of
	the proposed approach is analyzed with reliability considerations
	and evaluated by experiments over realistic workloads with respect
	to the reliability and performance improvement.},
  doi = {10.1109/TC.2010.126},
  file = {:PDF\\A_Management_Strategy_for_the_Reliability_and_Performance_Improvement_of_MLC-Based_Flash-Memory_Storage_Systems.pdf:PDF},
  issn = {0018-9340},
  keywords = {MLC based flash memory storage system;RAM;adaptive block mapping mechanism;advanced
	multilevel cell flash-memory chips;storage management strategy;storage
	system reliability;three-level address translation architecture;circuit
	reliability;flash memories;memory architecture;random-access storage;storage
	management chips;}
}

@INPROCEEDINGS{5513621,
  author = {Chao Chen and Baoming Bai and Xinmei Wang},
  title = {Two-dimensional generalized Reed-Solomon codes: A unified framework
	for quasi-cyclic LDPC codes constructed based on finite fields},
  booktitle = {Information Theory Proceedings (ISIT), 2010 IEEE International Symposium
	on},
  year = {2010},
  pages = {839 -843},
  month = june,
  abstract = {In this paper, we first propose a general framework for constructing
	quasi-cyclic low-density parity-check (QC-LDPC) codes based on a
	two-dimensional (2-D) maximum distance separable (MDS) code. Two
	classes of QC-LDPC codes are defined, whose parity-check matrices
	are transposes of each other. We then use a 2-D generalized Reed-Solomon
	(GRS) code to give a concrete construction. The decoding parity-check
	matrices have a large number of redundant parity-check equations
	while their Tanner graphs have a girth of at least 6. The minimum
	distances of the codes are very respectable as far as LDPC codes
	are concerned. We further show that many existing constructions of
	QC-LDPC codes based on finite fields in the literature can be unified
	under this construction. Experimental studies show that the constructed
	QC-LDPC codes perform well with the sum-product algorithm (SPA).},
  doi = {10.1109/ISIT.2010.5513621},
  file = {:PDF\\Two-Dimentional_Generalaized_Reed-Solomon_Codes_A_Unified_Framework_for_Quasi-Cyclic_LDPC_Codes_Constructed_Based_on_Finite_Field.pdf:PDF},
  keywords = {decoding;finite fields;generalized Reed-Solomon code;low-density parity-check;maximum
	distance separable code;parity-check matrices;quasi-cyclic LDPC codes;sum-product
	algorithm;Reed-Solomon codes;decoding;parity check codes;}
}

@ARTICLE{5467394,
  author = {Chen, E. and Apalkov, D. and Diao, Z. and Driskill-Smith, A. and
	Druist, D. and Lottis, D. and Nikitin, V. and Tang, X. and Watts,
	S. and Wang, S. and Wolf, S.A. and Ghosh, A.W. and Lu, J.W. and Poon,
	S.J. and Stan, M. and Butler, W.H. and Gupta, S. and Mewes, C. and
	Mewes, T. and Visscher, P.B.},
  title = {Advances and Future Prospects of Spin-Transfer Torque Random Access
	Memory},
  journal = {Magnetics, IEEE Transactions on},
  year = {2010},
  volume = {46},
  pages = {1873 -1878},
  number = {6},
  month = june,
  abstract = {Spin-transfer torque random access memory (STT-RAM) is a potentially
	revolutionary universal memory technology that combines the capacity
	and cost benefits of DRAM, the fast read and write performance of
	SRAM, the non-volatility of Flash, and essentially unlimited endurance.
	In order to realize a small cell size, high speed and achieve a fully
	functional STT-RAM chip, the MgO-barrier magnetic tunnel junctions
	(MTJ) used as the core storage and readout element must meet a set
	of performance requirements on switching current density, voltage,
	magneto-resistance ratio (MR), resistance-area product (RA), thermal
	stability factor (Â¿) , switching current distribution, read resistance
	distribution and reliability. In this paper, we report the progress
	of our work on device design, material improvement, wafer processing,
	integration with CMOS, and testing for a demonstration STT-RAM test
	chip, and projections based on modeling of the future characteristics
	of STT-RAM.},
  doi = {10.1109/TMAG.2010.2042041},
  file = {:PDF\\Advances_and_Future_Prospects_of_Spin-Transfer_Torque_Random_Access_Memory.pdf:PDF},
  issn = {0018-9464},
  keywords = {CMOS integration;device design;material improvement;spin-transfer
	torque random access memory;wafer processing;CMOS integrated circuits;random-access
	storage;}
}

@ARTICLE{1001666,
  author = {Chen, J. and Fossorier, M.P.C.},
  title = {Density evolution for two improved BP-Based decoding algorithms of
	LDPC codes},
  journal = {Communications Letters, IEEE},
  year = {2002},
  volume = {6},
  pages = {208 -210},
  number = {5},
  month = may,
  abstract = {In this letter, we analyze the performance of two improved belief
	propagation (BP) based decoding algorithms for LDPC codes, namely
	the normalized BP-based and the offset BP-based algorithms, by means
	of density evolution. The numerical calculations show that with one
	properly chosen parameter for each of these two improved BP-based
	algorithms, performances very close to that of the BP algorithm can
	be achieved. Simulation results for LDPC codes with code length moderately
	long validate the proposed optimization},
  doi = {10.1109/4234.1001666},
  file = {:PDF\\Density_Evolution_for_Two_Improved_BP-Based_Decoding_Algorithms_of_LDPC_Codes.pdf:PDF},
  issn = {1089-7798},
  keywords = {LDPC codes;belief propagation based decoding algorithms;density evolution;iterative
	decoding;low-density parity-check codes;normalized BP-based algorithms;numerical
	calculations;offset BP-based algorithms;AWGN channels;belief maintenance;block
	codes;iterative decoding;}
}

@ARTICLE{1327849,
  author = {Jia-Ping Chen and Chung-Chin Lu},
  title = {A serial-in-serial-out hardware architecture for systematic encoding
	of Hermitian codes via Gr ouml;bner bases},
  journal = {Communications, IEEE Transactions on},
  year = {2004},
  volume = {52},
  pages = { 1322 - 1332},
  number = {8},
  month = {aug},
  abstract = {When a nontrivial permutation of a Hermitian code is given, the code
	will have a module structure over a polynomial ring of one variable.
	By exploiting the theory of Gr ouml;bner bases for modules, a novel
	and elegant systematic encoding scheme for Hermitian codes is proposed
	by Heegard et al. (1995). The goal of this paper is to develop a
	serial-in-serial-out hardware architecture, similar to a classical
	cyclic encoder, for such a systematic encoding scheme. Moreover,
	we demonstrate that under a specific permutation, the upper bounds
	of the numbers of memory elements and constant multipliers in the
	proposed architecture are both proportional to O(n), where n is the
	length of the Hermitian code. To encode a codeword of length n, this
	architecture takes n clock cycles without any latency. Therefore,
	the hardware complexity of the proposed architecture is much less
	than that of the brute-force systematic encoding by matrix multiplication.},
  doi = {10.1109/TCOMM.2004.833020},
  file = {:PDF\\01327849.pdf:PDF},
  issn = {0090-6778},
  keywords = { Grobner bases; Hermitian codes; Reed-Solomon code; algebraic geometry
	code; brute-force systematic encoding; cyclic encoder; matrix multiplication;
	module structure; permutation; polynomial ring; serial-in serial-out
	hardware architecture; systematic encoding; Reed-Solomon codes; algebraic
	geometric codes; matrix multiplication;},
  timestamp = {2011.04.27}
}

@ARTICLE{1315899,
  author = {Lei Chen and Jun Xu and Djurdjevic, I. and Lin, S.},
  title = {Near-Shannon-limit quasi-cyclic low-density parity-check codes},
  journal = {Communications, IEEE Transactions on},
  year = {2004},
  volume = {52},
  pages = { 1038 - 1042},
  number = {7},
  month = {july},
  abstract = { This letter presents two classes of quasi-cyclic low-density parity-check
	codes that perform close to the Shannon limit.},
  doi = {10.1109/TCOMM.2004.831353},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\01315899.pdf:PDF},
  issn = {0090-6778},
  keywords = { algebraic method; array decomposition; iterative decoding; low-density
	parity-check codes; near-Shannon-limit codes; quasi-cyclic codes;
	random codes; algebraic geometric codes; cyclic codes; iterative
	decoding; parity check codes; random codes;},
  timestamp = {2011.12.06}
}

@ARTICLE{5954141,
  author = {Xiaoheng Chen and Jingyu Kang and Shu Lin and Akella, V.},
  title = {Hardware Implementation of a Backtracking-Based Reconfigurable Decoder
	for Lowering the Error Floor of Quasi-Cyclic LDPC Codes},
  journal = {Circuits and Systems I: Regular Papers, IEEE Transactions on},
  year = {2011},
  volume = {58},
  pages = {2931 -2943},
  number = {12},
  month = {dec. },
  abstract = {Emerging applications such as flash-based storage systems and 10 gigabit
	Ethernet require that there is no error floor even at bit error rates
	as low as 10-12 or so. It has been found that trapping sets are responsible
	for the error floors of many LDPC codes with AWGN channels. This
	paper presents a hardware based backtracking scheme to break the
	trapping sets at runtime for lowering the error floor of quasi-cyclic
	LDPC codes. Backtracking is implemented as a self-contained module
	that can be interfaced to any generic reconfigurable iterative decoder
	for QC-LDPC codes. The backtracking module and a reconfigurable decoder
	are implemented with a FPGA and an 180 nm standard cell library.
	The results indicate that the overhead of backtracking is modest
	- about 5% in terms of logic and 13% in terms of memory for the first
	level backtracking and 14% in terms of logic and 46% in terms of
	memory for a two-level backtracking scheme. Furthermore, it is shown
	that the increase in latency due to backtracking is modest in the
	average case and can be controlled by the system designer by choosing
	the appropriate values for the number of trials and the number of
	iterations of the backtracking module.},
  doi = {10.1109/TCSI.2011.2158712},
  file = {:\\\\homepd\\pd6u\\ユニット管理\\refereces\\PDF\\05954141.pdf:PDF},
  issn = {1549-8328},
  keywords = {AWGN channels;Ethernet;FPGA;backtracking module;bit error rates;error
	floor;flash-based storage systems;iteration number;quasicyclic LDPC
	codes;reconfigurable decoder;reconfigurable iterative decoder;self-contained
	module;size 180 nm;standard cell library;AWGN channels;backtracking;cyclic
	codes;error statistics;field programmable gate arrays;iterative decoding;local
	area networks;parity check codes;},
  timestamp = {2012.03.29}
}

@ARTICLE{5982105,
  author = {Xiaoheng Chen and Shu Lin and Akella, V.},
  title = {Efficient Configurable Decoder Architecture for Nonbinary Quasi-Cyclic
	LDPC Codes},
  journal = {Circuits and Systems I: Regular Papers, IEEE Transactions on},
  year = {2012},
  volume = {59},
  pages = {188-197},
  number = {1},
  abstract = {Nonbinary LDPC codes are effective in combating burst errors. This
	paper presents an efficient architecture for implementing nonbinary
	LDPC decoders. The Galois field power representation is used to organize
	the a priori, a posteriori, and extrinsic messages involved in decoding.
	The power representation in conjunction with the barrel shifter and
	multithreaded pipelining yields an efficient implementation. The
	proposed decoder is configurable, in the sense that a single decoder
	can be used to decode any code of a given field size. The decoder
	supports both regular and irregular nonbinary QC-LDPC codes. Using
	a practical metric of throughput per unit area, the proposed implementation
	outperforms the best implementations published in research literature
	to date.},
  doi = {10.1109/TCSI.2011.2161416},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\05982105.pdf:PDF},
  issn = {1549-8328},
  keywords = {Galois fields;binary codes;cyclic codes;decoding;error correction
	codes;parity check codes;Galois field power representation;barrel
	shifter;burst errors;configurable decoder architecture;decoding;irregular
	nonbinary QC-LDPC codes;multithreaded pipelining;nonbinary LDPC codes;nonbinary
	LDPC decoders;nonbinary quasi-cyclic LDPC codes;Complexity theory;Computer
	architecture;Decoding;Iterative decoding;Logic gates;Multiplexing;Configurable;VLSI
	design;decoder architecture;low-density parity-check (LDPC) codes;min-max;nonbinary},
  timestamp = {2013.05.28}
}

@INPROCEEDINGS{1258213,
  author = {Yanni Chen and Hocevar, D.},
  title = {A FPGA and ASIC implementation of rate 1/2, 8088-b irregular low
	density parity check decoder},
  booktitle = {Global Telecommunications Conference, 2003. GLOBECOM '03. IEEE},
  year = {2003},
  volume = {1},
  pages = { 113 - 117 Vol.1},
  month = {dec.},
  abstract = {This paper presents an implementation of irregular low density parity
	check decoder using both FPGA and ASIC. The considered low density
	parity check code has code rate 1/2, codeword length of 8088 bits
	and parallel factor of 24. The partly parallel structure, memory
	management, message alignment and addressing generation schemes needed
	to realize the underlying graph connectivity will be discussed. With
	the target FPGA device Xilinx XC2V8000 and maximum number of 25 iterations,
	the information decoding throughput could achieve up to 40 Mbps.
	By using the same configuration and Texas Instruments' GS-40 0.11
	mu;m ASIC process technology, decoder data rate of 188 Mbps could
	be achieved for this decoder.},
  doi = {10.1109/GLOCOM.2003.1258213},
  file = {:PDF\\01258213.pdf:PDF},
  keywords = { ASIC; FPGA; graph connectivity; information decoding; low density
	parity check decoder; memory management; message alignment; parallel
	structure; application specific integrated circuits; decoding; field
	programmable gate arrays; parallel architectures; parity check codes;
	storage management;},
  timestamp = {2011.04.22}
}

@ARTICLE{1291433,
  author = {Yanni Chen and Parhi, K.K.},
  title = {Small area parallel Chien search architectures for long BCH codes},
  journal = {Very Large Scale Integration (VLSI) Systems, IEEE Transactions on},
  year = {2004},
  volume = {12},
  pages = { 545 - 549},
  number = {5},
  month = {may.},
  doi = {10.1109/TVLSI.2004.826203},
  file = {:PDF\\Small_Area_Parallel_Chien_Search_Architecture_for_Long_BCH_Codes.pdf:PDF},
  issn = {1063-8210},
  keywords = { BCH codes; Bose-Chaud-huri-Hochquenghem codes; Chien search hardware
	complexity; finite field multiplier; group matching scheme; iterative
	matching algorithm; parallel Chien search architectures; BCH codes;
	iterative methods;}
}

@ARTICLE{1304967,
  author = {Chen, Yanni and Parhi, K.K.},
  title = {Overlapped message passing for quasi-cyclic low-density parity check
	codes},
  journal = {Circuits and Systems I: Regular Papers, IEEE Transactions on},
  year = {2004},
  volume = {51},
  pages = {1106-1113},
  number = {6},
  abstract = {In this paper, a systematic approach is proposed to develop a high
	throughput decoder for quasi-cyclic low-density parity check (LDPC)
	codes, whose parity check matrix is constructed by circularly shifted
	identity matrices. Based on the properties of quasi-cyclic LDPC codes,
	the two stages of belief propagation decoding algorithm, namely,
	check node update and variable node update, could be overlapped and
	thus the overall decoding latency is reduced. To avoid the memory
	access conflict, the maximum concurrency of the two stages is explored
	by a novel scheduling algorithm. Consequently, the decoding throughput
	could be increased by about twice assuming dual-port memory is available.},
  doi = {10.1109/TCSI.2004.826194},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\01304967.pdf:PDF},
  issn = {1549-8328},
  keywords = {decoding;matrix algebra;message passing;parity check codes;scheduling;belief
	propagation decoding;check node update;decoding latency;dual-port
	memory;low-density parity check codes;overlapped message passing;parity
	check matrix;quasi-cyclic LDPC codes;scheduling algorithm;variable
	node update;Belief propagation;Decoding;Hardware;Message passing;Parity
	check codes;Routing;Sparse matrices;Throughput;Turbo codes;Very large
	scale integration;High throughput;LDPC;MP;codes;low-density parity
	check;overlapped message passing;quasi-cyclic codes},
  timestamp = {2013.07.31}
}

@INPROCEEDINGS{1405303,
  author = {Chernyak, V. and Chertkov, M. and Stepanov, M. and Vasic, B.},
  title = {Instanton method of post-error-correction analytical evaluation},
  booktitle = {Information Theory Workshop, 2004. IEEE},
  year = {2004},
  pages = {220-224},
  month = {Oct},
  doi = {10.1109/ITW.2004.1405303},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\01405303.pdf:PDF},
  keywords = {block codes;error correction codes;error statistics;linear codes;tree
	codes;bit error rate;closed form expression;error code performance;error
	correction;instanton calculus;linear block codes;tree graphical model;Bit
	error rate;Block codes;Calculus;Graphical models;Iterative algorithms;Iterative
	decoding;Parity check codes;Performance analysis;Physics;Tree graphs},
  timestamp = {2014.03.05}
}

@ARTICLE{Chien,
  author = {R. Chien},
  title = {Cyclic decoding procedures for Bose-Chaudhuri-Hocquenghem codes},
  journal = {IEEE Transactions on Information Theory},
  year = {1964},
  volume = {10},
  pages = {357-363},
  file = {:PDF\\Chien_Cyclic_Decoding_Procedures_for_Bose-Chaundhuri-Hocquenghen_Codes.pdf:PDF},
  issue = {4}
}

@ARTICLE{5174515,
  author = {Chilappagari, S.K. and Chertkov, Michael and Stepanov, M.G. and Vasic,
	B.},
  title = {Instanton-based techniques for analysis and reduction of error floors
	of LDPC codes},
  journal = {Selected Areas in Communications, IEEE Journal on},
  year = {2009},
  volume = {27},
  pages = {855-865},
  number = {6},
  month = {August},
  abstract = {We describe a family of instanton-based optimization methods developed
	recently for the analysis of the error floors of low-density parity-check
	(LDPC) codes. Instantons are the most probable configurations of
	the channel noise which result in decoding failures. We show that
	the general idea and the respective optimization technique are applicable
	broadly to a variety of channels, discrete or continuous, and variety
	of sub-optimal decoders. Specifically, we consider: iterative belief
	propagation (BP) decoders, Gallager type decoders, and linear programming
	(LP) decoders performing over the additive white Gaussian noise channel
	(AWGNC) and the binary symmetric channel (BSC). The instanton analysis
	suggests that the underlying topological structures of the most probable
	instanton of the same code but different channels and decoders are
	related to each other. Armed with this understanding of the graphical
	structure of the instanton and its relation to the decoding failures,
	we suggest a method to construct codes whose Tanner graphs are free
	of these structures, and thus have less significant error floors.},
  doi = {10.1109/JSAC.2009.090804},
  issn = {0733-8716},
  keywords = {AWGN channels;coding errors;linear programming;optimisation;parity
	check codes;Gallager type decoder;LDPC codes;additive white Gaussian
	noise channel;binary symmetric channel;channel noise;error floor
	reduction;instanton based optimization;iterative belief propagation
	decoder;linear programming decoder;low density parity check code;Algorithm
	design and analysis;Belief propagation;Bit error rate;Error analysis;Iterative
	algorithms;Iterative decoding;Linear programming;Message passing;Optimization
	methods;Parity check codes;Low-density parity-check codes, Error
	Floor, Iterative Decoding, Linear Programming Decoding, Instantons,
	Pseudo-Codewords, Trapping Sets},
  timestamp = {2014.03.04}
}

@ARTICLE{5895058,
  author = {Chilappagari, S.K. and Chertkov, Michael and Vasic, B.},
  title = {An Efficient Instanton Search Algorithm for LP Decoding of LDPC Codes
	Over the BSC},
  journal = {Information Theory, IEEE Transactions on},
  year = {2011},
  volume = {57},
  pages = {4417-4426},
  number = {7},
  month = {July},
  abstract = {We consider linear programming (LP) decoding of a fixed low-density
	parity-check (LDPC) code over the binary symmetric channel (BSC).
	The LP decoder fails when it outputs a pseudo-codeword which is not
	equal to the transmitted codeword. We design an efficient algorithm
	termed the Instanton Search Algorithm (ISA) which generates an error
	vector called the BSC-instanton. We prove that: (a) the LP decoder
	fails for any error pattern with support that is a superset of the
	support of an instanton; (b) for any input, the ISA outputs an instanton
	in the number of steps upper-bounded by twice the number of errors
	in the input error vector. We then find the number of unique instantons
	of different sizes for a given LDPC code by running the ISA sufficient
	number of times.},
  doi = {10.1109/TIT.2011.2146670},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\05895058.pdf:PDF},
  issn = {0018-9448},
  keywords = {binary codes;channel coding;linear predictive coding;linear programming;parity
	check codes;search problems;BSC-instanton search algorithm;ISA output;LDPC
	codes;LP decoder;LP decoding;binary symmetric channel code;error
	pattern;fixed low density parity check code;input error vector;linear
	programming decoding;pseudo codeword;transmitted codeword;Algorithm
	design and analysis;Decoding;Iterative decoding;Signal to noise ratio;Support
	vector machines;Binary symmetric channel (BSC);error-floor;linear
	programming decoding;low-density parity-check (LDPC) codes;pseudo-codewords},
  timestamp = {2014.03.04}
}

@ARTICLE{5437420,
  author = {Chilappagari, S.K. and Nguyen, D.V. and Vasic, B. and Marcellin,
	M.W.},
  title = {On Trapping Sets and Guaranteed Error Correction Capability of LDPC
	Codes and GLDPC Codes},
  journal = {Information Theory, IEEE Transactions on},
  year = {2010},
  volume = {56},
  pages = {1600-1611},
  number = {4},
  abstract = {The relation between the girth and the guaranteed error correction
	capability of ?? -left-regular low-density parity-check (LDPC) codes
	when decoded using the bit flipping (serial and parallel) algorithms
	is investigated. A lower bound on the size of variable node sets
	which expand by a factor of at least 3 ??/4 is found based on the
	Moore bound. This bound, combined with the well known expander based
	arguments, leads to a lower bound on the guaranteed error correction
	capability. The decoding failures of the bit flipping algorithms
	are characterized using the notions of trapping sets and fixed sets.
	The relation between fixed sets and a class of graphs known as cage
	graphs is studied. Upper bounds on the guaranteed error correction
	capability are then established based on the order of cage graphs.
	The results are extended to left-regular and right-uniform generalized
	LDPC codes. It is shown that this class of generalized LDPC codes
	can correct a linear number of worst case errors (in the code length)
	under the parallel bit flipping algorithm when the underlying Tanner
	graph is a good expander. A lower bound on the size of variable node
	sets which have the required expansion is established.},
  doi = {10.1109/TIT.2010.2040962},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\05437420.pdf:PDF},
  issn = {0018-9448},
  keywords = {error correction codes;parallel algorithms;parity check codes;??-left-regular
	low-density parity-check codes;GLDPC Codes;LDPC Codes;Moore bound;Tanner
	graph;cage graphs;error correction capability;parallel bit flipping
	algorithm;variable node sets;Error correction;Error correction codes;Graph
	theory;Information theory;Iterative algorithms;Iterative decoding;Message
	passing;Parity check codes;Upper bound;Bit flipping algorithms;error
	correction capability;fixed sets;generalized low-density parity-check
	(LDPC) codes;low-density parity-check (LDPC) codes;trapping sets},
  timestamp = {2013.11.08}
}

@INPROCEEDINGS{4024284,
  author = {Chilappagari, S.K. and Sankaranarayanan, S. and Vasic, B.},
  title = {Error Floors of LDPC Codes on the Binary Symmetric Channel},
  booktitle = {Communications, 2006. ICC '06. IEEE International Conference on},
  year = {2006},
  volume = {3},
  pages = {1089 -1094},
  month = {june },
  abstract = {In this paper, we propose a semi-analytical method to compute error
	floors of LDPC codes on the binary symmetric channel decoded iteratively
	using the Gallager B algorithm. The error events of the decoder are
	characterized using combinatorial objects called trapping sets, originally
	defined by Richardson. In general, trapping sets are characteristic
	of the graphical representation of a code. We study the structure
	of trapping sets and explore their relation to graph parameters such
	as girth and vertex degrees. Using the proposed method, we compute
	error floors of regular structured and random LDPC codes with column
	weight three.},
  doi = {10.1109/ICC.2006.254892},
  file = {:PDF\\04024284.pdf:PDF},
  issn = {8164-9547},
  keywords = {AWGN;Additive white noise;Bipartite graph;Computer errors;Degradation;Error
	analysis;Iterative algorithms;Iterative decoding;Parity check codes;Signal
	to noise ratio;},
  timestamp = {2013.01.25}
}

@ARTICLE{2439,
  author = {Chu, K.M. and Pulfrey, D.L.},
  title = {An analysis of the DC and small-signal AC performance of the tunnel
	emitter transistor},
  journal = {Electron Devices, IEEE Transactions on},
  year = {1988},
  volume = {35},
  pages = {188 -194},
  number = {2},
  month = {feb},
  abstract = {A model to describe the I-V characteristics of the tunnel emitter
	transistor (the TETRAN) is developed. It is based on a general model
	for tunneling in metal thin-insulator semiconductor structures. The
	model is used to compute typical magnitudes for the parameters appearing
	in the small-signal hybrid- pi; equivalent circuit of this device.
	From these it is predicted that the cutoff frequency for realistic
	TETRANs based on Al/SiO2/n-Si structures is about 1 GHz. This is
	considerably less than the values recently predicted for a related
	device, the BICFET, which is similar to the TETRAN},
  doi = {10.1109/16.2439},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\00002439.pdf:PDF},
  issn = {0018-9383},
  keywords = {Al-SiO2-Si;DC performance;I-V characteristics;MIS structure;TETRAN;cutoff
	frequency;model;small-signal AC performance;small-signal hybrid-
	pi; equivalent circuit;tunnel emitter transistor;equivalent circuits;insulated
	gate field effect transistors;semiconductor device models;tunnelling;},
  timestamp = {2011.09.15}
}

@PHDTHESIS{10-1-1-133-2426,
  author = {Sae-Young Chung},
  title = {On the Construction of Some Capacity-Approaching Coding Schemes},
  school = {Department of Electrical Engineering and Computer Science, Massachusetts
	Institute of Technology},
  year = {2000},
  month = {September},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\10-1-1-133-2426.pdf:PDF},
  timestamp = {2011.12.02}
}

@ARTICLE{905935,
  author = {Sae-Young Chung and Forney, G.D., Jr. and Richardson, T.J. and Urbanke,
	R.},
  title = {On the design of low-density parity-check codes within 0.0045 dB
	of the Shannon limit},
  journal = {Communications Letters, IEEE},
  year = {2001},
  volume = {5},
  pages = {58 -60},
  number = {2},
  month = {feb },
  abstract = {We develop improved algorithms to construct good low-density parity-check
	codes that approach the Shannon limit very closely. For rate 1/2,
	the best code found has a threshold within 0.0045 dB of the Shannon
	limit of the binary-input additive white Gaussian noise channel.
	Simulation results with a somewhat simpler code show that we can
	achieve within 0.04 dB of the Shannon limit at a bit error rate of
	10/sup -6/ using a block length of 10/sup 7/.},
  doi = {10.1109/4234.905935},
  file = {:PDF\\00905935.pdf:PDF},
  issn = {1089-7798},
  keywords = {AWGN;Additive white noise;Bit error rate;Decoding;H infinity control;Laboratories;Parity
	check codes;Quantization;Sum product algorithm;Turbo codes;AWGN channels;block
	codes;decoding;error statistics;linear programming;AWGN channel;Shannon
	limit;binary-input additive white Gaussian noise channel;bit error
	rate;block length;code construction;iterative linear programming;low-density
	parity-check codes;rate 1/2 code;simulation results;sum-product decoding;},
  timestamp = {2013.01.17}
}

@ARTICLE{910580,
  author = {Sae-Young Chung and Richardson, T.J. and Urbanke, R.L.},
  title = {Analysis of sum-product decoding of low-density parity-check codes
	using a Gaussian approximation},
  journal = {Information Theory, IEEE Transactions on},
  year = {2001},
  volume = {47},
  pages = {657 -670},
  number = {2},
  month = {feb},
  abstract = {Density evolution is an algorithm for computing the capacity of low-density
	parity-check (LDPC) codes under message-passing decoding. For memoryless
	binary-input continuous-output additive white Gaussian noise (AWGN)
	channels and sum-product decoders, we use a Gaussian approximation
	for message densities under density evolution to simplify the analysis
	of the decoding algorithm. We convert the infinite-dimensional problem
	of iteratively calculating message densities, which is needed to
	find the exact threshold, to a one-dimensional problem of updating
	the means of the Gaussian densities. This simplification not only
	allows us to calculate the threshold quickly and to understand the
	behavior of the decoder better, but also makes it easier to design
	good irregular LDPC codes for AWGN channels. For various regular
	LDPC codes we have examined, thresholds can be estimated within 0.1
	dB of the exact value. For rates between 0.5 and 0.9, codes designed
	using the Gaussian approximation perform within 0.02 dB of the best
	performing codes found so far by using density evolution when the
	maximum variable degree is 10. We show that by using the Gaussian
	approximation, we can visualize the sum-product decoding algorithm.
	We also show that the optimization of degree distributions can be
	understood and done graphically using the visualization},
  doi = {10.1109/18.910580},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\00910580.pdf:PDF},
  issn = {0018-9448},
  keywords = {AWGN channels;Gaussian approximation;Gaussian densities;additive white
	Gaussian noise channels;code rates;degree distributions;density evolution
	algorithm;infinite-dimensional problem;irregular LDPC codes;low-density
	parity-check codes;maximum variable degree;memoryless binary-input
	continuous-output channels;message-passing decoding;optimization;regular
	LDPC codes;sum-product decoders;sum-product decoding algorithm;thresholds;visualization;AWGN
	channels;approximation theory;decoding;error detection codes;evolutionary
	computation;memoryless systems;optimisation;},
  timestamp = {2011.12.02}
}

@BOOK{Cox:2007:IVA:1204670,
  title = {Ideals, Varieties, and Algorithms: An Introduction to Computational
	Algebraic Geometry and Commutative Algebra, 3/e (Undergraduate Texts
	in Mathematics)},
  publisher = {Springer-Verlag New York, Inc.},
  year = {2007},
  author = {Cox, David A. and Little, John and O'Shea, Donal},
  address = {Secaucus, NJ, USA},
  isbn = {0387356509},
  timestamp = {2011.09.24}
}

@BOOK{GBBIB448,
  title = {Using Algebraic Geometry},
  publisher = {Springer},
  year = {2005},
  author = {D. Cox and J. Little and D. O'Shea},
  edition = {2nd},
  abstract = {This book is an introduction to Gr\"{o}bner bases and resultants,
	which are two of the main tools used in computational algebraic geometry
	and commutative algebra. It also discusses local methods and syzygies,
	and gives applications to integer programming, polynomial splines
	and algebraic coding theory.},
  language = {English},
  length = {558},
  timestamp = {2012.06.07},
  url = {http://www.cs.amherst.edu/~dac/uag.html}
}

@ARTICLE{AJP2004vol72no10pp1294,
  author = {D. P. Landau, Shan-Ho Tsai and M. Exler},
  title = {A new approach to Monte Carlo simulations in statistical physics:
	Wang-Landau sampling},
  journal = {American Journal of Physics},
  year = {2004},
  volume = {72},
  pages = {1294--1302},
  number = {10},
  month = {oct},
  abstract = {We describe a Monte Carlo algorithm for doing simulations in classical
	statistical physics in a different way. Instead of sampling the probability
	distribution at a fixed temperature, a random walk is performed in
	energy space to extract an estimate for the density of states. The
	probability can be computed at any temperature by weighting the density
	of states by the appropriate Boltzmann factor. Thermodynamic properties
	can be determined from suitable derivatives of the partition function
	and, unlike “standard” methods, the free energy and entropy can also
	be computed directly. To demonstrate the simplicity and power of
	the algorithm, we apply it to models exhibiting first-order or second-order
	phase transitions.},
  doi = {10.1119/1.1707017},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\AJP2004vol72no10pp1294.pdf:PDF},
  timestamp = {2012.03.27},
  url = {http://ajp.aapt.org/resource/1/ajpias/v72/i10/p1294_s1?isAuthorized=no}
}

@INPROCEEDINGS{1465805,
  author = {Darabiha, A. and Carusone, A.C. and Kschischang, F.R.},
  title = {Multi-Gbit/sec low density parity check decoders with reduced interconnect
	complexity},
  booktitle = {Circuits and Systems, 2005. ISCAS 2005. IEEE International Symposium
	on},
  year = {2005},
  pages = { 5194 - 5197 Vol. 5},
  month = {may},
  abstract = {A 3.2-Gbit/sec 2048-bit parallel LDPC decoder is implemented in a
	0.18 mu;m CMOS process. We employ two new techniques to address the
	interconnect problem: A broadcasting technique reduces the total
	amount of check-to-variable interconnect wires by more than 40%.
	A hierarchical placement algorithm places the variable and check
	nodes in the top-level hierarchy of the design and reduces the maximum
	wire length by up to 50%.},
  doi = {10.1109/ISCAS.2005.1465805},
  file = {:PDF\\01465805.pdf:PDF},
  keywords = { 0.18 micron; 2048 bit; 3.2 Gbit/s; CMOS; broadcasting technique;
	check nodes placement; check-to-variable interconnect wire reduction;
	hierarchical placement algorithm; low density parity check decoders;
	maximum wire length reduction; parallel LDPC decoder; random parity-check
	matrix; reduced interconnect complexity decoders; variable nodes
	placement; CMOS logic circuits; decoding; integrated circuit interconnections;
	integrated circuit layout; parallel processing; parity check codes;},
  timestamp = {2011.04.22}
}

@ARTICLE{681360,
  author = {Davey, M.C. and MacKay, D.},
  title = {Low-density parity check codes over GF(q)},
  journal = {Communications Letters, IEEE},
  year = {1998},
  volume = {2},
  pages = {165 -167},
  number = {6},
  month = {jun.},
  doi = {10.1109/4234.681360},
  file = {:PDF\\Low-Density_Parity_Check_Codes_over_GF(q).pdf:PDF},
  issn = {1089-7798},
  keywords = {GF(q);binary Gaussian channels;binary codes;binary symmetric channels;bit
	error probability;error-correction;low-density parity check codes;near-Shannon
	limit performance;probabilistic decoding algorithm;rate 1/4 code;Galois
	fields;Gaussian channels;binary sequences;channel coding;decoding;error
	correction codes;probability;sparse matrices;}
}

@ARTICLE{1480701,
  author = {Deal, B.E.},
  title = {Standardized terminology for oxide charges associated with thermally
	oxidized silicon},
  journal = {Electron Devices, IEEE Transactions on},
  year = {1980},
  volume = {27},
  pages = { 606 - 608},
  number = {3},
  month = mar,
  abstract = { Standarized terminology for oxide charges associated with the thermally
	oxidized silicon system is presented. This terminology is recommended
	by a committee established by the Electronics Division of the Electrochemical
	Society and the IEEE Semiconductor Interface Specialists Conference.
	All engineers and scientists concerned with oxide charges in silicon
	semiconductor applications are urged to adopt this terminology.},
  doi = {10.1109/T-ED.1980.19908},
  file = {:PDF\\Standardized_Terminology_for_Oxide_Charges_Associated_with_Thermally_Oxidized_Silicon.pdf:PDF},
  issn = {0018-9383}
}

@ARTICLE{5145891,
  author = {DiMaria, D. J. and Cartier, E. and Arnold, D.},
  title = {Impact ionization, trap creation, degradation, and breakdown in silicon
	dioxide films on silicon},
  journal = {Journal of Applied Physics},
  year = {1993},
  volume = {73},
  pages = {3367 -3384},
  number = {7},
  month = apr,
  abstract = {Degradation of silicon dioxide films is shown to occur primarily near
	interfaces with contacting metals or semiconductors. This deterioration
	is shown to be accountable through two mechanisms triggered by electron
	heating in the oxide conduction band. These mechanisms are trap creation
	and band #x2010;gap ionization by carriers with energies exceeding
	2 and 9 eV with respect to the bottom of the oxide conduction band,
	respectively. The relationship of band #x2010;gap ionization to defect
	production and subsequent degradation is emphasized. The dependence
	of the generated sites on electric field, oxide thickness, temperature,
	voltage polarity, and processing for each mechanism is discussed.
	A procedure for separating and studying these two generation modes
	is also discussed. A unified model from simple kinetic relationships
	is developed and compared to the experimental results. Destructive
	breakdown of the oxide is shown to be correlated with #x2018; #x2018;effective
	#x2019; #x2019; interface softening due to the total defect generation
	caused by both mechanisms.},
  doi = {10.1063/1.352936},
  file = {:PDF\\JAP_Vol73_P3367.pdf:PDF},
  issn = {0021-8979}
}

@ARTICLE{1214058,
  author = {Djurdjevic, I. and Jun Xu and Abdel-Ghaffar, K. and Shu Lin},
  title = {A class of low-density parity-check codes constructed based on Reed-Solomon
	codes with two information symbols},
  journal = {Communications Letters, IEEE},
  year = {2003},
  volume = {7},
  pages = { 317 - 319},
  number = {7},
  month = july,
  abstract = { This letter presents an algebraic method for constructing regular
	low-density parity-check (LDPC) codes based on Reed-Solomon codes
	with two information symbols. The construction method results in
	a class of LDPC codes in Gallager's original form. Codes in this
	class are free of cycles of length 4 in their Tanner graphs and have
	good minimum distances. They perform well with iterative decoding.},
  doi = {10.1109/LCOMM.2003.814716},
  file = {:PDF\\A_Class_of_Low-Density_Parity-Check_Codes_Constructed_Based_on_Reed-Solomon_Codes_With_Two_Information_Symbols.pdf:PDF},
  issn = {1089-7798},
  keywords = { LDPC codes; Reed-Solomon codes; Tanner graphs; information symbols;
	iterative decoding; low-density parity-check codes; minimum distances;
	Reed-Solomon codes; graph theory; iterative decoding; parity check
	codes;}
}

@ARTICLE{5174520,
  author = {Dolecek, L. and Lee, P. and Zhengya Zhang and Anantharam, V. and
	Nikolic, B. and Wainwright, M.},
  title = {Predicting error floors of structured LDPC codes: deterministic bounds
	and estimates},
  journal = {Selected Areas in Communications, IEEE Journal on},
  year = {2009},
  volume = {27},
  pages = {908-917},
  number = {6},
  abstract = {The error-correcting performance of low-density parity check (LDPC)
	codes, when decoded using practical iterative decoding algorithms,
	is known to be close to Shannon limits for codes with suitably large
	blocklengths. A substantial limitation to the use of finite-length
	LDPC codes is the presence of an error floor in the low frame error
	rate (FER) region. This paper develops a deterministic method of
	predicting error floors, based on high signal-to-noise ratio (SNR)
	asymptotics, applied to absorbing sets within structured LDPC codes.
	The approach is illustrated using a class of array-based LDPC codes,
	taken as exemplars of high-performance structured LDPC codes. The
	results are in very good agreement with a stochastic method based
	on importance sampling which, in turn, matches the hardware-based
	experimental results. The importance sampling scheme uses a mean-shifted
	version of the original Gaussian density, appropriately centered
	between a codeword and a dominant absorbing set, to produce an unbiased
	estimator of the FER with substantial computational savings over
	a standard Monte Carlo estimator. Our deterministic estimates are
	guaranteed to be a lower bound to the error probability in the high
	SNR regime, and extend the prediction of the error probability to
	as low as 10-30. By adopting a channel-independent viewpoint, the
	usefulness of these results is demonstrated for both the standard
	Gaussian channel and a channel with mixture noise.},
  doi = {10.1109/JSAC.2009.090809},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\05174520.pdf:PDF},
  issn = {0733-8716},
  keywords = {Gaussian channels;Gaussian processes;block codes;channel capacity;error
	correction codes;error statistics;importance sampling;iterative decoding;parity
	check codes;stochastic processes;Gaussian channel;Gaussian density;SNR
	asymptotics;Shannon limits;array-based LDPC codes;deterministic bounds;error
	floor prediction;error probability;error-correcting performance;finite
	blocklength;finite-length LDPC codes;frame error rate region;importance
	sampling;iterative decoding algorithm;low-density parity check codes;signal-to-noise
	ratio;stochastic method;structured LDPC codes;Code standards;Error
	analysis;Error probability;Gaussian channels;Iterative algorithms;Iterative
	decoding;Monte Carlo methods;Parity check codes;Signal to noise ratio;Stochastic
	processes;LDPC codes; belief propagation; hardware emulation; error
	floor; importance sampling; near-codeword; trapping set; absorbing
	set; pseudocodeword.},
  timestamp = {2013.12.17}
}

@ARTICLE{5629456,
  author = {Guiqiang Dong and Ningde Xie and Tong Zhang},
  title = {On the Use of Soft-Decision Error-Correction Codes in nand Flash
	Memory},
  journal = {Circuits and Systems I: Regular Papers, IEEE Transactions on},
  year = {2011},
  volume = {58},
  pages = {429 -439},
  number = {2},
  month = {feb. },
  abstract = {As technology continues to scale down, NAND Flash memory has been
	increasingly relying on error-correction codes (ECCs) to ensure the
	overall data storage integrity. Although advanced ECCs such as low-density
	parity-check (LDPC) codes can provide significantly stronger error-correction
	capability over BCH codes being used in current practice, their decoding
	requires soft-decision log-likelihood ratio (LLR) information. This
	results in two critical issues. First, accurate calculation of LLR
	demands fine-grained memory-cell sensing, which nevertheless tends
	to incur implementation overhead and access latency penalty. Hence,
	it is critical to minimize the fine-grained memory sensing precision.
	Second, accurate calculation of LLR also demands the availability
	of a memory-cell threshold-voltage distribution model. As the major
	source for memory-cell threshold-voltage distribution distortion,
	cell-to-cell interference must be carefully incorporated into the
	model. However, these two critical issues have not been ever addressed
	in the open literature. This paper attempts to address these open
	issues. We derive mathematical formulations to approximately model
	the threshold-voltage distribution of memory cells in the presence
	of cell-to-cell interference, based on which the calculation of LLRs
	is mathematically formulated. This paper also proposes a nonuniform
	memory sensing strategy to reduce the memory sensing precision and,
	thus, sensing latency while still maintaining good error-correction
	performance. In addition, we investigate these design issues under
	the scenario when we can also sense interfering cells and hence explicitly
	estimate cell-to-cell interference strength. We carry out extensive
	computer simulations to demonstrate the effectiveness and involved
	tradeoffs, assuming the use of LDPC codes in 2-bits/cell NAND Flash
	memory.},
  doi = {10.1109/TCSI.2010.2071990},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05629456.pdf:PDF},
  issn = {1549-8328},
  keywords = {BCH codes;ECC;LDPC codes;LLR information;NAND flash emory;access latency
	penalty;cell-to-cell interference;computer simulations;data storage
	integrity;decoding;error-correction capability;error-correction performance;fine-grained
	memory sensing precision;fine-grained memory-cell sensing;implementation
	overhead;interfering cells;low-density parity-check codes;mathematical
	formulations;memory-cell threshold-voltage distribution distortion;memory-cell
	threshold-voltage distribution model;nonuniform memory sensing strategy;sensing
	latency;soft-decision error-correction codes;soft-decision log-likelihood
	ratio information;BCH codes;NAND circuits;decoding;error correction
	codes;flash memories;interference;parity check codes;},
  timestamp = {2012.04.25}
}

@INPROCEEDINGS{Dong:2008:CME:1391469.1391610,
  author = {Dong, Xiangyu and Wu, Xiaoxia and Sun, Guangyu and Xie, Yuan and
	Li, Helen and Chen, Yiran},
  title = {Circuit and microarchitecture evaluation of 3D stacking magnetic
	RAM (MRAM) as a universal memory replacement},
  booktitle = {Proceedings of the 45th annual Design Automation Conference},
  year = {2008},
  series = {DAC '08},
  pages = {554--559},
  address = {New York, NY, USA},
  publisher = {ACM},
  acmid = {1391610},
  doi = {http://doi.acm.org/10.1145/1391469.1391610},
  file = {:PDF\\p554-dong.pdf:PDF},
  isbn = {978-1-60558-115-6},
  keywords = {3D stacking, MRAM},
  location = {Anaheim, California},
  numpages = {6},
  timestamp = {2011.06.03},
  url = {http://doi.acm.org/10.1145/1391469.1391610}
}

@ARTICLE{1057299,
  author = { Dornstetter, J.},
  title = {On the equivalence between Berlekamp's and Euclid's algorithms (Corresp.)},
  journal = {Information Theory, IEEE Transactions on},
  year = {1987},
  volume = {33},
  pages = { 428 - 431},
  number = {3},
  month = may,
  doi = {10.1109/TIT.1987.1057299},
  file = {:PDF\\On_the_Equivalence_Between_Berlekamp's_and_Euclid's_Algorithms.pdf:PDF},
  issn = {0018-9448},
  keywords = { Division; Polynomials;}
}

@ARTICLE{springerlink:10.1007_s00200-009-0095-3,
  author = {Esmaeili, M. and Yari, S.},
  title = {Generalized quasi-cyclic codes: structural properties and code construction},
  journal = {Applicable Algebra in Engineering, Communication and Computing},
  year = {2009},
  volume = {20},
  pages = {159-173},
  note = {10.1007/s00200-009-0095-3},
  abstract = {Generalized quasi-cyclic (GQC) codes are defined by generator matrices
	comprised of circulant matrices of lengths not necessarily identical.
	A decomposition of these codes is given by using the Chinese reminder
	theorem. The focus is to characterize ρ -generator GQC codes in details.
	A good lower bound on the minimum distance of such a code in terms
	of the minimum distance of the constituent codes is given. Construction
	methods are given and a set of GQC codes is provided that from minimum
	distance perspective are optimal codes among the known linear codes
	having the same length and dimension.},
  affiliation = {Isfahan University of Technology Department of Mathematical Sciences
	84156-83111 Isfahan Iran},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\springerlink10.1007_s00200-009-0095-3.pdf:PDF},
  issn = {0938-1279},
  issue = {2},
  keyword = {Computer Science},
  publisher = {Springer Berlin / Heidelberg},
  timestamp = {2011.10.13},
  url = {http://dx.doi.org/10.1007/s00200-009-0095-3}
}

@ARTICLE{179340,
  author = {Feng, G.-L. and Rao, T.R.N.},
  title = {Decoding algebraic-geometric codes up to the designed minimum distance},
  journal = {Information Theory, IEEE Transactions on},
  year = {1993},
  volume = {39},
  pages = {37 -45},
  number = {1},
  month = {jan},
  abstract = {A simple decoding procedure for algebraic-geometric codes C Omega;(D,G)
	is presented. This decoding procedure is a generalization of Peterson's
	decoding procedure for the BCH codes. It can be used to correct any
	[(d*-1)/2] or fewer errors with complexity O(n3), where d * is the
	designed minimum distance of the algebraic-geometric code and n is
	the codelength},
  doi = {10.1109/18.179340},
  file = {:PDF\\00179340.pdf:PDF},
  issn = {0018-9448},
  keywords = {algebraic-geometric codes;complexity;decoding procedure;error correction;minimum
	distance;computational complexity;decoding;error correction codes;},
  timestamp = {2011.06.14}
}

@INPROCEEDINGS{510063,
  author = {Liu Feng and Ebel, W.J.},
  title = {Algebraic geometry codes from Reed Solomon codes},
  booktitle = {Southeastcon '96. 'Bringing Together Education, Science and Technology'.,
	Proceedings of the IEEE},
  year = {1996},
  pages = {231 -237},
  month = apr,
  abstract = {This paper discusses algebraic geometry codes by their construction,
	parametric properties, and decoding algorithms. Comparisons between
	algebraic geometry codes and Reed Solomon codes are made to provide
	a deeper understanding of the algebraic geometry codes. Also, a worked
	example of algebraic geometry codes is given},
  doi = {10.1109/SECON.1996.510063},
  file = {:\\\\yrlshare.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\00510063.pdf:PDF},
  keywords = {Reed Solomon codes;algebraic geometry codes;code construction;decoding
	algorithms;error correcting codes;parametric properties;Reed-Solomon
	codes;algebraic geometric codes;decoding;error correction codes;}
}

@ARTICLE{PhysRev.185.832,
  author = {Ferdinand, Arthur E. and Fisher, Michael E.},
  title = {Bounded and Inhomogeneous Ising Models. I. Specific-Heat Anomaly
	of a Finite Lattice},
  journal = {Phys. Rev.},
  year = {1969},
  volume = {185},
  pages = {832--846},
  month = {Sep},
  doi = {10.1103/PhysRev.185.832},
  file = {:PDF\\PhysRev.185.832.pdf:PDF},
  issue = {2},
  publisher = {American Physical Society},
  timestamp = {2012.07.25},
  url = {http://link.aps.org/doi/10.1103/PhysRev.185.832}
}

@ARTICLE{PhysRevLett.63.1195,
  author = {Ferrenberg, Alan M. and Swendsen, Robert H.},
  title = {Optimized Monte Carlo data analysis},
  journal = {Phys. Rev. Lett.},
  year = {1989},
  volume = {63},
  pages = {1195--1198},
  month = {Sep},
  doi = {10.1103/PhysRevLett.63.1195},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\PhysRevLett.63.1195.pdf:PDF},
  issue = {12},
  publisher = {American Physical Society},
  timestamp = {2012.04.03},
  url = {http://link.aps.org/doi/10.1103/PhysRevLett.63.1195}
}

@ARTICLE{PhysRevLett.61.2635,
  author = {Ferrenberg, Alan M. and Swendsen, Robert H.},
  title = {New Monte Carlo technique for studying phase transitions},
  journal = {Phys. Rev. Lett.},
  year = {1988},
  volume = {61},
  pages = {2635--2638},
  month = {Dec},
  doi = {10.1103/PhysRevLett.61.2635},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\PhysRevLett.61.2635.pdf:PDF},
  issue = {23},
  publisher = {American Physical Society},
  timestamp = {2012.04.03},
  url = {http://link.aps.org/doi/10.1103/PhysRevLett.61.2635}
}

@BOOK{feynman_statistical_mechanics,
  title = {ファインマン統計力学},
  publisher = {シュプリンガー・ジャパン株式会社},
  year = {2011},
  author = {Richard P. Feynman},
  isbn = {978-4-431-100683},
  timestamp = {2012.03.15}
}

@ARTICLE{1317123,
  author = {Fossorier, M.P.C.},
  title = {Quasicyclic low-density parity-check codes from circulant permutation
	matrices},
  journal = {Information Theory, IEEE Transactions on},
  year = {2004},
  volume = {50},
  pages = { 1788 - 1793},
  number = {8},
  month = aug,
  abstract = { In this correspondence, the construction of low-density parity-check
	(LDPC) codes from circulant permutation matrices is investigated.
	It is shown that such codes cannot have a Tanner graph representation
	with girth larger than 12, and a relatively mild necessary and sufficient
	condition for the code to have a girth of 6, 8,10, or 12 is derived.
	These results suggest that families of LDPC codes with such girth
	values are relatively easy to obtain and, consequently, additional
	parameters such as the minimum distance or the number of redundant
	check sums should be considered. To this end, a necessary condition
	for the codes investigated to reach their maximum possible minimum
	Hamming distance is proposed.},
  doi = {10.1109/TIT.2004.831841},
  file = {:PDF\\Quasi-Cyclic_Low-Density_Parity-Check_Codes_From_Circulant_Permutation_Matrices.pdf:PDF},
  issn = {0018-9448},
  keywords = { Hamming distance; LDPC code; QC; Tanner graph representation; circulant
	permutation matrix; girth value; iterative decoding; low-density
	parity-check; quasicyclic codes; Hamming codes; cyclic codes; iterative
	decoding; matrix algebra; parity check codes;}
}

@ARTICLE{768759,
  author = {Fossorier, M.P.C. and Mihaljevic, M. and Imai, H.},
  title = {Reduced complexity iterative decoding of low-density parity check
	codes based on belief propagation},
  journal = {Communications, IEEE Transactions on},
  year = {1999},
  volume = {47},
  pages = {673 -680},
  number = {5},
  month = may,
  abstract = {Two simplified versions of the belief propagation algorithm for fast
	iterative decoding of low-density parity check codes on the additive
	white Gaussian noise channel are proposed. Both versions are implemented
	with real additions only, which greatly simplifies the decoding complexity
	of belief propagation in which products of probabilities have to
	be computed. Also, these two algorithms do not require any knowledge
	about the channel characteristics. Both algorithms yield a good performance-complexity
	trade-off and can be efficiently implemented in software as well
	as in hardware, with possibly quantized received values},
  doi = {10.1109/26.768759},
  file = {:PDF\\Reduced_Complexity_Iterative_Decoding_of_Low-Density_Parity_Check_Codes_Based_on_Belief_Propagation.pdf:PDF},
  issn = {0090-6778},
  keywords = {additive white Gaussian noise channel;belief propagation algorithm;binary
	code;block code;decoding complexity;fast iterative decoding;hardware;low-density
	parity check codes;performance-complexity trade-off;probabilities
	product;quantized received values;reduced complexity iterative decoding;software;AWGN
	channels;belief maintenance;binary codes;block codes;computational
	complexity;error detection codes;iterative decoding;probability;}
}

@INPROCEEDINGS{4024287,
  author = {Minyue Fu},
  title = {On Gaussian Approximation for Density Evolution of Low-Density Parity-Check
	Codes},
  booktitle = {Communications, 2006. ICC '06. IEEE International Conference on},
  year = {2006},
  volume = {3},
  pages = {1107 -1112},
  month = {june },
  abstract = {This paper is concerned with density evolution for iterative decoding
	of low-density parity-check (LDPC) codes. We first study the problem
	of density evolution computation for regular LDPC codes. For this,
	we propose a simple computational algorithm based on the ergodicity
	theory. This method is shown to match very well with explicit calculations
	of density functions. The second problem we study is about the approach
	of Gaussian approximation to density evolution. We point out that
	it is inappropriate to use the mean of the density only to model
	the iterative decoding process. Instead, both the mean and variance
	are needed for Gaussian approximation. Finally, we consider the problem
	of density evolution for irregular LDPC codes. For this, we extend
	the density evolution algorithm for regular LDPC codes to irregular
	LDPC codes. We then illustrate that Gaussian approximation is also
	valid provided that the degree distributions are not wide. A dynamic
	model is also presented based on Gaussian approximation.},
  doi = {10.1109/ICC.2006.254895},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\04024287.pdf:PDF},
  issn = {8164-9547},
  timestamp = {2011.12.02}
}

@INPROCEEDINGS{1523458,
  author = {Fujita, H. and Ohata, M. and Sakaniwa, K.},
  title = {An algebraic method for constructing efficiently encodable irregular
	LDPC codes},
  booktitle = {Information Theory, 2005. ISIT 2005. Proceedings. International Symposium
	on},
  year = {2005},
  pages = {855 -859},
  month = sept.,
  abstract = {In this paper we propose an algebraic construction of efficiently
	encodable irregular LDPC codes. The proposed irregular LDPC codes
	have not only an efficient encoding algorithm but also guaranteed
	minimum distances. Simulation results show that the proposed codes
	perform well compared to randomly constructed irregular LDPC codes},
  doi = {10.1109/ISIT.2005.1523458},
  file = {:PDF\\01523458.pdf:PDF},
  keywords = {algebraic construction;encoding algorithm;irregular LDPC codes;algebraic
	codes;matrix algebra;parity check codes;}
}

@INPROCEEDINGS{1365312,
  author = {Fujita, H. and Sakaniwa, K.},
  title = {An efficient encoding method for LDPC codes based on cyclic shift},
  booktitle = {Information Theory, 2004. ISIT 2004. Proceedings. International Symposium
	on},
  year = {2004},
  pages = { 276},
  month = june-2 # july,
  abstract = { Low-density parity-check (LDPC) codes are one of the most promising
	next generation error correcting codes and many investigations shows
	that LDPC codes suitable for many hardware implementation. Although
	randomly constructed LDPC codes are usually encoded by using generator
	matrix, this method requires quadratic time complexity and is not
	easy to implement. This work presents the encoding of array-type
	LDPC codes and a special class of Sridhara-Fuja-Tanner (SFT) codes
	by division circuits as cyclic codes, which are very easy to implement.},
  doi = {10.1109/ISIT.2004.1365312},
  file = {:PDF\\An_Efficient_Encoding_Method_for_LDPC_Codes_Based_on_Cyclic_Shift.pdf:PDF},
  keywords = { SFT codes; Sridhara-Fuja-Tanner codes; array-type LDPC codes; cyclic
	codes; division circuit; error correcting code; low-density parity-check
	code; cyclic codes; error correction codes; matrix algebra; parity
	check codes;}
}

@INPROCEEDINGS{4036049,
  author = {Gabidulin, E. and Moinian, A. and Honary, B.},
  title = {Generalized Construction of Quasi-Cyclic Regular LDPC Codes Based
	on Permutation Matrices},
  booktitle = {Information Theory, 2006 IEEE International Symposium on},
  year = {2006},
  pages = {679 -683},
  month = {july},
  abstract = {A new approach is proposed for constructing regular low-density parity-check
	(LDPC) codes based on tensor product of matrices. In this paper,
	first a general construction method of regular LDPC codes exploiting
	permutation matrices is described. Constructed codes have a quasi-cyclic
	structure with no short cycles of length 4 in their Tanner graph,
	hence simple encoding while maintaining good performance is achieved.
	The paper also demonstrates a generalized design, which covers a
	large family of LDPC codes and number of other construction methods.
	The new generalized LDPC codes are defined by a small number of parameters
	and cover a large set of code lengths and rates. Using these codes,
	LDPC matrices of any column weight and row weight can be constructed.
	Performance of these codes under iterative decoding compares well
	with other well-structured as well as random LDPC codes},
  doi = {10.1109/ISIT.2006.261871},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\04036049.pdf:PDF},
  keywords = {Tanner graph;code lengths;generalized construction;iterative decoding;low-density
	parity-check codes;matrix tensor product;permutation matrices;quasi-cyclic
	regular LDPC codes;quasi-cyclic structure;random LDPC codes;cyclic
	codes;iterative decoding;matrix algebra;parity check codes;tensors;},
  timestamp = {2012.04.17}
}

@ARTICLE{1057683,
  author = {Gallager, R.},
  title = {Low-density parity-check codes},
  journal = {Information Theory, IRE Transactions on},
  year = {1962},
  volume = {8},
  pages = {21 -28},
  number = {1},
  month = {jan.},
  doi = {10.1109/TIT.1962.1057683},
  file = {:PDF\\Low-Density_Parity-Check_Codes_Gallager.pdf:PDF},
  issn = {0096-1000},
  keywords = {Error-correcting codes;Parity checks;}
}

@BOOK{GallagerMITbook,
  title = {Low-Density Parity Check Codes},
  publisher = {THE MIT PRESS},
  year = {1963},
  author = {Robert G. Gallager},
  timestamp = {2012.01.13}
}

@BOOK{Singular_Introduction_to_Commutative_Algebra,
  title = {A Singular Introduction to Commutative Algebra},
  publisher = {Springer},
  year = {2008},
  author = {Gert-Martin Greuel, Gerhard Pfister},
  abstract = {This substantially enlarged second edition aims to lead a further
	stage in the computational revolution in commutative algebra. This
	is the first handbook/tutorial to extensively deal with SINGULAR.
	Among the book’s most distinctive features is a new, completely unified
	treatment of the global and local theories. Another feature of the
	book is its breadth of coverage of theoretical topics in the portions
	of commutative algebra closest to algebraic geometry, with algorithmic
	treatments of almost every topic.},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\A_SINGULAR_Introduction_to_Commutative_Algebra.pdf:PDF},
  isbn = {978-3540735410},
  timestamp = {2013.06.03}
}

@INPROCEEDINGS{996673,
  author = {Ghidini, G. and Sebastiani, A. and Brazzelli, D.},
  title = {Stress induced leakage current and bulk oxide trapping: temperature
	evolution},
  booktitle = {Reliability Physics Symposium Proceedings, 2002. 40th Annual},
  year = {2002},
  pages = { 415 - 416},
  doi = {10.1109/RELPHY.2002.996673},
  issn = { },
  keywords = { 10 nm; 8 nm; CMOS process; SILC temperature evolution; STI; bulk
	oxide trapping; degraded thick oxide conduction; dual-gate technology;
	fixed field stationary SILC; flash cell scaling; flat area capacitors;
	gate leakage current; multiple trap assisted tunneling; shallow trench
	isolation; stable charge measurement; stable charge position; stress
	induced leakage current; thermal cycling; tunnel oxide thickness;
	tunneling front model; CMOS integrated circuits; annealing; electric
	charge; electron traps; flash memories; integrated circuit measurement;
	isolation technology; leakage currents; stress effects; tunnelling;}
}

@ARTICLE{1054126,
  author = { Goethals, J.-M. and Delsarte, P.},
  title = {On a class of majority-logic decodable cyclic codes},
  journal = {Information Theory, IEEE Transactions on},
  year = {1968},
  volume = {14},
  pages = { 182 - 188},
  number = {2},
  month = mar,
  abstract = { A new infinite class of cyclic codes is studied. Codes of this class
	can be decoded in a step-by-step manner, using` majority logic. Some
	previously known codes fall in this class, and thus admit simpler
	decoding procedures. As random error-correcting codes, the codes
	are nearly as powerful as the Bose-Chaudhuri codes.},
  doi = {10.1109/TIT.1968.1054126},
  file = {:PDF\\On_a_Class_of_Majority-Logic_Decodable_Cyclic_Codes.pdf:PDF},
  issn = {0018-9448},
  keywords = { Cyclic codes; Information theory group; Majority logic decoding;}
}

@ARTICLE{782097,
  author = {Guruswami, V. and Sudan, M.},
  title = {Improved decoding of Reed-Solomon and algebraic-geometry codes},
  journal = {Information Theory, IEEE Transactions on},
  year = {1999},
  volume = {45},
  pages = {1757 -1767},
  number = {6},
  month = {sep},
  abstract = {Given an error-correcting code over strings of length n and an arbitrary
	input string also of length n, the list decoding problem is that
	of finding all codewords within a specified Hamming distance from
	the input string. We present an improved list decoding algorithm
	for decoding Reed-Solomon codes. The list decoding problem for Reed-Solomon
	codes reduces to the following ldquo;curve-fitting rdquo; problem
	over a field F: given n points ((xi middot;yi))i=1 n, xi, yi isin;F,
	and a degree parameter k and error parameter e, find all univariate
	polynomials p of degree at most k such that yi=p(xi) for all but
	at most e values of i isin;(1,...,n). We give an algorithm that solves
	this problem for e lt;n- radic;(kn), which improves over the previous
	best result, for every choice of k and n. Of particular interest
	is the case of k/n gt;1/3, where the result yields the first asymptotic
	improvement in four decades. The algorithm generalizes to solve the
	list decoding problem for other algebraic codes, specifically alternant
	codes (a class of codes including BCH codes) and algebraic-geometry
	codes. In both cases, we obtain a list decoding algorithm that corrects
	up to n- radic;(n(n-d')) errors, where n is the block length and
	d' is the designed distance of the code. The improvement for the
	case of algebraic-geometry codes extends the methods of Shokrollahi
	and Wasserman (see in Proc. 29th Annu. ACM Symp. Theory of Computing,
	p.241-48, 1998) and improves upon their bound for every choice of
	n and d'. We also present some other consequences of our algorithm
	including a solution to a weighted curve-fitting problem, which may
	be of use in soft-decision decoding algorithms for Reed-Solomon codes},
  doi = {10.1109/18.782097},
  file = {:PDF\\00782097.pdf:PDF},
  issn = {0018-9448},
  keywords = {BCH codes;Hamming distance;Reed-Solomon codes;algebraic-geometry codes;alternant
	codes;asymptotic improvement;block length;bound;code distance;codewords;curve-fitting
	problem;degree parameter;error parameter;error-correcting code;input
	string;list decoding algorithm;polynomial time algorithm;soft-decision
	decoding algorithms;string length;univariate polynomials;weighted
	curve-fitting problem;BCH codes;Reed-Solomon codes;algebraic geometric
	codes;coding errors;curve fitting;decoding;},
  timestamp = {2011.04.18}
}

@ARTICLE{476233,
  author = {Hache, G. and Le Brigand, D.},
  title = {Effective construction of algebraic geometry codes},
  journal = {Information Theory, IEEE Transactions on},
  year = {1995},
  volume = {41},
  pages = {1615 -1628},
  number = {6},
  month = nov,
  abstract = {We intend to show that algebraic geometry codes may be constructed
	easily using blowing-up theory for any projective plane algebraic
	curve. Our paper is based on a paper by Le Brigand and Risler (1988).
	We try to be as explicit as possible},
  doi = {10.1109/18.476233},
  file = {:PDF\\Effective_Construction_of_Algebraic_Geometry_Codes.pdf:PDF},
  issn = {0018-9448},
  keywords = {algebraic geometry codes;blowing-up theory;code construction;geometric
	Goppa codes;projective plane algebraic curve;Goppa codes;algebraic
	geometric codes;}
}

@ARTICLE{PhysRev_123_85,
  author = {Harrison, Walter A.},
  title = {Tunneling from an Independent-Particle Point of View},
  journal = {Phys. Rev.},
  year = {1961},
  volume = {123},
  pages = {85--89},
  month = {Jul},
  doi = {10.1103/PhysRev.123.85},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\PhysRev_123_85.pdf:PDF},
  issue = {1},
  publisher = {American Physical Society},
  timestamp = {2011.09.15},
  url = {http://link.aps.org/doi/10.1103/PhysRev.123.85}
}

@ARTICLE{1055198,
  author = { Hartmann, C. and Ducey, J. and Rudolph, L.},
  title = {On the structure of generalized finite-geometry codes},
  journal = {Information Theory, IEEE Transactions on},
  year = {1974},
  volume = {20},
  pages = { 240 - 252},
  number = {2},
  month = {mar},
  abstract = { Some new results on the structure of generalized finite-geometry
	codes are presented.},
  doi = {10.1109/TIT.1974.1055198},
  file = {:PDF\\01055198.pdf:PDF},
  issn = {0018-9448},
  keywords = {Geometry codes;},
  timestamp = {2011.04.19}
}

@ARTICLE{476247,
  author = {Heegard, C. and Little, J. and Saints, K.},
  title = {Systematic encoding via Gr\"{o}bner bases for a class of algebraic-geometric
	Goppa codes},
  journal = {Information Theory, IEEE Transactions on},
  year = {1995},
  volume = {41},
  pages = {1752 -1761},
  number = {6},
  month = {nov},
  abstract = {Any linear code with a nontrivial automorphism has the structure of
	a module over a polynomial ring. The theory of Grobner bases for
	modules gives a compact description and implementation of a systematic
	encoder. We present examples of algebraic-geometric Goppa codes that
	can be encoded by these methods, including the one-point Hermitian
	codes },
  doi = {10.1109/18.476247},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\00476247.pdf:PDF},
  issn = {0018-9448},
  keywords = {Grobner bases;algebraic-geometric Goppa codes;linear code;modules;nontrivial
	automorphism;one-point Hermitian codes;polynomial ring;systematic
	encoding;Goppa codes;algebraic geometric codes;linear algebra;linear
	codes;polynomials;},
  timestamp = {2012.04.27}
}

@ARTICLE{Herlihy:1993:TMA:173682.165164,
  author = {Herlihy, Maurice and Moss, J. Eliot B.},
  title = {Transactional memory: architectural support for lock-free data structures},
  journal = {SIGARCH Comput. Archit. News},
  year = {1993},
  volume = {21},
  pages = {289--300},
  month = {May},
  acmid = {165164},
  address = {New York, NY, USA},
  doi = {http://doi.acm.org/10.1145/173682.165164},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\p289-herlihy.pdf:PDF},
  issn = {0163-5964},
  issue = {2},
  issue_date = {May 1993},
  numpages = {12},
  publisher = {ACM},
  timestamp = {2011.11.25},
  url = {http://doi.acm.org/10.1145/173682.165164}
}

@ARTICLE{Hess2002425,
  author = {F. Hess},
  title = {Computing Riemann–Roch Spaces in Algebraic Function Fields and Related
	Topics},
  journal = {Journal of Symbolic Computation},
  year = {2002},
  volume = {33},
  pages = {425 - 445},
  number = {4},
  abstract = {We develop a simple and efficient algorithm to compute Riemann–Roch
	spaces of divisors in general algebraic function fields which does
	not use the Brill–Noether method of adjoints or any series expansions.
	The basic idea also leads to an elementary proof of the Riemann–Roch
	theorem. We describe the connection to the geometry of numbers of
	algebraic function fields and develop a notion and algorithm for
	divisor reduction. An important application is to compute in the
	divisor class group of an algebraic function field.},
  doi = {10.1006/jsco.2001.0513},
  file = {:PDF\\Hess2002425.pdf:PDF},
  issn = {0747-7171},
  timestamp = {2012.05.17},
  url = {http://www.sciencedirect.com/science/article/pii/S0747717101905139}
}

@ARTICLE{887869,
  author = {Heydtmann, A.E. and Jensen, J.M.},
  title = {On the equivalence of the Berlekamp-Massey and the Euclidean algorithms
	for decoding},
  journal = {Information Theory, IEEE Transactions on},
  year = {2000},
  volume = {46},
  pages = {2614 -2624},
  number = {7},
  month = nov,
  doi = {10.1109/18.887869},
  file = {:PDF\\On_the_Equivalence_of_the_Berlekamp-Massey_and_the_Euclidean_Algorithms_for_Decoding.pdf:PDF},
  issn = {0018-9448},
  keywords = {Berlekamp-Massey algorithm;Euclidean algorithm;Reed Solomon codes;alternant
	codes;arithmetic;decoding;error coevaluator;error evaluator;error
	locator;fundamental iterative algorithm;key equation;syndrome matrix;error
	analysis;iterative decoding;matrix algebra;}
}

@INPROCEEDINGS{4895509,
  author = {Hirotomo, M. and Konishi, Y. and Morii, M.},
  title = {Approximate examination of trapping sets of LDPC codes using the
	probabilistic algorithm},
  booktitle = {Information Theory and Its Applications, 2008. ISITA 2008. International
	Symposium on},
  year = {2008},
  pages = {1 -6},
  month = {dec.},
  abstract = {The performance of LDPC codes decoded by iterative algorithms depends
	on the structural properties of their underlying Tanner graphs. For
	general memoryless channels, error patterns dominating the bit and
	frame error probabilities at the error floor region are termed trapping
	sets. In this paper, we propose an effective method for finding small-size
	trapping sets of LDPC codes. In the proposed method, a probabilistic
	algorithm to find low-weight codewords is applied to finding small
	trapping sets of LDPC codes. Furthermore, we show numerical results
	of examining small trapping sets of (504, 252) and (1008, 504) LDPC
	codes.},
  doi = {10.1109/ISITA.2008.4895509},
  file = {:PDF\\04895509.pdf:PDF},
  keywords = {Bit error rate;Error analysis;Error probability;Information theory;Iterative
	algorithms;Iterative decoding;Linear code;Memoryless systems;Parity
	check codes;Sparse matrices;error statistics;graph theory;iterative
	decoding;parity check codes;LDPC codes;Tanner graphs;error floor
	region;error patterns;frame error probabilities;general memoryless
	channels;iterative algorithms;low-weight codewords;probabilistic
	algorithm;trapping sets;},
  timestamp = {2013.02.05}
}

@INPROCEEDINGS{5654357,
  author = {Hirotomo, M. and Morii, M.},
  title = {Detailed evaluation of error floors of LDPC codes using the probabilistic
	algorithm},
  booktitle = {Information Theory and its Applications (ISITA), 2010 International
	Symposium on},
  year = {2010},
  pages = {513 -518},
  month = {oct},
  abstract = {The performance of LDPC codes decoded by iterative algorithm depends
	on the structural properties of their underlying Tanner graphs. For
	discrete memoryless channels, error patterns dominating the frame
	error rate (FER) in the error floor region are termed trapping set.
	We have shown approximate examinations of small trapping sets of
	LDPC codes using the probabilistic algorithm. In this paper, we propose
	an efficient method for the detailed evaluation of the FER of LDPC
	codes in the error floor region. The accuracy of the FER lies on
	the failure probability of the number of trapping sets determined
	by our probabilistic algorithm and the FER estimated by the importance
	sampling.},
  doi = {10.1109/ISITA.2010.5654357},
  file = {:PDF\\05654357.pdf:PDF},
  keywords = {Charge carrier processes;Decoding;Monte Carlo methods;Parity check
	codes;Probabilistic logic;Sparse matrices;Vectors;error statistics;parity
	check codes;LDPC code;error floor region;frame error rate;importance
	sampling;low density parity check code;probabilistic algorithm;tanner
	graph;},
  timestamp = {2013.01.25}
}

@INPROCEEDINGS{1204466,
  author = {Hocevar, D.E.},
  title = {LDPC code construction with flexible hardware implementation},
  booktitle = {Communications, 2003. ICC '03. IEEE International Conference on},
  year = {2003},
  volume = {4},
  pages = { 2708 - 2712 vol.4},
  month = may,
  abstract = { This paper presents an LDPC code construction technique for irregular
	codes of various block sizes and code rates, thus obtaining the performance
	benefit of irregular distributions. It represents an extension of
	the methods presented by Sridhara et al. [2001] for regular codes.
	More importantly, an efficient and practical decoder architecture
	is also presented that achieves flexibility in block size and code
	rate for this broad family of codes, a capability not present in
	other approaches. This decoder can also achieve a high degree of
	parallelism, thus exploiting one of the benefits of belief propagation.},
  doi = {10.1109/ICC.2003.1204466},
  file = {:PDF\\LDPC_Code_Construction_with_Flexible_Hardware_Implementation.pdf:PDF},
  keywords = { LDPC code construction technique; addressing; belief propagation;
	block sizes; code rates; decoder architecture; error correction performance;
	flexible hardware decoder; irregular codes; irregular distributions;
	parallelism; routing; codecs; error correction codes; iterative decoding;
	parity check codes;}
}

@ARTICLE{29471,
  author = {Hochet, B. and Quinton, P. and Robert, Y.},
  title = {Systolic Gaussian elimination over GF(p) with partial pivoting},
  journal = {Computers, IEEE Transactions on},
  year = {1989},
  volume = {38},
  pages = {1321 -1324},
  number = {9},
  month = sep,
  abstract = {A systolic architecture is proposed for the triangularization by means
	of the Gaussian elimination algorithm of large dense n times;n matrices
	over GF(p), where p is a prime number. The solution of large dense
	linear systems over GF(p) is the major computational step in various
	algorithms issuing from arithmetic number theory and computer algebra.
	The proposed architecture implements the elimination with partial
	pivoting, although the operation of the array remains purely systolic.
	Extension of the array to the complete solution of a linear system
	Ax=b over GF(p) is also considered},
  doi = {10.1109/12.29471},
  file = {:PDF\\Systolic_Gaussian_Elimination_over_GF(p)_with_Partial_Pivoting.pdf:PDF},
  issn = {0018-9340},
  keywords = {arithmetic number theory;computer algebra;large dense linear systems;partial
	pivoting;prime number;systolic Gaussian elimination;systolic architecture;triangularization;digital
	arithmetic;number theory;parallel architectures;}
}

@ARTICLE{476214,
  author = {Hoholdt, T. and Pellikaan, R.},
  title = {On the decoding of algebraic-geometric codes},
  journal = {Information Theory, IEEE Transactions on},
  year = {1995},
  volume = {41},
  pages = {1589-1614},
  number = {6},
  month = {Nov},
  abstract = {This paper provides a survey of the existing literature on the decoding
	of algebraic-geometric codes. Definitions, theorems, and cross references
	will be given. We show what has been done, discuss what still has
	to be done, and pose some open problems},
  doi = {10.1109/18.476214},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\00476214.pdf:PDF},
  issn = {0018-9448},
  keywords = {Goppa codes;algebraic geometric codes;decoding;reviews;Goppa codes;algebraic-geometric
	codes;algorithm;decoding;definitions;theorems;Concrete;Decoding;Elliptic
	curves;Error correction;Galois fields;Helium;Mathematics;Poles and
	zeros;Reed-Solomon codes;Strontium},
  timestamp = {2014.05.21}
}

@ARTICLE{924876,
  author = {Jilei Hou and Siegel, P.H. and Milstein, L.B.},
  title = {Performance analysis and code optimization of low density parity-check
	codes on Rayleigh fading channels},
  journal = {Selected Areas in Communications, IEEE Journal on},
  year = {2001},
  volume = {19},
  pages = {924 -934},
  number = {5},
  month = {may},
  abstract = {A numerical method has been presented to determine the noise thresholds
	of low density parity-check (LDPC) codes that employ the message
	passing decoding algorithm on the additive white Gaussian noise (AWGN)
	channel. In this paper, we apply the technique to the uncorrelated
	flat Rayleigh fading channel. Using a nonlinear code optimization
	technique, we optimize irregular LDPC codes for such a channel. The
	thresholds of the optimized irregular LDPC codes are very close to
	the Shannon limit for this channel. For example, at rate one-half,
	the optimized irregular LDPC code has a threshold only 0.07 dB away
	from the capacity of the channel. Furthermore, we compare simulated
	performance of the optimized irregular LDPC codes and turbo codes
	on a land mobile channel, and the results indicate that at a block
	size of 3072, irregular LDPC codes can outperform turbo codes over
	a wide range of mobile speeds},
  doi = {10.1109/49.924876},
  file = {:PDF\\00924876.pdf:PDF},
  issn = {0733-8716},
  keywords = {AWGN;Additive white noise;Channel capacity;Decoding;Fading;Gaussian
	noise;Message passing;Parity check codes;Performance analysis;Turbo
	codes;AWGN channels;Rayleigh channels;channel capacity;error correction
	codes;error detection codes;land mobile radio;nonlinear codes;optimisation;turbo
	codes;AWGN channel;Shannon limit;additive white Gaussian noise channel;block
	size;channel capacity;channel coding;code thresholds;error correcting
	codes;land mobile channel;low density parity-check codes;message
	passing decoding algorithm;mobile speeds;noise thresholds;nonlinear
	code optimization;numerical method;optimized irregular LDPC codes;performance
	analysis;simulated performance;turbo codes;uncorrelated flat Rayleigh
	fading channel;},
  timestamp = {2013.01.21}
}

@INPROCEEDINGS{5555509,
  author = {Wanbao Hu and Huaping Cai and Yanxia Wu and Zhen Wang},
  title = {A note on relationship between algebraic geometric codes and LDPC
	codes},
  booktitle = {Signal Processing Systems (ICSPS), 2010 2nd International Conference
	on},
  year = {2010},
  volume = {1},
  pages = {V1-56 -V1-58},
  month = july,
  abstract = {Low-density parity-check (LDPC) codes constructed by a sparse parity-check
	matrix are of very fast encoding and decoding algorithms. Another
	kind of codes, which improved the well-known Gilbert-Varshamov bound,
	are algebraic geometry codes (Goppa geometry codes) from algebraic
	curves over finite fields. In the note, we analyze their characteristic
	of the two class of codes and show that the algebraic geometric codes
	are seldom LDPC codes.},
  doi = {10.1109/ICSPS.2010.5555509},
  file = {:\\\\yrlshare.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05555509.pdf:PDF},
  keywords = {Gilbert-Varshamov bound;LDPC code;algebraic curves;algebraic geometric
	code;decoding algorithm;encoding algorithm;low density parity check
	code;sparse parity check matrix;algebraic geometric codes;decoding;parity
	check codes;sparse matrices;}
}

@INPROCEEDINGS{4410966,
  author = {Xinde Hu and Kumar, B.V.K.V. and Zongwang Li and Barndt, R.},
  title = {Error Floor Estimation of Long LDPC Codes on Partial Response Channels},
  booktitle = {Global Telecommunications Conference, 2007. GLOBECOM '07. IEEE},
  year = {2007},
  pages = {259-264},
  abstract = {The presence of error floor in low density parity check (LDPC) codes
	is of great concern for potential applications of LDPC codes to data
	storage channels, which require the error correcting code (ECC) to
	maintain the near-capacity error correcting performance at frame
	error rate as low as 10-12. In order to investigate the error floor
	of LDPC codes under partial response channels used in data storage
	systems, we propose a new estimation method combining analytical
	tools and simulation, based on the concept of trapping sets. The
	definition of trapping sets is based on the dominant error patterns
	observed in the decoding process. The goal is to accurately estimate
	the error rate in the error floor region for certain types of LDPC
	codes under the partial response channel and further extend the frame
	error rate down to 10-14 or lower. Towards this goal, we first use
	field programmable gate array (FPGA) hardware simulation to find
	the trapping sets that cause the decoding failure in the error floor
	region. For each trapping set, we extract the parameters which are
	key to the decoding failure rate caused by this trapping set. Then
	we use a much simpler in situ hardware simulation with these parameters
	to obtain the conditional decoding failure rate. By considering all
	the trapping sets we find, we obtain the overall frame error rate
	in the error floor region. The estimation results for a length -4623
	QC-LDPC code under the EPR4 channel are within 0.3 dB of the direct
	simulation results. In addition, this method allows us to estimate
	the frame error rate of a LDPC code down to 10-14 or lower.},
  doi = {10.1109/GLOCOM.2007.56},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\04410966.pdf:PDF},
  keywords = {error correction codes;field programmable gate arrays;parity check
	codes;FPGA;LDPC codes;data storage systems;error correcting code;error
	floor estimation;field programmable gate array;low density parity
	check codes;partial response channels;Data storage systems;Decoding;Error
	analysis;Error correction codes;Estimation error;Field programmable
	gate arrays;Hardware;Memory;Parity check codes;Partial response channels},
  timestamp = {2013.10.10}
}

@ARTICLE{4069025,
  author = {Hu, X. and Kumar, B. V. K. V.},
  title = {Evaluation of Low-Density Parity-Check Codes on Perpendicular Magnetic
	Recording Model},
  journal = {Magnetics, IEEE Transactions on},
  year = {2007},
  volume = {43},
  pages = {727 -732},
  number = {2},
  month = feb.,
  abstract = {Low-density parity-check (LDPC) codes have shown superior error-correcting
	performance in a variety of data storage system studies, including
	traditional longitudinal magnetic recording systems. However, perpendicular
	magnetic recording systems (of increasing interest) exhibit impairments
	different from longitudinal magnetic recording systems, and thus
	present new challenges for error-correcting codes. In this effort,
	we evaluate a structured LDPC code using a perpendicular magnetic
	recording channel model that includes impairments such as transition
	noise, nonlinear transition shift, transition percolation, and baseline
	wander (BLW). The channel model, as well as the LDPC encoder and
	the decoder are implemented in field-programmable gate array (FPGA)
	hardware. The LDPC coded system is evaluated down to bit error rate
	(BER) of 10-12 and frame error rate (FER) of 10-8. The impact of
	individual impairments on coding performance is studied separately.
	The soft output Viterbi algorithm (SOVA) + LDPC system maintains
	its superior error-correcting performance under the perpendicular
	recording channel},
  doi = {10.1109/TMAG.2006.888370},
  file = {:PDF\\Evaluation_of_Low-Density_Parity-Check_Codes_on_Perpendicular_Magnetic_Recording_Model.pdf:PDF},
  issn = {0018-9464},
  keywords = {BER;FPGA;LDPC codes;baseline wander;bit error rate;channel model;data
	storage system;error-correcting codes;field-programmable gate array;frame
	error rate;low-density parity-check codes;nonlinear transition shift;perpendicular
	magnetic recording model;soft output Viterbi algorithm;transition
	noise;transition percolation;Viterbi decoding;error correction codes;error
	statistics;field programmable gate arrays;parity check codes;perpendicular
	magnetic recording;}
}

@INPROCEEDINGS{1312605,
  author = {Xiao-Yu Hu and Fossorier, M.P.C. and Eleftheriou, E.},
  title = {On the computation of the minimum distance of low-density parity-check
	codes},
  booktitle = {Communications, 2004 IEEE International Conference on},
  year = {2004},
  volume = {2},
  pages = { 767 - 771 Vol.2},
  month = june,
  abstract = { Low-density parity-check (LDPC) codes in their broader-sense definition
	are linear codes whose parity-check matrices have fewer 1s than 0s.
	Finding their minimum distance is therefore in general an NP-hard
	problem. We propose a randomized algorithm called nearest nonzero
	codeword search (NNCS) approach to tackle this problem for iteratively
	decodable LDPC codes. The principle of the NNCS approach is to search
	codewords locally around the all-zero codeword perturbed by minimal
	noise, anticipating that the resultant nearest nonzero codewords
	will most likely contain the minimum-Hamming- weight codeword whose
	Hamming weight is equal to the minimum distance of the linear code.
	This approach has its roots in Berrou et al.'s error-impulse method
	and a form of Fossorier's list decoding for LDPC codes.},
  doi = {10.1109/ICC.2004.1312605},
  file = {:PDF\\On_the_Computation_of_the_Minimum_Distance_of_Low-Density_Parity-Check_Codes.pdf:PDF},
  issn = { },
  keywords = { LDPC codes; NP-hard problem; all-zero codeword; error-impulse method;
	linear codes; list decoding; low-density parity-check codes; minimum-Hamming-
	weight codeword; nearest nonzero codeword search; parity-check matrices;
	decoding; linear codes; optimisation; parity check codes;}
}

@ARTICLE{5605922,
  author = {Jie Huang and Lei Liu and Wuyang Zhou and Shengli Zhou},
  title = {Large-Girth Nonbinary QC-LDPC Codes of Various Lengths},
  journal = {Communications, IEEE Transactions on},
  year = {2010},
  volume = {58},
  pages = {3436 -3447},
  number = {12},
  month = {december },
  abstract = {In this paper, we construct nonbinary quasi-cyclic low-density parity-check
	(QC-LDPC) codes whose parity check matrices consist of an array of
	square sub-matrices which are either zero matrices or circulant permutation
	matrices. We propose a novel method to design the shift offset values
	of the circulant permutation sub-matrices, so that the code length
	can vary while maintaining a large girth. Extensive Monte Carlo simulations
	demonstrate that the obtained codes of a wide range of rates (from
	1/2 to 8/9) with length from 1000 to 10000 bits have very good performance
	over both AWGN and Rayleigh fading channels. Furthermore, the proposed
	method is extended to design multiple nonbinary QC-LDPC codes simultaneously
	where each individual code can achieve large girth with variable
	lengths. The proposed codes are appealing to practical adaptive systems
	where the block length and code rate need to be adaptively adjusted
	depending on traffic characteristics and channel conditions.},
  doi = {10.1109/TCOMM.2010.101210.090757},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05605922.pdf:PDF},
  issn = {0090-6778},
  keywords = {AWGN channels;Monte Carlo simulations;Rayleigh fading channels;adaptive
	systems;block length;channel conditions;circulant permutation matrices;code
	rate;large-girth nonbinary QC-LDPC codes;parity check matrices;quasicyclic
	low-density parity-check codes;square submatrices;traffic characteristics;zero
	matrices;AWGN channels;Monte Carlo methods;Rayleigh channels;cyclic
	codes;matrix algebra;parity check codes;},
  timestamp = {2011.12.07}
}

@ARTICLE{1056864,
  author = { Huang, J. and Shiva, S. and Seguin, G.},
  title = {On certain projective geometry codes (Corresp.)},
  journal = {Information Theory, IEEE Transactions on},
  year = {1984},
  volume = {30},
  pages = { 385 - 388},
  number = {2},
  month = {mar},
  abstract = { LetVbe an(n, k, d)binary projective geometry code withn = (q^{m}-1)/(q
	- 1), q = 2^{s}, andd geq [(q^{m-r}-1)/(q - 1)] + 1. This code isr-step
	majority-logic decodable. With reference to the GF(q^{m}) = {0, 1,
	alpha , alpha^{2} , cdots , alpha^{n(q-1)-1} }, the generator polynomialg(X),
	ofV, hasalpha^{nu}as a root if and only ifnuhas the formnu = i(q
	- 1)andmax_{0 leq l lt; s} W_{q}(2^{l} nu) leq (m - r - 1)(q - 1),
	whereW_{q}(x)indicates the weight of the radix-qrepresentation of
	the numberx. LetSbe the set of nonzero numbersnu, such thatalpha^{nu}is
	a root ofg(X). LetC_{1}, C_{2}, cdots, C_{nu}be the cyclotomic cosets
	such thatSis the union of these cosets. It is clear that the process
	of findingg(X)becomes simpler if we can find a representative from
	eachC_{i}, since we can then refer to a table, of irreducible factors,
	as given by, say, Peterson and Weldon. In this correspondence it
	was determined that the coset representatives for the cases ofm-r
	= 2, withs = 2, 3, andm-r=3, withs=2.},
  doi = {10.1109/TIT.1984.1056864},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\01056864.pdf:PDF},
  issn = {0018-9448},
  keywords = { Geometry coding; Majority logic decoding;},
  timestamp = {2012.02.07}
}

@ARTICLE{Huang19981,
  author = {Ming-Deh Huang and Doug Ierardi},
  title = {Counting Points on Curves over Finite Fields},
  journal = {Journal of Symbolic Computation},
  year = {1998},
  volume = {25},
  pages = {1 - 21},
  number = {1},
  abstract = {We consider the problem of counting the number of points on a plane
	curve, defined by a homogeneous polynomialF(x,y,z) &#xa0;∈&#xa0;Fq[x,y,z],
	which are rational over a ground field Fq. More precisely, we show
	that if we are given a projective plane curve C of degreen, and if
	C has only ordinary multiple points, then one can compute the number
	of Fq-rational points on C in randomized time (logq)Δwhere Δ&#xa0;=&#xa0;nO(1).
	Since our algorithm actually computes the characteristic polynomial
	of the Frobenius endomorphism on the Jacobian of C, it ,follows that
	we may also compute (1) the number of Fq-rational points on the smooth
	projective model of C, (2) the number of Fq-rational points on the
	Jacobian of C, and (3) the number of Fqm-rational points on C in
	any given finite extension Fqmof the ground field, each in a similar
	time bound.},
  doi = {10.1006/jsco.1997.0164},
  file = {:PDF\\Huang19981.pdf:PDF},
  issn = {0747-7171},
  timestamp = {2012.05.17},
  url = {http://www.sciencedirect.com/science/article/pii/S0747717197901644}
}

@ARTICLE{HuangDiaoLinAbdel-Ghaffar2012,
  author = {Qin Huang and Qiuju Diao and Shu Lin and Abdel-Ghaffar, K.},
  title = {Cyclic and Quasi-Cyclic LDPC Codes on Constrained Parity-Check Matrices
	and Their Trapping Sets},
  journal = {Information Theory, IEEE Transactions on},
  year = {2012},
  volume = {58},
  pages = {2648-2671},
  number = {5},
  month = {May},
  abstract = {This paper is concerned with construction and structural analysis
	of both cyclic and quasi-cyclic codes, particularly low-density parity-check
	(LDPC) codes. It consists of three parts. The first part shows that
	a cyclic code given by a parity-check matrix in circulant form can
	be decomposed into descendant cyclic and quasi-cyclic codes of various
	lengths and rates. Some fundamental structural properties of these
	descendant codes are developed, including the characterization of
	the roots of the generator polynomial of a cyclic descendant code.
	The second part of the paper shows that cyclic and quasi-cyclic descendant
	LDPC codes can be derived from cyclic finite-geometry LDPC codes
	using the results developed in the first part of the paper. This
	enlarges the repertoire of cyclic LDPC codes. The third part of the
	paper analyzes the trapping set structure of regular LDPC codes whose
	parity-check matrices satisfy a certain constraint on their rows
	and columns. Several classes of finite-geometry and finite-field
	cyclic and quasi-cyclic LDPC codes with large minimum distances are
	shown to have no harmful trapping sets of size smaller than their
	minimum distances. Consequently, their error-floor performances are
	dominated by their minimum distances.},
  doi = {10.1109/TIT.2011.2179842},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\06135499.pdf:PDF},
  issn = {0018-9448},
  keywords = {geometric codes;parity check codes;polynomials;constrained parity-check
	matrices;cyclic descendant code;cyclic finite-geometry LDPC codes;error-floor
	performances;finite-field cyclic codes;generator polynomial;low-density
	parity-check codes;quasi-cyclic LDPC codes;trapping sets;Arrays;Decoding;Generators;Matrix
	decomposition;Null space;Parity check codes;Polynomials;Circulant
	decomposition;cyclic code;finite-geometry (FG) code;low-density parity-check
	(LDPC) code;orthogonal parity-check sums;quasi-cyclic (QC) code;row–column
	(RC)-constrained LDPC code;trapping set},
  timestamp = {2014.03.04}
}

@INPROCEEDINGS{6033698,
  author = {Qin Huang and Qiuju Diao and Shu Lin and Abdel-Ghaffar, K.},
  title = {Trapping sets of structured LDPC codes},
  booktitle = {Information Theory Proceedings (ISIT), 2011 IEEE International Symposium
	on},
  year = {2011},
  pages = {1086-1090},
  month = {July},
  abstract = {THIS PAPER IS ELIGIBLE FOR THE STUDENT PAPER AWARD. This paper analyzes
	trapping set structure of binary regular LDPC codes whose parity-check
	matrices satisfy the constraint that no two rows (or two columns)
	have more than one place where they both have non-zero components,
	which is called row-column (RC) constraint. For a (γ,ρ)-regular LDPC
	code whose parity-check matrix satisfies the RC-constraint, its Tanner
	graph contains no (κ, τ) trapping set with size κ ≤ γ and number
	τ of odd degree check nodes less than γ. For several classes of RC-constrained
	regular LDPC codes constructed algebraically, we show that their
	Tanner graphs contain no trapping sets of sizes smaller than their
	minimum weights.},
  doi = {10.1109/ISIT.2011.6033698},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\06033698.pdf:PDF},
  issn = {2157-8095},
  keywords = {binary codes;graph theory;matrix algebra;parity check codes;RC constraint;Tanner
	graph;parity-check matrices;row-column constraint;structured LDPC
	codes;trapping sets;Charge carrier processes;Decoding;Geometry;Iterative
	decoding;Null space;Redundancy},
  timestamp = {2014.03.04}
}

@ARTICLE{1742-6596-95-1-012005,
  author = {K Hukushima and Y Iba},
  title = {A Monte Carlo algorithm for sampling rare events: application to
	a search for the Griffiths singularity},
  journal = {Journal of Physics: Conference Series},
  year = {2008},
  volume = {95},
  pages = {012005},
  number = {1},
  abstract = {We develop a recently proposed importance-sampling Monte Carlo algorithm
	for sampling rare events and quenched variables in random disordered
	systems. We apply it to a two dimensional bond-diluted Ising model
	and study the Griffiths singularity which is considered to be due
	to the existence of rare large clusters. It is found that the distribution
	of the inverse susceptibility has an exponential tail down to the
	origin which is considered the consequence of the Griffiths singularity.},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\1742-6596_95_1_012005.pdf:PDF},
  timestamp = {2012.03.06},
  url = {http://stacks.iop.org/1742-6596/95/i=1/a=012005}
}

@ARTICLE{JPSJ.65.1604,
  author = {Koji Hukushima and Koji Nemoto},
  title = {Exchange Monte Carlo Method and Application to Spin Glass Simulations},
  journal = {Journal of the Physical Society of Japan},
  year = {1996},
  volume = {65},
  pages = {1604-1608},
  number = {6},
  doi = {10.1143/JPSJ.65.1604},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\JPSJ.65.1604.pdf:PDF},
  numpages = {4},
  publisher = {The Physical Society of Japan},
  timestamp = {2014.02.25},
  url = {http://jpsj.ipap.jp/link?JPSJ/65/1604/}
}

@ARTICLE{iba_EXTENDED_ENSEMBLE_MONTE_CARLO,
  author = {YUKITO IBA},
  title = {EXTENDED ENSEMBLE MONTE CARLO},
  journal = {International Journal of Modern Physics C},
  year = {2001},
  volume = {12},
  pages = {623-656},
  number = {5},
  month = {June},
  abstract = {"Extended Ensemble Monte Carlo" is a generic term that indicates a
	set of algorithms, which are now popular in a variety of fields in
	physics and statistical information processing. Exchange Monte Carlo
	(Metropolis-Coupled Chain, Parallel Tempering), Simulated Tempering
	(Expanded Ensemble Monte Carlo) and Multicanonical Monte Carlo (Adaptive
	Umbrella Sampling) are typical members of this family. Here, we give
	a cross-disciplinary survey of these algorithms with special emphasis
	on the great flexibility of the underlying idea. In Sec. 2, we discuss
	the background of Extended Ensemble Monte Carlo. In Secs. 3, 4 and
	5, three types of the algorithms, i.e., Exchange Monte Carlo, Simulated
	Tempering, Multicanonical Monte Carlo, are introduced. In Sec. 6,
	we give an introduction to Replica Monte Carlo algorithm by Swendsen
	and Wang. Strategies for the construction of special-purpose extended
	ensembles are discussed in Sec. 7. We stress that an extension is
	not necessary restricted to the space of energy or temperature. Even
	unphysical (unrealizable) configurations can be included in the ensemble,
	if the resultant fast mixing of the Markov chain offsets the increasing
	cost of the sampling procedure. Multivariate (multicomponent) extensions
	are also useful in many examples. In Sec. 8, we give a survey on
	extended ensembles with a state space whose dimensionality is dynamically
	varying. In the appendix, we discuss advantages and disadvantages
	of three types of extended ensemble algorithms.},
  doi = {10.1142/S0129183101001912},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\iba10_1142_S0129183101001912.pdf:PDF},
  keywords = {Extended Ensemble; Exchange Monte Carlo; Simulated Tempering; Multicanonical
	Monte Carlo; Replica Monte Carlo; Complexity Ladder; Bridge; Multivariate
	Extension},
  timestamp = {2012.03.14},
  url = {http://www.worldscinet.com/ijmpc/12/1205/S0129183101001912.html}
}

@ARTICLE{JPSJ.77.103801,
  author = {Yukito Iba and Koji Hukushima},
  title = {Testing Error Correcting Codes by Multicanonical Sampling of Rare
	Events},
  journal = {Journal of the Physical Society of Japan},
  year = {2008},
  volume = {77},
  pages = {103801},
  number = {10},
  doi = {10.1143/JPSJ.77.103801},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\JPSJ.77.103801.pdf:PDF;:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\0709.2578v2.pdf:PDF},
  numpages = {3},
  publisher = {The Physical Society of Japan},
  timestamp = {2012.03.06},
  url = {http://jpsj.ipap.jp/link?JPSJ/77/103801/}
}

@INPROCEEDINGS{5984637,
  author = {Iba, Y. and Tsunoda, K. and Lee, Y. M. and Yoshida, C. and Noshiro,
	H. and Takahashi, A. and Yamazaki, Y. and Nakabayashi, M. and Hatada,
	A. and Aoki, M. and Sugii, T.},
  title = {Strain-engineering for high-performance STT-MRAM},
  booktitle = {VLSI Technology (VLSIT), 2011 Symposium on},
  year = {2011},
  pages = {212 -213},
  month = {june},
  abstract = {Strain-engineering using the inverse magnetostrictive effect has been
	performed to improve the performance of spin transfer torque magnetoresistance
	random access memory (STT-MRAM). The thermal stability factor E/kBT
	has been enhanced by 40% without increasing a switching current by
	controlling the process-induced stress in a free layer in MTJ (magnetic
	tunnel junctions) with mechanically engineered manufacturing steps.},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05984637.pdf:PDF},
  issn = {0743-1562},
  timestamp = {2011.08.28}
}

@ARTICLE{Ielmini20021749,
  author = {Daniele Ielmini and Alessandro S. Spinelli and Andrea L. Lacaita
	and Alberto Modelli},
  title = {Modeling of anomalous SILC in flash memories based on tunneling at
	multiple defects},
  journal = {Solid-State Electronics},
  year = {2002},
  volume = {46},
  pages = {1749 - 1756},
  number = {11},
  doi = {DOI: 10.1016/S0038-1101(02)00144-2},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\Ielmini20021749.pdf:PDF},
  issn = {0038-1101},
  keywords = {Flash memory},
  timestamp = {2011.08.21},
  url = {http://www.sciencedirect.com/science/article/pii/S0038110102001442}
}

@ARTICLE{jfs280320,
  author = {Yasutaka Ihara},
  title = {Some remarks on the number of rational points of algebratic curves
	over finite fields},
  journal = {Journal of the Faculty of Science, the University of Tokyo. Sect.
	1 A, Mathematics},
  year = {1982},
  volume = {28},
  pages = {721-724},
  number = {3},
  month = {February},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\jfs280320.pdf:PDF},
  issn = {00408980},
  publisher = {Faculty of Science, The University of Tokyo},
  timestamp = {2011.07.06},
  url = {http://hdl.handle.net/2261/6319}
}

@ARTICLE{5601690,
  author = {Soojun Im and Dongkun Shin},
  title = {Flash-Aware RAID Techniques for Dependable and High-Performance Flash
	Memory SSD},
  journal = {Computers, IEEE Transactions on},
  year = {2011},
  volume = {60},
  pages = {80 -92},
  number = {1},
  month = {jan. },
  abstract = {Solid-state disks (SSDs), which are composed of multiple NAND flash
	chips, are replacing hard disk drives (HDDs) in the mass storage
	market. The performances of SSDs are increasing due to the exploitation
	of parallel I/O architectures. However, reliability remains as a
	critical issue when designing a large-scale flash storage. For both
	high performance and reliability, Redundant Arrays of Inexpensive
	Disks (RAID) storage architecture is essential to flash memory SSD.
	However, the parity handling overhead for reliable storage is significant.
	We propose a novel RAID technique for flash memory SSD for reducing
	the parity updating cost. To reduce the number of write operations
	for the parity updates, the proposed scheme delays the parity update
	which must accompany each data write in the original RAID technique.
	In addition, by exploiting the characteristics of flash memory, the
	proposed scheme uses the partial parity technique to reduce the number
	of read operations required to calculate a parity. We evaluated the
	performance improvements using a RAID-5 SSD simulator. The proposed
	techniques improved the performance of the RAID-5 SSD by 47 percent
	and 38 percent on average in comparison to the original RAID-5 technique
	and the previous delayed parity updating technique, respectively.},
  doi = {10.1109/TC.2010.197},
  file = {:PDF\\05601690.pdf:PDF},
  issn = {0018-9340},
  keywords = {NAND flash chips;RAID storage architecture;RAID-5 SSD simulator;flash
	memory SSD;flash-aware RAID techniques;hard disk drives;mass storage
	market;parallel I/O architectures;redundant arrays of inexpensive
	disks;solid-state disks;RAID;disc drives;flash memories;hard discs;},
  timestamp = {2011.05.20}
}

@INPROCEEDINGS{5556126,
  author = {Ishigaki, T. and Kawahara, T. and Takemura, R. and Ono, K. and Ito,
	K. and Matsuoka, H. and Ohno, H.},
  title = {A multi-level-cell spin-transfer torque memory with series-stacked
	magnetotunnel junctions},
  booktitle = {VLSI Technology (VLSIT), 2010 Symposium on},
  year = {2010},
  pages = {47 -48},
  month = june,
  abstract = {We first report a multi-level-cell (MLC) spin-transfer torque memory
	(SPRAM) with series-connected magnetotunnel junctions (MTJs). The
	series MTJs (with different areas) show multi-level resistances by
	a combination of their magnetization directions. A four-level operation
	by spin-transfer-torque writing was experimentally demonstrated.
	A scheme for the write/read operation of the MLC SPRAM was also presented.},
  doi = {10.1109/VLSIT.2010.5556126},
  file = {:PDF\\A_Multi-Level-Cell_Spin-Transfer_Torque_Memory_with_Series-Stacked_Magnetotunnel_Junction.pdf:PDF},
  keywords = {SPRAM;magnetization directions;multilevel resistances;multilevel-cell
	spin-transfer torque memory;series-stacked magnetotunnel junctions;write/read
	operation;MRAM devices;magnetic tunnelling;magnetisation;}
}

@ARTICLE{MERLTR-2001-16,
  author = {J.S. Yedidia, W. T. Freeman, and Y. Weiss},
  title = {Bethe Free Energy, Kikuchi Approximations, and Belief Propagation},
  journal = {Technical Report of Mitsubishi Electric Research Laboratory},
  year = {2001},
  volume = {2001},
  pages = {16},
  file = {:PDF\\MERLTR2001-16.pdf:PDF},
  timestamp = {2012.10.15}
}

@ARTICLE{4840630,
  author = {Xueqin Jiang and Moon Ho Lee},
  title = {Large Girth Non-Binary LDPC Codes Based on Finite Fields and Euclidean
	Geometries},
  journal = {Signal Processing Letters, IEEE},
  year = {2009},
  volume = {16},
  pages = {521 -524},
  number = {6},
  month = {june },
  abstract = {This letter presents an approach to the construction of non-binary
	low-density parity-check (LDPC) codes based on alpha-multiplied circulant
	permutation matrices and hyperplanes of two different dimensions
	in Euclidean geometries. Codes constructed by this method have large
	girth and high binary column weight when the order of Galois field
	is high. Simulation results show that these codes perform very well
	with fast Fourier transform (FFT) based sum-product algorithm (SPA).},
  doi = {10.1109/LSP.2009.2016830},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\04840630.pdf:PDF},
  issn = {1070-9908},
  keywords = {Galois field;alpha-multiplied circulant permutation matrices;euclidean
	geometries;fast Fourier transform;large girth nonbinary LDPC codes;nonbinary
	low-density parity-check codes;sum-product algorithm;Galois fields;binary
	codes;fast Fourier transforms;matrix algebra;parity check codes;},
  timestamp = {2011.12.06}
}

@ARTICLE{4939346,
  author = {Xueqin Jiang and Moon Ho Lee},
  title = {Large girth quasi-cyclic LDPC codes based on the chinese remainder
	theorem},
  journal = {Communications Letters, IEEE},
  year = {2009},
  volume = {13},
  pages = {342 -344},
  number = {5},
  month = {may },
  abstract = {In this letter, we consider two problems associated with quasi-cyclic
	low-density parity-check (QC-LDPC) codes. The first is how to extend
	the code length of a QC-LDPC code without reducing the girth. The
	second is how to design a QCLDPC code with a prescribed girth easily.
	We deal with these two problems by using a combining method of QC-LDPC
	codes via the Chinese Remainder Theorem (CRT). Codes constructed
	with our proposed method have flexible code lengths, flexible code
	rates and large girth. Simulation results show that they perform
	very well with the iterative decoding.},
  doi = {10.1109/LCOMM.2009.082115},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\04939346.pdf:PDF},
  issn = {1089-7798},
  keywords = {LDPC codes;chinese remainder theorem;iterative decoding;large girth
	codes;low-density parity check codes;quasi-cyclic codes;cyclic codes;iterative
	decoding;parity check codes;},
  timestamp = {2011.12.06}
}

@BOOK{Sarah_Johnson_IterativeErrorCorrection,
  title = {Iterative Error Correction: Turbo, Low-Density Parity-Check and Repeat-Accumulate
	Codes},
  publisher = {Cambridge University Press},
  year = {2009},
  author = {Sarah Johnson},
  month = {November},
  abstract = {Iterative error correction codes have found widespread application
	in cellular communications, digital video broadcasting and wireless
	LANs. This self-contained treatment of iterative error correction
	presents all the key ideas needed to understand, design, implement
	and analyse these powerful codes. Turbo, low-density parity-check,
	and repeat-accumulate codes are given equal, detailed coverage, with
	precise presentations of encoding and decoding procedures. Worked
	examples are integrated into the text to illuminate each new idea
	and pseudo-code is included for important algorithms to facilitate
	the reader's development of the techniques described. For each subject,
	the treatment begins with the simplest case before generalizing.
	There is also coverage of advanced topics such as density-evolution
	and EXIT charts for those readers interested in gaining a deeper
	understanding of the field. This text is ideal for graduate students
	in electrical engineering and computer science departments, as well
	as practitioners in the communications industry.},
  timestamp = {2013.02.05}
}

@ARTICLE{MERLTR2000-26,
  author = {Jonathan S. Yedidia, William T. Freeman, and Yair Weiss},
  title = {Generalized Belief Propagation},
  journal = {Technical Report of Mitsubishi Electric Research Laboratory},
  year = {2000},
  volume = {2000},
  pages = {26},
  file = {:PDF\\MERLTR2000-26.pdf:PDF},
  timestamp = {2012.10.15}
}

@ARTICLE{1394085,
  author = {Joyner, David and Miller, Robert},
  title = {SAGE and coding theory(abstract only)},
  journal = {ACM Communication in Compter Algebra},
  year = {2008},
  volume = {42},
  pages = {74--78},
  number = {1-2},
  address = {New York, NY, USA},
  doi = {http://doi.acm.org/10.1145/1394042.1394085},
  issn = {1932-2240},
  publisher = {ACM}
}

@ARTICLE{4137888,
  author = {Kamiya, N.},
  title = {High-Rate Quasi-Cyclic Low-Density Parity-Check Codes Derived From
	Finite Affine Planes},
  journal = {Information Theory, IEEE Transactions on},
  year = {2007},
  volume = {53},
  pages = {1444 -1459},
  number = {4},
  month = {april },
  abstract = {This paper shows that several attractive classes of quasi-cyclic (QC)
	low-density parity-check (LDPC) codes can be obtained from affine
	planes over finite fields. One class of these consists of duals of
	one-generator QC codes. Presented here for codes contained in this
	class are the exact minimum distance and a lower bound on the multiplicity
	of the minimum-weight codewords. Further, it is shown that the minimum
	Hamming distance of a code in this class is equal to its minimum
	additive white Gaussian noise (AWGN) pseudoweight. Also discussed
	is a class consisting of codes from circulant permutation matrices,
	and an explicit formula for the rank of the parity-check matrix is
	presented for these codes. Additionally, it is shown that each of
	these codes can be identified with a code constructed from a constacyclic
	maximum distance separable code of dimension 2. The construction
	is similar to the derivation of Reed-Solomon (RS)-based LDPC codes
	presented by Chen and Djurdjevic Experimental results show that a
	number of high rate QC-LDPC codes with excellent error performance
	are contained in these classes},
  doi = {10.1109/TIT.2007.892770},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\04137888.pdf:PDF},
  issn = {0018-9448},
  keywords = {AWGN;additive white Gaussian noise;finite affine planes;high-rate
	quasicyclic code;low-density parity-check code;minimum Hamming distance;parity-check
	matrix;AWGN;Hamming codes;cyclic codes;parity check codes;},
  timestamp = {2011.10.11}
}

@ARTICLE{springerlink:10.1007_s10623-005-6685-6,
  author = {Kamiya, Norifumi and Fossorier, Marc},
  title = {Quasi-Cyclic Codes from a Finite Affine Plane},
  journal = {Designs, Codes and Cryptography},
  year = {2006},
  volume = {38},
  pages = {311-329},
  note = {10.1007/s10623-005-6685-6},
  abstract = {Finite geometry codes are defined as the null spaces of the incidence
	matrices of points and flats in finite geometries. In this paper,
	we investigate the incidence matrix of points other than the origin
	and lines not passing through the origin in the affine plane AG (2,2
	s ), and we present two classes of quasi-cyclic codes derived from
	submatrices of the point-line incidence matrix. We also investigate
	the 2-ranks of those submatrices.},
  affiliation = {NEC Corporation Internet Systems Research Laboratories 1753 Shimonumabe
	Nakahara, Kawasaki Kanagawa 211-8666 Japan 1753 Shimonumabe Nakahara,
	Kawasaki Kanagawa 211-8666 Japan},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\springerlink10.1007_s10623-005-6685-6.pdf:PDF},
  issn = {0925-1022},
  issue = {3},
  keyword = {Mathematics and Statistics},
  publisher = {Springer Netherlands},
  timestamp = {2011.10.13},
  url = {http://dx.doi.org/10.1007/s10623-005-6685-6}
}

@INPROCEEDINGS{523390,
  author = {Rom-Shen Kao and Taylor, F.J.},
  title = {A Fast Galois-Field Transform Algorithm Using Normal Bases},
  booktitle = {Signals, Systems and Computers, 1990 Conference Record Twenty-Fourth
	Asilomar Conference on},
  year = {1990},
  volume = {1},
  pages = {511},
  month = {nov},
  abstract = {Not available},
  doi = {10.1109/ACSSC.1990.523390},
  file = {:PDF\\00523390.pdf:PDF},
  issn = {1058-6393},
  keywords = {Computational complexity;Computer architecture;Discrete Fourier transforms;Equations;Fourier
	transforms;Galois fields;Hardware;Polynomials;Power engineering computing;Very
	large scale integration;},
  timestamp = {2013.01.21}
}

@ARTICLE{6259854,
  author = {Karimi, M. and Banihashemi, A.H.},
  title = {Efficient Algorithm for Finding Dominant Trapping Sets of LDPC Codes},
  journal = {Information Theory, IEEE Transactions on},
  year = {2012},
  volume = {58},
  pages = {6942-6958},
  number = {11},
  abstract = {This paper presents an efficient algorithm for finding the dominant
	trapping sets of a low-density parity-check (LDPC) code. The algorithm
	can be used to estimate the error floor of LDPC codes or as a tool
	to design LDPC codes with low error floors. For regular codes, the
	algorithm is initiated with a set of short cycles as the input. For
	irregular codes, in addition to short cycles, variable nodes with
	low degree and cycles with low approximate cycle extrinsic message
	degree (ACE) are also used as the initial inputs. The initial inputs
	are then expanded recursively to dominant trapping sets of increasing
	size. At the core of the algorithm lies the analysis of the graphical
	structure of dominant trapping sets and the relationship of such
	structures to short cycles, low-degree variable nodes, and cycles
	with low ACE. The algorithm is universal in the sense that it can
	be used for an arbitrary graph and that it can be tailored to find
	a variety of graphical objects, such as absorbing sets and Zyablov-Pinsker
	trapping sets, known to dominate the performance of LDPC codes in
	the error floor region over different channels and for different
	iterative decoding algorithms. Simulation results on several LDPC
	codes demonstrate the accuracy and efficiency of the proposed algorithm.
	In particular, the algorithm is significantly faster than the existing
	search algorithms for dominant trapping sets.},
  doi = {10.1109/TIT.2012.2205663},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\06259854.pdf:PDF},
  issn = {0018-9448},
  keywords = {graph theory;iterative decoding;parity check codes;LDPC code;Zyablov-Pinsker
	trapping sets;arbitrary graph;different iterative decoding algorithms;dominant
	trapping sets;irregular codes;low ACE;low error floors;low-degree
	variable nodes;low-density parity-check codes;regular codes;search
	algorithms;Accuracy;Algorithm design and analysis;Charge carrier
	processes;Complexity theory;Decoding;Iterative decoding;Absorbing
	sets;approximate cycle extrinsic message degree (ACE);dominant trapping
	sets;elementary trapping sets;error floor;error floor estimation;low-density
	parity-check (LDPC) codes;short cycles;trapping sets},
  timestamp = {2013.12.17}
}

@ARTICLE{1054640,
  author = { Kasami, T. and Shu Lin},
  title = {On majority-logic decoding for duals of primitive polynomial codes},
  journal = {Information Theory, IEEE Transactions on},
  year = {1971},
  volume = {17},
  pages = { 322 - 331},
  number = {3},
  month = may,
  abstract = { The class of polynomial codes introduced by Kasami et al. has considerable
	inherent algebraic and geometric structure. It has been shown that
	this class of codes and their dual codes contain many important classes
	of cyclic codes as subclasses, such as BCH codes, Reed-Solomon codes,
	generalized Reed-Muller codes, projective geometry codes, and Euclidean
	geometry codes. The purpose of this paper is to investigate further
	properties of polynomial codes and their duals. First, majority-logic
	decoding for the duals of certain primitive polynomial codes is considered.
	Two methods of forming nonorthogonal parity-check sums are presented.
	Second, the maximality of Euclidean geometry codes is proved. The
	roots of the generator polynomial of an Euclidean geometry code are
	specified.},
  doi = {10.1109/TIT.1971.1054640},
  file = {:PDF\\On_Majority-Logic_Decoding_for_Duals_of_Primitive_polynomial_Codes.pdf:PDF},
  issn = {0018-9448},
  keywords = { Dual codes; Geometry codes; Majority logic decoding; Polynomial codes;}
}

@ARTICLE{1054127,
  author = { Kasami, T. and Shu Lin and Peterson, W.},
  title = {New generalizations of the Reed-Muller codes--I: Primitive codes},
  journal = {Information Theory, IEEE Transactions on},
  year = {1968},
  volume = {14},
  pages = { 189 - 199},
  number = {2},
  month = mar,
  abstract = { First it is shown that all binary Reed-Muller codes with one digit
	dropped can be made cyclic by rearranging the digits. Then a natural
	generalization to the nonbinary case is presented, which also includes
	the Reed-Muller codes and Reed-Solomon codes as special cases. The
	generator polynomial is characterized and the minimum weight is established.
	Finally, some results on weight distribution are given.},
  doi = {10.1109/TIT.1968.1054127},
  file = {:C\:\\Users\\Public\\Documents\\My eBooks\\refereces\\PDF\\01054127.pdf:PDF},
  issn = {0018-9448},
  keywords = { Cyclic codes; Reed-Muller codes;}
}

@ARTICLE{1054226,
  author = { Kasami, T. and Shu Lin and Peterson, W.},
  title = {Polynomial codes},
  journal = {Information Theory, IEEE Transactions on},
  year = {1968},
  volume = {14},
  pages = { 807 - 814},
  number = {6},
  month = nov,
  abstract = { A class of cyclic codes is introduced by a polynomial approach that
	is an extension of the Mattson-Solomon method and of the Muller method.
	This class of codes contains several important classes of codes as
	subclasses, namely, BCH codes, Reed-Solomon codes, generalized primitive
	Reed-Muller codes, and finite geometry codes. Certain fundamental
	properties of this class of codes are derived. Some subclasses are
	shown to be majority-logic decodable.},
  doi = {10.1109/TIT.1968.1054226},
  file = {:PDF\\Polynomial_Codes.pdf:PDF},
  issn = {0018-9448},
  keywords = { BCH codes; Geometry codes; Polynomial codes; Reed-Muller codes; Reed-Solomon
	codes;}
}

@ARTICLE{PhysRev.76.1232,
  author = {Kaufman, Bruria},
  title = {Crystal Statistics. II. Partition Function Evaluated by Spinor Analysis},
  journal = {Phys. Rev.},
  year = {1949},
  volume = {76},
  pages = {1232--1243},
  month = {Oct},
  doi = {10.1103/PhysRev.76.1232},
  file = {:PDF\\PhysRev.76.1232.pdf:PDF},
  issue = {8},
  publisher = {American Physical Society},
  timestamp = {2012.07.25},
  url = {http://link.aps.org/doi/10.1103/PhysRev.76.1232}
}

@ARTICLE{PhysRev.76.1244,
  author = {Kaufman, Bruria and Onsager, Lars},
  title = {Crystal Statistics. III. Short-Range Order in a Binary Ising Lattice},
  journal = {Phys. Rev.},
  year = {1949},
  volume = {76},
  pages = {1244--1252},
  month = {Oct},
  doi = {10.1103/PhysRev.76.1244},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\PhysRev.76.1244.pdf:PDF},
  issue = {8},
  publisher = {American Physical Society},
  timestamp = {2013.06.25},
  url = {http://link.aps.org/doi/10.1103/PhysRev.76.1244}
}

@ARTICLE{1207365,
  author = {Kavcic, A. and Xiao Ma and Mitzenmacher, M.},
  title = {Binary intersymbol interference channels: Gallager codes, density
	evolution, and code performance bounds},
  journal = {Information Theory, IEEE Transactions on},
  year = {2003},
  volume = {49},
  pages = { 1636 - 1652},
  number = {7},
  month = {july},
  abstract = {We study the limits of performance of Gallager codes (low-density
	parity-check (LDPC) codes) over binary linear intersymbol interference
	(ISI) channels with additive white Gaussian noise (AWGN). Using the
	graph representations of the channel, the code, and the sum-product
	message-passing detector/decoder, we prove two error concentration
	theorems. Our proofs expand on previous work by handling complications
	introduced by the channel memory. We circumvent these problems by
	considering not just linear Gallager codes but also their cosets
	and by distinguishing between different types of message flow neighborhoods
	depending on the actual transmitted symbols. We compute the noise
	tolerance threshold using a suitably developed density evolution
	algorithm and verify, by simulation, that the thresholds represent
	accurate predictions of the performance of the iterative sum-product
	algorithm for finite (but large) block lengths. We also demonstrate
	that for high rates, the thresholds are very close to the theoretical
	limit of performance for Gallager codes over ISI channels. If C denotes
	the capacity of a binary ISI channel and if Ci.i.d. denotes the maximal
	achievable mutual information rate when the channel inputs are independent
	and identically distributed (i.i.d.) binary random variables (Ci.i.d.
	le;C), we prove that the maximum information rate achievable by the
	sum-product decoder of a Gallager (coset) code is upper-bounded by
	Ci.i.d.. The last topic investigated is the performance limit of
	the decoder if the trellis portion of the sum-product algorithm is
	executed only once; this demonstrates the potential for trading off
	the computational requirements and the performance of the decoder.},
  doi = {10.1109/TIT.2003.813563},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\01207365.pdf:PDF},
  issn = {0018-9448},
  keywords = { AWGN; Gallager codes; LDPQ codes; additive white Gaussian noise;
	binary ISI channel; binary intersymbol interference channels; binary
	linear intersymbol interference channels; channel inputs; channel
	memory; code performance bounds; cosets; density evolution; error
	concentration theorems; graph representations; i.i.d. binary random
	variables; independent and identically distributed binary random
	variables; iterative sum-product algorithm; low-density parity-check
	codes; maximal achievable mutual information rate; maximum information
	rate achievable; message flow neighborhoods; noise tolerance threshold;
	sum-product decoder; sum-product message-passing detector/decoder;
	AWGN channels; binary codes; channel capacity; channel coding; decoding;
	graph theory; intersymbol interference; linear codes; message passing;},
  timestamp = {2011.11.24}
}

@INPROCEEDINGS{5335615,
  author = {Kawahara, T.},
  title = {Spin-transfer torque RAM: A road to universal memory},
  booktitle = {Modern Problems of Nanoelectronics, Micro- and Nanosystem Technologies,
	2009. INTERNANO 2009. International School and Seminar on},
  year = {2009},
  pages = {160 -185},
  month = oct,
  abstract = {A collection of slides from the author's conference presentation is
	given. The title is "Spin-transfer torque RAM: a road to universal
	memory".},
  doi = {10.1109/INTERNANO.2009.5335615},
  file = {:PDF\\Spin-Transfer_Torque_RAM_A_road_to_universal_memory.pdf:PDF},
  keywords = {RAM;random-access storage;universal memory;random-access storage;}
}

@INPROCEEDINGS{6026357,
  author = {Jonghong Kim and Junhee Cho and Wonyong Sung},
  title = {A high-speed layered min-sum LDPC decoder for error correction of
	NAND Flash memories},
  booktitle = {Circuits and Systems (MWSCAS), 2011 IEEE 54th International Midwest
	Symposium on},
  year = {2011},
  pages = {1 -4},
  month = {aug},
  abstract = {NAND Flash memory controllers need to equip strong and high speed
	error correction blocks as the cell size scales down and multi-level
	cell technology is employed. We have developed an LDPC (low-density
	parity-check) decoder for NAND Flash memory error correction, and
	implemented it using a layered min-sum decoding architecture. A shortened
	(69615, 66897) regular EG-LDPC code that has the code rate of 96%
	is used, which has a good minimum distance and quasi-cyclic structure.
	In order to increase the decoding throughput and reduce the chip
	area, the word-length reduction of variable-to-check messages, compression
	of the check-to-variable information, and pipelined parallel architecture
	are employed. Furthermore, fixed-point arithmetic optimization of
	node update processing units is also conducted to mitigate the quantization
	error, thereby enhances the error performance of the decoder. The
	synthesis and simulation results show that the SRAM area storing
	check-to-variable messages is much reduced, which leads to 38% saving
	in hardware area compared to the non-optimized serial architecture,
	and the design also exhibits a good error performance that is close
	to that of the floating-point implementation. The decoder can achieve
	the maximum decoding throughput of 6.24Gb/s and occupies the chip
	area of 48m m2 with 0.13um CMOS process.},
  doi = {10.1109/MWSCAS.2011.6026357},
  file = {:PDF\\06026357.pdf:PDF},
  issn = {1548-3746},
  keywords = {CMOS process;NAND Flash memory controllers;SRAM area;check-to-variable
	information compression;check-to-variable messages;fixed-point arithmetic
	optimization;floating-point implementation;high-speed error correction
	blocks;high-speed layered min-sum LDPC decoder;low density parity
	check decoder;multilevel cell technology;node update processing units;pipelined
	parallel architecture;quantization error mitigation;regular EG-LDPC
	code;size 0.13 mum;variable-to-check messages;word-length reduction;CMOS
	logic circuits;CMOS memory circuits;NAND circuits;SRAM chips;error
	correction codes;flash memories;parallel architectures;parity check
	codes;},
  timestamp = {2012.10.22}
}

@INPROCEEDINGS{6364973,
  author = {Jonghong Kim and Dong-hwan Lee and Wonyong Sung},
  title = {Performance of rate 0.96 (68254, 65536) EG-LDPC code for NAND Flash
	memory error correction},
  booktitle = {Communications (ICC), 2012 IEEE International Conference on},
  year = {June},
  pages = {7029-7033},
  abstract = {As the process technology scales down and the number of bits per cell
	increases, NAND Flash memory is more prone to bit errors. In this
	paper, we employ a rate-0.96 (68254, 65536) Euclidean geometry (EG)
	low-density parity-check (LDPC) code for NAND Flash memory error
	correction, and evaluate the performance under binary input (BI)
	additive white Gaussian noise (AWGN) and NAND Flash memory channels.
	The performance effect of output signal quantization is also studied.
	We show the strategies for determining the optimum quantization boundaries
	and computing the quantized log-likelihood ratio (LLR) for the NAND
	Flash channel model that is approximated as a mixture of Gaussian
	distributions. Simulation results show that the error performance
	with the NAND Flash memory channel is much different from that with
	the BI-AWGN channel. Since the distribution of NAND Flash memory
	output signal is not stationary, it is important to accurately assess
	the stochastic distribution of the signal for optimum sensing.},
  doi = {10.1109/ICC.2012.6364973},
  file = {:PDF\\06364973.pdf:PDF},
  issn = {1550-3607},
  keywords = {Gaussian distribution;NAND circuits;error correction codes;flash memories;geometry;parity
	check codes;signal processing;BI AWGN channel;EG;Euclidean geometry
	low-density parity-check code;Gaussian distributions;LLR;NAND Flash
	memory error correction;binary input additive white Gaussian noise;optimum
	quantization boundaries;optimum sensing;output signal quantization;quantized
	log-likelihood ratio;rate 0.96 (68254, 65536) EG-LDPC code;stochastic
	distribution;Bit error rate;Error correction codes;Flash memory;Parity
	check codes;Quantization;Sensors;Threshold voltage;LDPC codes;NAND
	Flash memory;quantization},
  timestamp = {2013.03.11}
}

@ARTICLE{4276926,
  author = {Sunghwan Kim and Jong-Seon No and Habong Chung and Dong-Joon Shin},
  title = {Quasi-Cyclic Low-Density Parity-Check Codes With Girth Larger Than
	12},
  journal = {Information Theory, IEEE Transactions on},
  year = {2007},
  volume = {53},
  pages = {2885 -2891},
  number = {8},
  month = {aug},
  abstract = {A quasi-cyclic (QC) low-density parity-check (LDPC) code can be viewed
	as the protograph code with circulant permutation matrices (or circulants).
	In this correspondence, we find all the subgraph patterns of protographs
	of QC LDPC codes having inevitable cycles of length 2i, i = 6, 7,
	8, 9,10, i.e., the cycles that always exist regardless of the shift
	values of circulants. It is also derived that if the girth of the
	protograph is 2g, g gt; 2, its protograph code cannot have the inevitable
	cycles of length smaller than 6g. Based on these subgraph patterns,
	we propose new combinatorial construction methods of the protographs,
	whose protograph codes can have girth larger than or equal to 14
	or 18. We also propose a couple of shift value assigning rules for
	circulants of a QC LDPC code guaranteeing the girth 14.},
  doi = {10.1109/TIT.2007.901193},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\04276926.pdf:PDF},
  issn = {0018-9448},
  keywords = {circulant permutation matrices;combinatorial construction method;protograph
	code;quasi-cyclic low-density parity-check codes;subgraph patterns;graph
	theory;matrix algebra;parity check codes;},
  timestamp = {2011.12.06}
}

@INPROCEEDINGS{1010932,
  author = {Sungwook Kim and Sobelman, G.E. and Jaekyun Moon},
  title = {Parallel VLSI architectures for a class of LDPC codes},
  booktitle = {Circuits and Systems, 2002. ISCAS 2002. IEEE International Symposium
	on},
  year = {2002},
  volume = {2},
  pages = { II-93 - II-96 vol.2},
  abstract = {This paper presents high-performance encoder and decoder architectures
	for a class of low density parity check (LDPC) codes. The codes considered
	here are based on the parallelly concatenated parity check encoder
	structure. A major advantage of these codes is that the generator
	matrix and the parity check matrix are both sparse, which leads to
	efficient VLSI implementations for the encoder and the decoder. Our
	designs use 6-bit quantization with a code rate of 8/9 and a block
	size of 576 bits. An evaluation of the speed and hardware complexity
	is given, and simulation results for the bit error rate are obtained},
  doi = {10.1109/ISCAS.2002.1010932},
  file = {:PDF\\01010932.pdf:PDF},
  keywords = {LDPC codes; VLSI architectures; bit error rate; decoder architecture;
	generator matrix; low density parity check codes; parallel architectures;
	parallelly concatenated parity check encoder; parity check matrix;
	quantization; VLSI; concatenated codes; decoding; error statistics;
	integrated circuit design; parallel architectures; quantisation (signal);
	sparse matrices;},
  timestamp = {2011.04.22}
}

@INPROCEEDINGS{5984614,
  author = {Kim, Wanki and Park, Sung Il and Zhang, Zhiping and Yang-Liauw, Young
	and Sekar, Deepak and Wong, H.-S. Philip and Wong, S. Simon},
  booktitle = {VLSI Technology (VLSIT), 2011 Symposium on},
  pages = {22 -23},
  month = {june},
  abstract = {Nitrogen-doped AlOX Resistive RAM has been integrated on CMOS. The
	memory cell requires no forming, and sub- #x03BC;A programming currents.
	The cell is capable of multi-bit storage, reliable for over 105 switching
	cycles and 10 years retention at 125 #x00B0;C.},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05984614.pdf:PDF},
  issn = {0743-1562},
  timestamp = {2011.08.28}
}

@INPROCEEDINGS{5984636,
  author = {Kim, Y. and Oh, S. C. and Lim, W. C. and Kim, J. H. and Kim, W. J.
	and Jeong, J. H. and Shin, H. J. and Kim, K. W. and Kim, K. S. and
	Park, J. H. and Park, S. H. and Kwon, H. and Ah, K.H. and Lee, J.
	E. and Park, S. O. and Choi, S. and Kang, H. K. and Chung, C.},
  booktitle = {VLSI Technology (VLSIT), 2011 Symposium on},
  pages = {210 -211},
  month = {june},
  abstract = {28nm MTJ for 8 #x223C;16Gb MRAM device has been successfully integrated
	with special patterning amp; etch technique. Resistance (R) separation
	between high and low R states was 15.2 #x03C3;, comparable to that
	for 80nm MTJ cells. Thermal stability factor ( #x0394;) followed
	prediction fairly well, and MTJ with free layer (FL) of 25 #x00C5;
	and aspect ratio (AR) of 3 showed #x0394; of 56. In order to realize
	sub-30nm MRAM device, a novel FL with substantially low critical
	current density (Jc) or revolutionary MTJ scheme needs to be developed.},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05984636.pdf:PDF},
  issn = {0743-1562},
  timestamp = {2011.08.28}
}

@INPROCEEDINGS{5984628,
  author = {Kim, Young-Bae and Lee, Seung Ryul and Lee, Dongsoo and Lee, Chang
	Bum and Chang, Man and Hur, Ji Hyun and Lee, Myoung-Jae and Park,
	Gyeong-Su and Kim, Chang Jung and Chung, U-In and Yoo, In-Kyeong
	and Kim, Kinam},
  title = {Bi-layered RRAM with unlimited endurance and extremely uniform switching},
  booktitle = {VLSI Technology (VLSIT), 2011 Symposium on},
  year = {2011},
  pages = {52 -53},
  month = {june},
  abstract = {We demonstrate resistive random access memory (RRAM) architecture
	with bi-layered switching element for reliable resistive switching
	memory. Based on the modulated Schottky barrier modeling, several
	key functions to achieve a realiable bipolar switching property are
	extracted. Our device shows an excellent memory performance such
	as enduracne of 1011 cycles at 30ns, data retention of #x003E;104s
	at 200 #x00B0;C, and calculated bit error rate below 10 #x2212;11.},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05984628.pdf:PDF},
  issn = {0743-1562},
  timestamp = {2011.08.28}
}

@ARTICLE{Kirchner2010,
  author = {Kirchner, Stefan},
  title = {Spin Path Integrals, Berry Phase, and the Quantum Phase Transition
	in the Sub-Ohmic Spin-Boson Model},
  journal = {Journal of Low Temperature Physics},
  year = {2010},
  volume = {161},
  pages = {282-298},
  number = {1-2},
  doi = {10.1007/s10909-010-0193-4},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\raey.pdf:PDF},
  issn = {0022-2291},
  keywords = {Quantum criticality; Quantum phase transition; Quantum-to-classical
	mapping; Spin-boson model; Spin coherent states; Spin path integrals;
	Berry phase},
  language = {English},
  publisher = {Springer US},
  timestamp = {2013.10.10},
  url = {http://dx.doi.org/10.1007/s10909-010-0193-4}
}

@ARTICLE{shiftregistersynthesis,
  author = {Andrew Klapper and Jinzhong Xu},
  title = {Register synthesis for algebraic feedback shift registers based on
	non-primes},
  journal = {Designs, Codes and Cryptography},
  year = {2004},
  volume = {31},
  pages = {227-250},
  number = {3},
  month = {March},
  publisher = {Springer Netherlands}
}

@INPROCEEDINGS{4813497,
  author = {Dae-Sik Ko and Seung-Kook Cheong},
  title = {Web Performance Enhancement of E-business System Using the SSD},
  booktitle = {Future Generation Communication and Networking Symposia, 2008. FGCNS
	'08. Second International Conference on},
  year = {2008},
  volume = {1},
  pages = {81 -84},
  month = {dec.},
  abstract = {In this paper, we proposed web performance enhancement of the e-business
	system using the SSD (Solid State Drive). In the e-business system,
	transactions must be strictly ordered, should occur at a fixed point
	time, and not be lost. We approached to solve e-business problem
	by using software and hardware techniques such as SSD. X-internet
	has advantage of C/S and Web applications. Since X-internet technique
	enables to reduce the loading time of web pages, X-internet is useful
	to enhance performance of the e-business system. SSD is a storage
	device that uses DRAM or NAND Flash as primary storage media. Since
	the SSD stores and accesses data directly to memory chips, which
	results in storage speeds far greater than conventional magnetic
	storage devices (HDD). Therefore x-internet techniques can be used
	to solve confused browsing and poor UI, and SSD can be used to solve
	I/O bottleneck of the e-business system.},
  doi = {10.1109/FGCNS.2008.100},
  file = {:PDF\\05370894.pdf:PDF},
  keywords = {DRAM;NAND Flash;SSD;Web performance enhancement;X- internet technique;e-business
	system;memory chips;primary storage media;solid state drive;DRAM
	chips;Internet;electronic commerce;flash memories;},
  timestamp = {2011.05.19}
}

@ARTICLE{681314,
  author = {Kotter, R.},
  title = {A fast parallel implementation of a Berlekamp-Massey algorithm for
	algebraic-geometric codes},
  journal = {Information Theory, IEEE Transactions on},
  year = {1998},
  volume = {44},
  pages = {1353-1368},
  number = {4},
  month = {Jul},
  abstract = {We obtain a parallel Berlekamp-Massey-type algorithm for determining
	error locating functions for the class of one point algebraic-geometric
	codes. The proposed algorithm has a regular and simple structure
	and is suitable for VLSI implementation. We give an outline for an
	implementation, which uses as main blocks γ copies of a modified
	one-dimensional Berlekamp-Massey algorithm, where γ is the order
	of the first nongap in the function space associated with the code.
	Such a parallel implementation determines the error locator for an
	algebraic-geometric code using the same time requirements as the
	underlying one-dimensional Berlekamp-Massey algorithm applied to
	the decoding of Reed-Solomon codes},
  doi = {10.1109/18.681314},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\00681314.pdf:PDF},
  issn = {0018-9448},
  keywords = {algebraic geometric codes;decoding;parallel algorithms;Berlekamp-Massey
	algorithm;VLSI implementation;algebraic-geometric codes;decoding;error
	locating functions;fast parallel implementation;one-dimensional Berlekamp-Massey
	algorithm;Algorithm design and analysis;Computational complexity;Concatenated
	codes;Decoding;Hamming distance;Hardware;Helium;Logic;Maintenance
	engineering;Very large scale integration},
  timestamp = {2014.05.16}
}

@ARTICLE{959255,
  author = {Kou, Y. and Lin, S. and Fossorier, M.P.C.},
  title = {Low-density parity-check codes based on finite geometries: a rediscovery
	and new results},
  journal = {Information Theory, IEEE Transactions on},
  year = {2001},
  volume = {47},
  pages = {2711 -2736},
  number = {7},
  month = nov,
  abstract = {This paper presents a geometric approach to the construction of low-density
	parity-check (LDPC) codes. Four classes of LDPC codes are constructed
	based on the lines and points of Euclidean and projective geometries
	over finite fields. Codes of these four classes have good minimum
	distances and their Tanner (1981) graphs have girth 6. Finite-geometry
	LDPC codes can be decoded in various ways, ranging from low to high
	decoding complexity and from reasonably good to very good performance.
	They perform very well with iterative decoding. Furthermore, they
	can be put in either cyclic or quasi-cyclic form. Consequently, their
	encoding can be achieved in linear time and implemented with simple
	feedback shift registers. This advantage is not shared by other LDPC
	codes in general and is important in practice. Finite-geometry LDPC
	codes can be extended and shortened in various ways to obtain other
	good LDPC codes. Several techniques of extension and shortening are
	presented. Long extended finite-geometry LDPC codes have been constructed
	and they achieve a performance only a few tenths of a decibel away
	from the Shannon theoretical limit with iterative decoding },
  doi = {10.1109/18.959255},
  file = {:PDF\\Low-Density_Parity-Check_Codes_Based_on_Finite_Geometries_A_Rediscovery_and_New_Results.pdf:PDF},
  issn = {0018-9448},
  keywords = {Euclidean geometry;Shannon theoretical limit;Tanner graphs;concatenated
	codes;cyclic codes;decoding complexity;error control codes;feedback
	shift registers;finite geometries;finite-geometry LDPC codes;iterative
	decoding;linear time encoding;long extended finite-geometry LDPC
	codes;low-density parity-check codes;minimum distance;projective
	geometry;quasi-cyclic codes;shortened codes;turbo codes;computational
	complexity;concatenated codes;cyclic codes;error correction codes;error
	detection codes;iterative decoding;turbo codes;}
}

@INPROCEEDINGS{5513685,
  author = {Kuijper, M. and Schindelar, K.},
  title = {The predictable leading monomial property for polynomial vectors
	over a ring},
  booktitle = {Information Theory Proceedings (ISIT), 2010 IEEE International Symposium
	on},
  year = {2010},
  pages = {1133 -1137},
  month = {june},
  abstract = {The "predictable degree property", a terminology introduced by Forney
	in 1970, is a property of polynomial matrices over a field F that
	has proven itself to be fundamentally useful for a range of applications.
	In this paper we strengthen this property into the "predictable leading
	monomial" property, and show that this PLM property is shared by
	minimal Gr\"{o}bner bases for any positional term order (here: TOP
	and POT) in $\mathbb{F}_{q}[t]$. The property is useful particularly
	for minimal interpolation-type problems. Because of the presence
	of zero divisors, minimal Gro\"{o}bner bases over a finite ring of
	the type $\mathbb{Z}_{p^{r}}$ (where p is a prime integer and r is
	an integer > 1) do not have the PLM property. We show how to construct,
	from an ordered minimal Gr\"{o}bner basis, a so-called minimal Gr\"{o}bner
	p-basis that does have a PLM property. The parametrization of all
	shortest linear recurrence relations of a finite sequence over $\mathbb{Z}_{p^{r}}$
	is a type of problem for which this is useful and we include an illustrative
	example.},
  doi = {10.1109/ISIT.2010.5513685},
  file = {:PDF\\05513685.pdf:PDF},
  keywords = {minimal Gr\"{o}bner p-basis;polynomial vectors;predictable degree
	property;predictable leading monomial property;polynomial matrices;vectors;},
  timestamp = {2012.05.22}
}

@INPROCEEDINGS{5399804,
  author = {Kuijper, M. and Schindelar, K.},
  title = {Gr\"{o}bner bases and behaviors over finite rings},
  booktitle = {Decision and Control, 2009 held jointly with the 2009 28th Chinese
	Control Conference. CDC/CCC 2009. Proceedings of the 48th IEEE Conference
	on},
  year = {2009},
  pages = {8101 -8106},
  month = {dec.},
  abstract = {For several decades Grobner bases have proved useful tools for different
	areas in system theory, particularly multidimensional system theory.
	These areas range from controller design to minimal realizations
	of linear systems over fields. In this paper we focus on the univariate
	case and identify the so-called "predictable leading monomial property"
	as a property of a minimal Gr\"{o}bner basis that is crucial in many
	of these areas. The property is stronger than "row reducedness".
	We revisit the recently developed theory of in which row reducedness
	is extended to polynomial matrices over the finite ring \mathbb{Z}_{p^{r}}
	(with p a prime integer and r a positive integer), which find applications
	in error control coding over $\mathbb{Z}_{p^{r}}$. We recast the
	ideas of in the more general setting of Grobner bases and derive
	new results on how to use minimal Gr\"{o}bner bases to achieve the
	predictable leading monomial property over \mathbb{Z}_{p^{r}}. A
	major advantage of the Gr\"{o}bner approach is that computational
	packages are available to compute a minimal Grobner basis over $\mathbb{Z}_{p^{r}}$,
	such as the SINGULAR computer algebra system. Another advantage of
	the Grobner approach is its generality with respect to the choice
	of ordering of polynomial vectors.},
  doi = {10.1109/CDC.2009.5399804},
  file = {:PDF\\05399804.pdf:PDF},
  issn = {0191-2216},
  keywords = {Gr\"{o}bner bases;SINGULAR computer algebra system;computational packages;controller
	design;error control coding;finite rings;linear systems;minimal Grobner
	basis;multidimensional system theory;polynomial matrices;predictable
	leading monomial property;row reducedness;mathematics computing;polynomial
	matrices;system theory;},
  timestamp = {2012.05.22}
}

@INPROCEEDINGS{5513611,
  author = {Gyu Bum Kyung and Chih-Chun Wang},
  title = {Exhaustive search for small fully absorbing sets and the corresponding
	low error-floor decoder},
  booktitle = {Information Theory Proceedings (ISIT), 2010 IEEE International Symposium
	on},
  year = {2010},
  pages = {739-743},
  abstract = {This work provides an exhaustive search algorithm for finding small
	fully absorbing sets (FASs) of arbitrary low-density parity-check
	(LDPC) codes. In particular, given any LDPC code, the problem of
	finding all FASs of size less than t is formulated as an integer
	programming problem, for which a new branch-&-bound algorithm is
	devised. New node selection and the tree-trimming mechanisms are
	designed to further enhance the efficiency of the algorithm. The
	proposed algorithm is capable of finding all FASs of size ≤ 11 with
	no larger than 2 induced odd-degree check nodes for LDPC codes of
	length ≤ 1000. The resulting exhaustive list of small FASs is then
	used to devise a new post-processing decoder. Numerical results show
	that by taking advantage of the exhaustive list of small FASs, the
	proposed decoder significantly lowers the error floor for codes of
	practical lengths and outperforms the state-of-the-art low-error-floor
	decoders.},
  doi = {10.1109/ISIT.2010.5513611},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\05513611.pdf:PDF},
  keywords = {decoding;integer programming;parity check codes;set theory;tree searching;LDPC
	codes;arbitrary low-density parity-check;branch-bound algorithms;exhaustive
	search;fully absorbing sets;integer programming problem;low error-floor
	decoder;low-error-floor decoders;post-processing decoder;trimming
	mechanisms;Algorithm design and analysis;Belief propagation;Computer
	errors;Electronic mail;Hamming distance;Iterative algorithms;Iterative
	decoding;Linear programming;Maximum likelihood decoding;Parity check
	codes},
  timestamp = {2013.12.19}
}

@ARTICLE{4100904,
  author = {Laendner, S. and Milenkovic, O.},
  title = {LDPC Codes Based on Latin Squares: Cycle Structure, Stopping Set,
	and Trapping Set Analysis},
  journal = {Communications, IEEE Transactions on},
  year = {2007},
  volume = {55},
  pages = {303-312},
  number = {2},
  month = {Feb},
  abstract = {It is well known that certain combinatorial structures in the Tanner
	graph of a low-density parity-check (LDPC) code exhibit a strong
	influence on its performance under iterative decoding. These structures
	include cycles, stopping/trapping sets, and parameters such as the
	diameter of the code. In general, it is very hard to find a complete
	characterization of such configurations in an arbitrary code, and
	even harder to understand the intricate relationships that exist
	between these entities. It is, therefore, of interest to identify
	a simple setting in which all the described combinatorial structures
	can be enumerated and studied within a joint framework. One such
	setting is developed in this paper, for the purpose of analyzing
	the distribution of short cycles and the structure of stopping and
	trapping sets in Tanner graphs of LDPC codes based on idempotent
	and symmetric Latin squares. The parity-check matrices of LDPC codes
	based on Latin squares have a special form that allows for connecting
	combinatorial parameters of the codes with the number of certain
	subrectangles in the Latin squares. Subrectangles of interest can
	be easily identified, and in certain instances, completely enumerated.
	This study can be extended in several different directions, one of
	which is concerned with modifying the code design process in order
	to eliminate or reduce the number of configurations bearing a negative
	influence on the performance of the code. Another application of
	the results includes determining to which extent a configuration
	governs the behavior of the bit-error rate curve in the waterfall
	and error-floor regions},
  doi = {10.1109/TCOMM.2006.888633},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\04100904.pdf:PDF},
  issn = {0090-6778},
  keywords = {graph theory;iterative decoding;matrix algebra;parity check codes;set
	theory;LDPC codes;Latin squares;Tanner graph;cycle structure;iterative
	decoding;low-density parity-check code;parity-check matrices;stopping
	set;trapping set analysis;AWGN;Bit error rate;Data communication;Iterative
	decoding;Joining processes;Memoryless systems;Parity check codes;Performance
	analysis;Process design;Symmetric matrices;Cayley Latin squares;design
	theory;low-density parity-check (LDPC) codes;stopping sets;trapping
	sets},
  timestamp = {2014.03.04}
}

@ARTICLE{Lally2001157,
  author = {Kristine Lally and Patrick Fitzpatrick},
  title = {Algebraic structure of quasicyclic codes},
  journal = {Discrete Applied Mathematics},
  year = {2001},
  volume = {111},
  pages = {157 - 175},
  number = {1-2},
  abstract = {We use Groebner bases of modules as a tool in the construction and
	classification of quasicyclic codes. Whereas previous studies have
	been mainly concerned with the 1-generator case, our results elucidate
	the structure of arbitrary quasicyclic codes and their duals. As
	an application we provide a complete characterisation of self-dual
	quasicyclic codes of index 2.},
  doi = {DOI: 10.1016/S0166-218X(00)00350-4},
  file = {:PDF\\lally-fitzpatrick-quasi-cyclic-code.pdf:PDF;:PDF\\10.1.1.5.3522.pdf:PDF},
  issn = {0166-218X},
  timestamp = {2011.04.15},
  url = {http://www.sciencedirect.com/science/article/B6TYW-43CHKRC-C/2/24015ad5fa81e63f1066bd6bd7186551}
}

@INPROCEEDINGS{1549481,
  author = {Landner, S. and Milenkovic, O.},
  title = {Algorithmic and combinatorial analysis of trapping sets in structured
	LDPC codes},
  booktitle = {Wireless Networks, Communications and Mobile Computing, 2005 International
	Conference on},
  year = {2005},
  volume = {1},
  pages = {630-635 vol.1},
  abstract = {Several combinatorial properties of low-density parity-check (LDPC)
	codes, such as minimum distance, diameter, stopping number, girth
	and cycle-length distribution of the corresponding Tanner graph,
	are known to influence their performance under iterative decoding.
	Recently, a new class of combinatorial configurations, termed trapping
	sets, was shown to be of significant importance in determining the
	properties of LDPC codes in the error-floor region. Very little is
	known both about the existence/parameters of trapping sets in structured
	LDPC codes and about possible techniques for reducing their negative
	influence on the code's performance. In this paper, we address both
	these problems from an algorithmic and combinatorial perspective.
	We first provide a numerical study of the trapping phenomena for
	the Margulis code, which exhibits a fairly high error-floor. Based
	on this analysis, conducted for two different implementations of
	iterative belief propagation, we propose a novel decoding process,
	termed averaged decoding. Averaged decoding provides for a significant
	reduction in the number of incorrectly decoded frames in the error-floor
	region of the Margulis code. Furthermore, based on the results of
	the algorithmic approach, we suggest a novel combinatorial characterizations
	of trapping sets in the class of LDPC codes based on finite geometries.
	Projective geometry LDPC codes are suspected to have extremely low
	error-floors, which is a property that we may attribute to the non-existence
	of certain small trapping sets in the code graph.},
  doi = {10.1109/WIRLES.2005.1549481},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\01549481.pdf:PDF},
  keywords = {combinatorial mathematics;iterative decoding;parity check codes;Margulis
	code;Tanner graph;averaged decoding;code graph;combinatorial analysis;decoding
	process;error-floor region;finite geometries;iterative belief propagation;iterative
	decoding;low-density parity-check codes;structured LDPC codes;trapping
	sets;Algorithm design and analysis;Belief propagation;Bit error rate;Code
	standards;Computer errors;Geometry;Iterative algorithms;Iterative
	decoding;Parity check codes;Signal to noise ratio},
  timestamp = {2013.12.17}
}

@ARTICLE{1210770,
  author = {Larcher, L.},
  title = {Statistical simulation of leakage currents in MOS and flash memory
	devices with a new multiphonon trap-assisted tunneling model},
  journal = {Electron Devices, IEEE Transactions on},
  year = {2003},
  volume = {50},
  pages = { 1246 - 1253},
  number = {5},
  month = may,
  abstract = {A new physics-based model of leakage current suitable for MOS and
	Flash memory gate oxide is presented in this paper. This model, which
	assumes the multiphonon trap-assisted tunneling as conduction mechanism,
	calculates the total leakage current summing the contributions of
	the percolation paths formed by one or more aligned traps. Spatial
	positions and energetic levels of traps have been randomly generated
	within the oxide by a random number generator which has been integrated
	into the model. Using this model, statistical simulations of leakage
	currents measured from both MOS and Flash EEPROM memory tunnel oxides
	have been carried out. In this way, experimental leakage current
	distributions can be directly reproduced, thus opening a wide range
	of useful applications in MOS and Flash EEPROM memory reliability
	prediction.},
  doi = {10.1109/TED.2003.813236},
  issn = {0018-9383},
  keywords = { EEPROM; MOS memory devices; flash memory devices; leakage currents;
	multiphonon trap-assisted tunneling model; percolation paths; physics-based
	model; random number generator; reliability prediction; statistical
	simulation; tunnel oxides; MOS memory circuits; circuit simulation;
	flash memories; integrated circuit modelling; integrated circuit
	reliability; leakage currents; statistical analysis; tunnelling;}
}

@ARTICLE{1337175,
  author = {Larcher, L. and Pavan, P.},
  title = {Statistical simulations for flash memory reliability analysis and
	prediction},
  journal = {Electron Devices, IEEE Transactions on},
  year = {2004},
  volume = {51},
  pages = { 1636 - 1643},
  number = {10},
  month = oct,
  abstract = {In this paper, through the use of a recently proposed statistical
	model of stress-induced leakage current, we will investigate the
	reliability of actual flash memory technologies and predict future
	trends. We investigate either program disturbs (namely gate and drain
	disturbs) and data retention of state-of-the-art flash memory cells
	and use this model to correlate the induced threshold voltage shift
	to the typical outputs coming from oxide characterization, that are
	density, cross section, and energy level of defects. Physical mechanisms
	inducing the largest threshold voltage (VT) degradation will be identified
	and explained. Furthermore, we predict the effects of tunnel oxide
	scaling on flash memory data retention, giving a rule of thumb to
	scale the tunnel oxide while maintaining the same retention requirements.},
  doi = {10.1109/TED.2004.835023},
  issn = {0018-9383},
  keywords = { device simulations; drain disturbs; flash memories; flash memory
	cells; flash memory data retention; flash memory reliability analysis;
	flash memory reliability prediction; gate disturbs; oxide characterization;
	semiconductor device reliability; semiconductor memories; statistical
	model; statistical simulations; stress-induced leakage current; threshold
	voltage shift; tunnel oxide scaling; failure analysis; flash memories;
	integrated circuit modelling; integrated circuit reliability; integrated
	memory circuits; semiconductor storage; statistical analysis;}
}

@ARTICLE{981221,
  author = {Larcher, L. and Pavan, P. and Pietri, S. and Albani, L. and Marmiroli,
	A.},
  title = {A new compact DC model of floating gate memory cells without capacitive
	coupling coefficients},
  journal = {Electron Devices, IEEE Transactions on},
  year = {2002},
  volume = {49},
  pages = {301 -307},
  number = {2},
  month = feb,
  abstract = {This paper presents for the first time a new compact SPICE model of
	floating gate nonvolatile memory cells capable to reproduce effectively
	the complete DC electrical behavior in every bias conditions. This
	model features many advantages compared to previous ones: it is simple
	and easy to implement since it uses SPICE circuit elements, is scalable,
	and its computational time is not excessive. It is based on a new
	procedure that calculates the floating gate voltage without using
	fixed capacitive coupling coefficients, thus improving the floating
	gate voltage estimate that is fundamental for the correct modeling
	of cell operations. Moreover, this model requires only the usual
	parameters adopted for SPICE-like models of MOS transistors plus
	the floating gate-control gate capacitance, making it very attractive
	to industry as the same parameter extraction procedure used for MOS
	transistors can be directly applied. The model we propose has been
	validated on E2PROM and flash memory cells manufactured in existing
	technology (0.35 mu;m and 0.25 mu;m) by STMicroelectronics },
  doi = {10.1109/16.981221},
  file = {:PDF\\A_New_Compact_DC_Model_of_Floating_Gate_Memory_Cells_Without_Capacitive_Coupling_Coefficients.pdf:PDF},
  issn = {0018-9383},
  keywords = {0.25 micron;0.35 micron;DC electrical behavior;E2PROM memory cells;EEPROM;MOS
	transistors;SPICE circuit elements;SPICE-like models;capacitive coupling
	coefficients;cell operation modeling;compact DC model;compact SPICE
	model;computational time;flash memory cells;floating gate memory
	cells;floating gate nonvolatile memory cells;floating gate voltage;floating
	gate voltage estimate;floating gate-control gate capacitance;parameter
	extraction procedure;scalable model;semiconductor device modeling;EPROM;SPICE;circuit
	simulation;flash memories;integrated circuit modelling;parameter
	estimation;}
}

@INPROCEEDINGS{5370894,
  author = {Dongkyu Lee and Kern Koh},
  title = {Popularity Based Cache Management Scheme for RAID Which Uses an SSD
	as a Cache},
  booktitle = {Computer Sciences and Convergence Information Technology, 2009. ICCIT
	'09. Fourth International Conference on},
  year = {2009},
  pages = {913 -915},
  month = {nov.},
  abstract = {In this paper, we propose a static cache management policy for RAID
	architecture which uses an SSD as a disk cache. Because SSD has several
	drawbacks, dynamic cache management scheme such as LRU can cause
	serious problem i.e., too much write operations can be occurred in
	order to contain most recently used data. Frequent write operations
	on SSD not only slow down the performance but also shorten the lifetime.
	For this reason, we propose a static cache management policy which
	takes popularity and size into account. Popular data of some period
	is copied into SSD and too large files are excluded since too large
	files can lower the hit ratio. We used real trace to evaluate our
	scheme and the results showed that write counts are reduced to 1/37
	of LRU policy as we desired. Unfortunately, the average hit ratio
	was lower than LRU policy but when popular data is similar between
	two periods, our scheme showed better hit ratio than LRU. This implies
	that our scheme can be used on web server which follows zipf's distribution
	and has smooth changing of popular data.},
  doi = {10.1109/ICCIT.2009.310},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05370894.pdf:PDF},
  keywords = {LRU policy;RAID architecture;SSD;Web server;disk cache;dynamic cache
	management scheme;popularity based cache management scheme;static
	cache management policy;zipf's distribution;RAID;cache storage;memory
	architecture;},
  timestamp = {2011.05.20}
}

@ARTICLE{PhysRevLett.71.211,
  author = {Lee, Jooyoung},
  title = {New Monte Carlo algorithm: Entropic sampling},
  journal = {Phys. Rev. Lett.},
  year = {1993},
  volume = {71},
  pages = {211--214},
  month = {Jul},
  doi = {10.1103/PhysRevLett.71.211},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\PhysRevLett.71.211.pdf:PDF},
  issue = {2},
  publisher = {American Physical Society},
  timestamp = {2012.03.09},
  url = {http://link.aps.org/doi/10.1103/PhysRevLett.71.211}
}

@INPROCEEDINGS{1197798,
  author = {Jae-Duk Lee and Jeong-Hyuk Choi and Donggun Park and Kinam Kim},
  title = {Degradation of tunnel oxide by FN current stress and its effects
	on data retention characteristics of 90 nm NAND flash memory cells},
  booktitle = {Reliability Physics Symposium Proceedings, 2003. 41st Annual. 2003
	IEEE International},
  year = {2003},
  pages = { 497 - 501},
  month = {march-4 april},
  doi = {10.1109/RELPHY.2003.1197798},
  issn = { },
  keywords = { 2 Gbit; 90 nm; 90-nm cell transistors; FN current stress; NAND flash
	memory cells; cell transistor width; data retention characteristics;
	electron trap sites; failure mechanism; fast traps; hole trap site;
	interface trap generation; interface trap relaxation; slow traps;
	tunnel oxide; tunnel oxide degradation;Id-Vg hysteresis curve; MOSFET;
	NAND circuits; electron traps; failure analysis; flash memories;
	hole traps; integrated circuit reliability; interface states; tunnelling;}
}

@ARTICLE{1475332,
  author = {Lenzlinger, M. and Snow, E.H.},
  title = {Fowler-Nordheim tunneling into thermally grown SiO2},
  journal = {Electron Devices, IEEE Transactions on},
  year = {1968},
  volume = {15},
  pages = { 686},
  number = {9},
  month = sep,
  doi = {10.1109/T-ED.1968.16430},
  issn = {0018-9383}
}

@ARTICLE{1576951,
  author = {Zongwang Li and Lei Chen and Lingqi Zeng and Shu Lin and Fong, W.H.},
  title = {Efficient encoding of quasi-cyclic low-density parity-check codes},
  journal = {Communications, IEEE Transactions on},
  year = {Jan.},
  volume = {54},
  pages = {71-81},
  number = {1},
  abstract = {Quasi-cyclic (QC) low-density parity-check (LDPC) codes form an important
	subclass of LDPC codes. These codes have encoding advantage over
	other types of LDPC codes. This paper addresses the issue of efficient
	encoding of QC-LDPC codes. Two methods are presented to find the
	generator matrices of QC-LDPC codes in systematic-circulant (SC)
	form from their parity-check matrices, given in circulant form. Based
	on the SC form of the generator matrix of a QC-LDPC code, various
	types of encoding circuits using simple shift registers are devised.
	It is shown that the encoding complexity of a QC-LDPC code is linearly
	proportional to the number of parity bits of the code for serial
	encoding, and to the length of the code for high-speed parallel encoding.},
  doi = {10.1109/TCOMM.2005.861667},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\01576951.pdf:PDF},
  issn = {0090-6778},
  keywords = {matrix algebra;parity check codes;shift registers;LDPC codes;encoding
	circuits;encoding complexity;generator matrices;high-speed parallel
	encoding;parity bits;parity-check matrices;quasicyclic low-density
	parity-check codes;serial encoding;shift registers;systematic-circulant
	form;Application software;Belief propagation;Circuits;Computer errors;Encoding;Iterative
	decoding;NASA;Parity check codes;Shift registers;Sparse matrices;Array
	of circulants;quasi-cyclic (QC) low-density parity-check (LDPC) codes;systematic-circulant
	(SC) form},
  timestamp = {2013.03.14}
}

@INPROCEEDINGS{1312997,
  author = {Liao, E. and Engling Yeo and Nikolic, B.},
  title = {Low-density parity-check code constructions for hardware implementation},
  booktitle = {Communications, 2004 IEEE International Conference on},
  year = {2004},
  volume = {5},
  pages = { 2573 - 2577 Vol.5},
  month = {june},
  abstract = {We present several hardware architectures to implement low-density
	parity-check (LDPC) decoders for codes constructed with a hierarchical
	structure. The proposed hierarchical formulation of the LDPC code
	allows a structured hardware realization of the decoder. For a fully-parallel
	implementation, there is a reduced routing congestion that allows
	implementations for blocks sizes up to 1024 bits in 0.13 mu;m technology.
	Partially and fully serial implementations benefits greatly from
	the structure of the code as well, leading to several flexible, efficient
	architectures. In a general purpose 0.13 mu;m technology, the approximate
	area required by a 1024-bit fully-parallel LDPC decoder is found
	to be 12.5 mm2 while a serial decoder can be implemented in an area
	of 0.15 mm2.},
  doi = {10.1109/ICC.2004.1312997},
  file = {:PDF\\01312997.pdf:PDF},
  issn = { },
  keywords = { 0.13 mum; 1024 bit; BER performance; bit error rate; fully-parallel
	decoder; hardware implementation; low-density parity-check code constructions;
	routing congestion; serial decoder; sum-product algorithm; decoding;
	error statistics; parity check codes; telecommunication congestion
	control; telecommunication network routing;},
  timestamp = {2011.04.22}
}

@ARTICLE{springerlink:10.1023/A:1018679025763,
  author = {Lima, A. R. and de Oliveira, P. M. C. and Penna, T. J. P.},
  title = {A Comparison Between Broad Histogram and Multicanonical Methods},
  journal = {Journal of Statistical Physics},
  year = {2000},
  volume = {99},
  pages = {691-705},
  note = {10.1023/A:1018679025763},
  abstract = {We discuss the conceptual differences between the broad histogram
	(BHM) and reweighting methods in general, and particularly the so-called
	multicanonical (MUCA) approaches. The main difference is that BHM
	is based on microcanonical, fixed-energy averages which depend only
	on the good statistics taken inside each energy level. The detailed
	distribution of visits among different energy levels, determined
	by the particular dynamic rule one adopts, is irrelevant. Contrary
	to MUCA, where the results are extracted from the dynamic rule itself,
	within BHM any microcanonical dynamics could be adopted. As a numerical
	test, we have used both BHM and MUCA in order to obtain the spectral
	energy degeneracy of the Ising model in 4×4×4 and 32×32 lattices,
	for which exact results are known. We discuss why BHM gives more
	accurate results than MUCA, even using the same Markovian sequence
	of states. In addition, such an advantage increases for larger systems.},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\springerlink_10_1023_A_1018679025763.pdf:PDF},
  issn = {0022-4715},
  issue = {3},
  keyword = {Physics and Astronomy},
  publisher = {Springer Netherlands},
  timestamp = {2012.03.12},
  url = {http://dx.doi.org/10.1023/A:1018679025763}
}

@TECHREPORT{NASA_NAG5-1278,
  author = {Shu Lin},
  title = {Quasi-Cyclic LDPC Codes},
  institution = {Department of Electrical and Computer Engineering, University of
	California, Davis, CA},
  file = {:PDF\\Quasi-Cyclic_LDPC_Codes_NASA_NAG5-1278.pdf:PDF},
  timestamp = {2013.04.26}
}

@ARTICLE{1055019,
  author = { Shu Lin},
  title = {Multifold Euclidean geometry codes},
  journal = {Information Theory, IEEE Transactions on},
  year = {1973},
  volume = {19},
  pages = { 537 - 548},
  number = {4},
  month = jul,
  abstract = { This paper presents a class of majority-logic decodable codes whose
	structure is based on the structural properties of Euclidean geometries
	(EG) and codes that are invariant under the affine group of permutations.
	This new class of codes contains the ordinary EG codes and some generalized
	EG codes as subclasses. One subclass of new codes is particularly
	interesting: they are the most efficient majority-logic decodable
	codes that have been constructed.},
  doi = {10.1109/TIT.1973.1055019},
  file = {:C\:\\Users\\Public\\Documents\\My eBooks\\refereces\\PDF\\01055019.pdf:PDF},
  issn = {0018-9448},
  keywords = { Geometry codes; Majority logic decoding;}
}

@ARTICLE{1054900,
  author = { Shu Lin},
  title = {On the Number of information symbols in polynomial codes},
  journal = {Information Theory, IEEE Transactions on},
  year = {1972},
  volume = {18},
  pages = { 785 - 794},
  number = {6},
  month = nov,
  abstract = { Polynomial codes and their dual codes as introduced by Kasami, Lin,
	and Peterson have considerable algebraic and geometric structure.
	It has been shown that these codes contain many well-known classes
	of cyclic codes as subclasses, such as BCH codes, projective geometry
	codes (PG codes), Euclidean geometry codes (EG codes), and generalized
	Reed-Muller codes (GRM codes). In this paper, combinatorial expressions
	for the number of information symbols and parity-check symbols in
	polynomial codes are derived. The results are applied to two important
	subclasses of codes, the PG codes and EG codes.},
  doi = {10.1109/TIT.1972.1054900},
  file = {:PDF\\01054900.pdf:PDF},
  issn = {0018-9448},
  keywords = { Polynomial codes;}
}

@BOOK{Lin:2004:ECC:983680,
  title = {Error Control Coding, Second Edition},
  publisher = {Prentice-Hall, Inc.},
  year = {2004},
  author = {Lin, Shu and Costello, Daniel J.},
  address = {Upper Saddle River, NJ, USA},
  isbn = {0130426725}
}

@BOOK{lincost,
  title = {Error Control Coding: Fundamentals and Applications},
  publisher = {Prentice-Hall},
  year = {1983},
  author = {Shu Lin and Costello, Jr., Daniel J.},
  note = {Englewood Clifts, NJ},
  timestamp = {2011.04.11}
}

@ARTICLE{1459069,
  author = {Ling, S. and Sole, P.},
  title = {On the algebraic structure of quasi-cyclic codes III: generator theory},
  journal = {Information Theory, IEEE Transactions on},
  year = {2005},
  volume = {51},
  pages = {2692 -2700},
  number = {7},
  month = july,
  abstract = {Following Parts I and II, quasi-cyclic codes of given index are studied
	as codes over a finite polynomial ring. These latter codes are decomposed
	by the Chinese Remainder Theorem (CRT), or equivalently the Mattson-Solomon
	transform, into products of shorter codes over larger alphabets.
	We characterize and enumerate self-dual one-generator quasi-cyclic
	codes in that context. We give an algorithm to remove some equivalent
	codes from that enumeration. A generalization to multigenerator codes
	is sketched.},
  doi = {10.1109/TIT.2005.850142},
  file = {:PDF\\01459069.pdf:PDF},
  issn = {0018-9448},
  keywords = {CRT;Chinese remainder theorem;DFT;Mattson-Solomon transform;automorphism
	group;discrete Fourier transform;equivalent code;finite polynomial
	ring;multigenerator code;quasi-cyclic code;self-dual one-generator;cyclic
	codes;discrete Fourier transforms;dual codes;transform coding;}
}

@ARTICLE{959257,
  author = {San Ling and Sole, P.},
  title = {On the algebraic structure of quasi-cyclic codes .I. Finite fields
	},
  journal = {Information Theory, IEEE Transactions on},
  year = {2001},
  volume = {47},
  pages = {2751 -2760},
  number = {7},
  month = nov,
  abstract = {A new algebraic approach to quasi-cyclic codes is introduced. The
	key idea is to regard a quasi-cyclic code over a field as a linear
	code over an auxiliary ring. By the use of the Chinese remainder
	theorem (CRT), or of the discrete Fourier transform (DFT), that ring
	can be decomposed into a direct product of fields. That ring decomposition
	in turn yields a code construction from codes of lower lengths which
	turns out to be in some cases the celebrated squaring and cubing
	constructions and in other cases the (u+ upsi;|u- upsi;) and Vandermonde
	constructions. All binary extended quadratic residue codes of length
	a multiple of three are shown to be attainable by the cubing construction.
	Quinting and septing constructions are introduced. Other results
	made possible by the ring decomposition are a characterization of
	self-dual quasi-cyclic codes, and a trace representation that generalizes
	that of cyclic codes},
  doi = {10.1109/18.959257},
  file = {:PDF\\00959257.pdf:PDF},
  issn = {0018-9448},
  keywords = {Chinese remainder theorem;DFT;Golay codes;Vandermonde construction;algebraic
	structure;auxiliary ring;binary extended quadratic residue codes;code
	construction;code length;cubing construction;cyclic codes;discrete
	Fourier transform;finite fields;linear code;polynomial ring;quasi-cyclic
	codes;quinting construction;ring decomposition;self-dual quasi-cyclic
	codes;septing construction;squaring construction;trace representation;Golay
	codes;binary codes;cyclic codes;discrete Fourier transforms;dual
	codes;linear codes;residue codes;}
}

@ARTICLE{1057320,
  author = { van Lint, J. and Springer, T.},
  title = {Generalized Reed - Solomon codes from algebraic geometry},
  journal = {Information Theory, IEEE Transactions on},
  year = {1987},
  volume = {33},
  pages = { 305 - 309},
  number = {3},
  month = may,
  abstract = {A few years ago Tsfasman {em et al.,} using results from algebraic
	geometry, showed that there is a sequence of codes which are generalizations
	of Goppa codes and which exceed the Gilbert-Varshamov bound. We show
	that a similar sequence of codes (in fact, the duals of the previous
	codes) can be found by generalizing the construction of Reed-Solomon
	codes. Our approach has the advantage that it uses less complicated
	concepts from algebraic geometry.},
  doi = {10.1109/TIT.1987.1057320},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\01057320.pdf:PDF},
  issn = {0018-9448},
  keywords = { Dual coding; Geometry; Goppa coding; Reed-Solomon coding;}
}

@ARTICLE{992777,
  author = {Litsyn, S. and Shevelev, V.},
  title = {On ensembles of low-density parity-check codes: asymptotic distance
	distributions},
  journal = {Information Theory, IEEE Transactions on},
  year = {2002},
  volume = {48},
  pages = {887 -908},
  number = {4},
  month = {apr},
  abstract = {We derive expressions for the average distance distributions in several
	ensembles of regular low-density parity-check codes (LDPC). Among
	these ensembles are the standard one defined by matrices having given
	column and row sums, ensembles defined by matrices with given column
	sums or given row sums, and an ensemble defined by bipartite graphs},
  doi = {10.1109/18.992777},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\00992777.pdf:PDF},
  issn = {0018-9448},
  keywords = {LDPC;asymptotic distance distributions;average distance distributions;binary
	parity-check matrices;bipartite graphs;column sums;low-density parity-check
	codes;row sums;error detection codes;graph theory;matrix algebra;},
  timestamp = {2011.11.17}
}

@ARTICLE{Little1997293,
  author = {John Little and Keith Saints and Chris Heegard},
  title = {On the structure of Hermitian codes},
  journal = {Journal of Pure and Applied Algebra},
  year = {1997},
  volume = {121},
  pages = {293 - 314},
  number = {3},
  abstract = {Let Xm denote the Hermitian curve xm + 1 = ym + y over the field Fm2.
	Let Q be the single point at infinity, and let D be the sum of the
	other m3 points of Xm rational over Fm2, each with multiplicity 1.
	Xm has a cyclic group of automorphisms of order m2 竏・1, which induces
	automorphisms of each of the the one-point algebraic geometric Goppa
	codes CL(D, aQ) and their duals. As a result, these codes have the
	structure of modules over the ring Fq[t], and this structure can
	be used to good effect in both encoding and decoding. In this paper
	we examine the algebraic structure of these modules by means of the
	theory of Groebner bases. We introduce a root diagram for each of
	these codes (analogous to the set of roots for a cyclic code of length
	q 竏・1 over Fq), and show how the root diagram may be determined combinatorially
	from a. We also give a specialized algorithm for computing Groebner
	bases, adapted to these particular modules. This algorithm has a
	much lower complexity than general Groebner basis algorithms, and
	has been successfully implemented in the Maple computer algebra system.
	This permits the computation of Groebner bases and the construction
	of compact systematic encoders for some quite large codes (e.g. codes
	such as CL(D, 4010Q) on the curve X16, with parameters n = 4096,
	k = 3891).},
  doi = {10.1016/S0022-4049(96)00067-9},
  file = {:PDF\\Little1997293.pdf:PDF},
  issn = {0022-4049},
  timestamp = {2012.06.20},
  url = {http://www.sciencedirect.com/science/article/pii/S0022404996000679}
}

@ARTICLE{4524251,
  author = {Yuanhua Liu and Xinmei Wang and Ruwei Chen and Yucheng He},
  title = {Generalized Combining Method for Design of Quasi-Cyclic LDPC Codes},
  journal = {Communications Letters, IEEE},
  year = {2008},
  volume = {12},
  pages = {392 -394},
  number = {5},
  month = {may },
  abstract = {A generalization of the Chinese Remainder Theorem (CRT) combining
	method is proposed to design much more and better quasi-cyclic (QC)
	LDPC codes when the parity check matrices of the component codes
	are given. It can design a much larger class of QC-LDPC codes with
	similar performance by loosening the condition for determining the
	intermediate parameters. By permuting the block rows of the parity
	check matrices of the component codes, a lot of QC-LDPC codes with
	much less 6-cycles and better performance can be designed. At a BER
	of 10-6 some QC-LDPC codes designed by the generalized combining
	method outperform those designed by the CRT combining method by 0.5
	dB.},
  doi = {10.1109/LCOMM.2008.080132},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\04524251.pdf:PDF},
  issn = {1089-7798},
  keywords = {BER;QC-low density parity check codes;bit error rate;quasi-cyclic
	LDPC codes;error statistics;parity check codes;},
  timestamp = {2011.12.06}
}

@INPROCEEDINGS{5076201,
  author = {Yuan-Hua Liu and Xin-Mei Wang and Jian-Hua Ma},
  title = {Design of Quasi-Cyclic LDPC Codes Based on Euclidean Geometries},
  booktitle = {Advanced Information Networking and Applications, 2009. AINA '09.
	International Conference on},
  year = {2009},
  pages = {207 -211},
  month = may,
  abstract = {This paper presents an algebraic method for constructing quasi-cyclic
	(QC) low-density parity-check (LDPC) codes based on the structural
	properties of Euclidean geometries. The construction method results
	in a class of QC-LDPC codes with girth of at least 6. Codes in this
	class perform very close to the Shannon limit with iterative decoding.
	Simulations show that the designed QC-LDPC codes have almost the
	same performance with the existing QC Euclidean geometry LDPC codes.},
  doi = {10.1109/AINA.2009.35},
  file = {:\\\\yrlshare.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05076201.pdf:PDF},
  issn = {1550-445X},
  keywords = {Shannon limit;algebraic method;euclidean geometries;iterative decoding;low
	density parity check codes;quasi cyclic LDPC codes;cyclic codes;geometric
	codes;iterative decoding;parity check codes;}
}

@ARTICLE{1580676,
  author = {Lu, J. and Moura, J.M.F.},
  title = {Structured LDPC codes for high-density recording: large girth and
	low error floor},
  journal = {Magnetics, IEEE Transactions on},
  year = {2006},
  volume = {42},
  pages = { 208 - 213},
  number = {2},
  month = {feb.},
  abstract = { High-rate low-density parity-check (LDPC) codes are the focus of
	intense research in magnetic recording because, when decoded by the
	iterative sum-product algorithm, they show decoding performance close
	to the Shannon capacity. However, cycles, especially short cycles,
	are harmful to LDPC codes. The paper describes the partition-and-shift
	LDPC (PS-LDPC) codes, a new class of regular, structured LDPC codes
	that can be designed with large girth and arbitrary large minimum
	distance. Large girth leads to more efficient iterative decoding
	and codes with better error-floor properties than random LDPC codes.
	PS-LDPC codes can be designed for any desired column weight and with
	flexible code rates. The paper details the girth and distance properties
	of the codes and their systematic construction and presents analytical
	and simulation performance results that show that, in the high signal-to-noise
	ratio region, PS-LDPC codes outperform random codes, alleviating
	the error floor phenomenon.},
  doi = {10.1109/TMAG.2005.861748},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\01580676.pdf:PDF},
  issn = {0018-9464},
  keywords = { Shannon capacity; column weight; decoding performance; distance property;
	error floor phenomenon; error-floor properties; flexible code rates;
	girth property; high-density recording; iterative codes; iterative
	decoding; iterative sum-product algorithm; magnetic recording; partition-and-shift
	LDPC codes; simulation performance; systematic construction; error
	statistics; iterative decoding; magnetic recording; parity check
	codes;},
  timestamp = {2011.12.06}
}

@ARTICLE{748992,
  author = {MacKay, D.J.C.},
  title = {Good error-correcting codes based on very sparse matrices},
  journal = {Information Theory, IEEE Transactions on},
  year = {1999},
  volume = {45},
  pages = {399 -431},
  number = {2},
  month = {March},
  doi = {10.1109/18.748992},
  file = {:PDF\\Good_Error-Correcting_Codes_Based_on_Very_Sparse_Matrices.pdf:PDF},
  issn = {0018-9448},
  keywords = {Gallager codes;Gaussian channels;MacKay-Neal codes;Shannon limit;binary-symmetric
	channel;channel coding;code sequences;concatenated codes;convolutional
	codes;decoding;error-correcting codes;information rates;optimally
	decoded codes;performance;sum-product algorithm;symmetric stationary
	ergodic noise;turbo codes;very sparse matrices;Gaussian channels;channel
	coding;decoding;error correction codes;noise;sparse matrices;}
}

@ARTICLE{585036,
  author = {MacKay, D.J.C. and Neal, R.M.},
  title = {Near Shannon limit performance of low density parity check codes
	},
  journal = {Electronics Letters},
  year = {1997},
  volume = {33},
  pages = {457 -458},
  number = {6},
  month = mar,
  abstract = {The authors report the empirical performance of Gallager's low density
	parity check codes on Gaussian channels. They show that performance
	substantially better than that of standard convolutional and concatenated
	codes can be achieved; indeed the performance is almost as close
	to the Shannon limit as that of turbo codes},
  doi = {10.1049/el:19970362},
  file = {:PDF\\Near_Shannon_Limit_Performance_of_Low_Density_Parity_Check_Codes.pdf:PDF},
  issn = {0013-5194},
  keywords = { Gaussian channels; empirical performance; low density parity check
	codes; near Shannon limit performance; error correction codes;}
}

@INPROCEEDINGS{5372237,
  author = {Maeda, Y. and Kaneko, H.},
  title = {Error Control Coding for Multilevel Cell Flash Memories Using Nonbinary
	Low-Density Parity-Check Codes},
  booktitle = {Defect and Fault Tolerance in VLSI Systems, 2009. DFT '09. 24th IEEE
	International Symposium on},
  year = {2009},
  pages = {367 -375},
  month = {oct},
  abstract = {Conventional flash memories generally utilize simple error control
	codes, such as Hamming code and BCH code. In future high-density
	multilevel cell (MLC) flash memories, however, it is estimated that
	raw bit error rate (BER) will soar with increasing number of charge
	levels, and hence the conventional error control coding will not
	be sufficient for these memories. Low-density parity-check (LDPC)
	code is a class of strong error control codes which are adopted in
	practical wired/wireless communication systems, and hence the LDPC
	code is an important candidate for error control code in future MLC
	memories. Application of the LDPC code to MLC memory is not so straightforward
	as conventional error control codes because the LDPC code usually
	employs soft-input decoding to achieve low decoded BER, and hence
	analysis of the error probability is crucial, especially when nonbinary
	codes are applied. Therefore, this paper analyzes error characteristics
	of MLC flash memory from error control coding viewpoint, and then
	proposes an error control coding using nonbinary LDPC codes. Evaluation
	shows that the decoded BER of the nonbinary LDPC code is lower than
	that of conventional binary irregular LDPC code, and also demonstrates
	that nonbinary LDPC code defined by a parity-check matrix having
	average column weight w = 2.5 has lower decoded BER than nonbinary
	LDPC codes with w = 2 and 3.},
  doi = {10.1109/DFT.2009.25},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05372237.pdf:PDF},
  issn = {1550-5774},
  keywords = {BCH code;Bose-Chaudhuri-Hocquenghem codes;Hamming code;LDPC code;bit
	error rate;error control coding;error probability;flash memory;multilevel
	cell flash memories;nonbinary low-density parity-check codes;parity-check
	matrix;soft-input decoding;wired/wireless communication systems;BCH
	codes;Hamming codes;error correction codes;flash memories;parity
	check codes;},
  timestamp = {2012.03.29}
}

@ARTICLE{Maharaj2005261,
  author = {Hiren Maharaj and Gretchen L. Matthews and Gottlieb Pirsic},
  title = {Riemann-Roch spaces of the Hermitian function field with applications
	to algebraic geometry codes and low-discrepancy sequences},
  journal = {Journal of Pure and Applied Algebra},
  year = {2005},
  volume = {195},
  pages = {261 - 280},
  number = {3},
  abstract = {This paper is concerned with two applications of bases of Riemann-Roch
	spaces. In the first application, we define the floor of a divisor
	and obtain improved bounds on the parameters of algebraic geometry
	codes. These bounds apply to a larger class of codes than that of
	Homma and Kim (J. Pure Appl. Algebra 162 (2001) 273). Then we determine
	explicit bases for large classes of Riemann-Roch spaces of the Hermitian
	function field. These bases give better estimates on the parameters
	of a large class of m-point Hermitian codes. In the second application,
	these bases are used for fast implementation of Xing and Niederreiter's
	method (Acta. Arith. 72 (1995) 281) for the construction of low-discrepancy
	sequences.},
  doi = {DOI: 10.1016/j.jpaa.2004.06.010},
  file = {:\\\\yrlshare.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\Maharaj2005261.pdf:PDF},
  issn = {0022-4049},
  url = {http://www.sciencedirect.com/science/article/B6V0K-4D0Y5RT-1/2/c7af6d8d01a2c3def3da19a087b2acd4}
}

@ARTICLE{762874,
  author = {Manabe, Y. and Okuyama, K. and Kubota, K. and Nozoe, A. and Karashima,
	T. and Ujiie, K. and Kanno, H. and Nakashima, M. and Ajika, N.},
  title = {Detailed observation of small leak current in flash memories with
	thin tunnel oxides},
  journal = {Semiconductor Manufacturing, IEEE Transactions on},
  year = {1999},
  volume = {12},
  pages = {170 -174},
  number = {2},
  month = may,
  abstract = {This paper describes a method for measuring the small current through
	the oxides on the order of 10-20 A or less using a floating gate
	MOSFET and the application results on flash memories with thin tunnel
	oxides. The method is based on an accurate measurement of the threshold
	voltage of a floating gate MOSFET with no charge in the floating
	gate. We applied this method to flash memories to investigate the
	leak current behavior through thin tunnel oxides with very small
	areas ( lt;0.16 mu;m2), and found some anomalous phenomena which
	cannot be observed from SILC measurements if we use large capacitors.
	We also discuss possible mechanisms to explain the phenomena },
  doi = {10.1109/66.762874},
  file = {:PDF\\Detailed_Observation_of_Small_Leak_Current_in_Flash_Memories_with_Thin_Tunnel_Oxides.pdf:PDF},
  issn = {0894-6507},
  keywords = {UV irradiation;charge-up;flash memory;floating gate MOSFET;leak current
	measurement;threshold voltage;tunnel oxide;MOSFET;electric current
	measurement;flash memories;integrated circuit measurement;leakage
	currents;tunnelling;}
}

@INPROCEEDINGS{1029622,
  author = {Mansour, M.M. and Shanbhag, N.R.},
  title = {Low-power VLSI decoder architectures for LDPC codes},
  booktitle = {Low Power Electronics and Design, 2002. ISLPED '02. Proceedings of
	the 2002 International Symposium on},
  year = {2002},
  pages = { 284 - 289},
  doi = {10.1109/LPE.2002.146756},
  file = {:PDF\\01029622.pdf:PDF},
  keywords = { BCJR algorithm; LDPC codes; algorithmic performance; interconnect-driven
	code design; iterative decoding; low-density parity check codes;
	low-power VLSI; message switching activity; message-passing algorithm;
	power savings; random codes; reduced-complexity parallel decoder
	architecture; reliability metrics; structural regularity; VLSI; error
	detection codes; iterative decoding; low-power electronics; message
	passing; parity check codes; random codes; reliability;},
  timestamp = {2011.04.22}
}

@INPROCEEDINGS{5454106,
  author = {Martalo, M. and Ferrari, G. and Abrardo, A. and Franceschini, M.
	and Raheli, R.},
  title = {Density evolution-based analysis and design of LDPC codes with a
	priori information},
  booktitle = {Information Theory and Applications Workshop (ITA), 2010},
  year = {2010},
  pages = {1 -9},
  month = {31 2010-feb. 5},
  abstract = {In this paper, we consider multiple access schemes with correlated
	sources, where a priori information, in terms of source correlation,
	is available at the access point (AP). In particular, we assume that
	each source uses a proper low-density parity-check (LDPC) code to
	transmit, through an additive white Gaussian noise (AWGN) channel,
	its information sequence to the AP. At the AP, the information sequences
	are recovered by an iterative decoder, with component decoders associated
	with the sources, which exploit the available a priori information.
	In order to analyze the behaviour of the considered multiple access
	coded system, we propose a density evolution-based approach, which
	allows to determine a signal-to-noise ratio (SNR) transfer chart
	and compute the system multi-dimensional SNR feasible region. The
	proposed technique, besides characterizing the performance of LDPC-coded
	multiple access scheme, is expedient to design optimized LDPC codes
	for this application.},
  doi = {10.1109/ITA.2010.5454106},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05454106.pdf:PDF},
  keywords = {LDPC-coded multiple access scheme;access point;additive white Gaussian
	noise channel;correlated sources;density evolution-based analysis;iterative
	decoder;low density parity check code;multidimensional SNR feasible
	region;multiple access coded system;multiple access schemes;optimized
	LDPC codes;signal-to-noise ratio transfer chart;AWGN channels;decoding;multi-access
	systems;parity check codes;},
  timestamp = {2012.02.29}
}

@CONFERENCE{a4ea8d1a1a0a9e726c3ba9737db6ca22,
  author = {Martin, D.},
  title = {Analyst Perspective: Deep Dive into Solid State Storage - Technologies
	and Architecture},
  year = {2011},
  volume = {2011},
  number = {Spring},
  series = {Storage Networking World},
  month = {April},
  organization = {Demartek},
  publisher = {COMPUTERWORLD},
  comment = {[読む価値あり]HDDとSSDの性能比較の概要。読んでおいた方がよい。},
  file = {:PDF\\a4ea8d1a1a0a9e726c3ba9737db6ca22_Martin_Monday_0845_SNWS11.pdf:PDF},
  keywords = {Solid State Storage Track},
  memo = {Solid State Storage Track},
  url = {https://www.eiseverywhere.com/ehome/SNWS2011/31916/}
}

@ARTICLE{ieice_e92_a7_1677-1689,
  author = {Masanori HIROTOMO,Masami MOHRI,Masakatu MORII},
  title = {A Probabilistic Algorithm for Computing the Weight Distribution of
	LDPC Codes},
  journal = {IEICE TRANSACTIONS on Fundamentals of Electronics, Communications
	and Computer Sciences},
  year = {2009},
  volume = {E92-A},
  pages = {1677-1689},
  number = {7},
  month = {July},
  abstract = {Low-density parity-check (LDPC) codes are linear block codes defined
	by sparse parity-check matrices. The codes exhibit excellent performance
	under iterative decoding, and the weight distribution is used to
	analyze lower error probability of their decoding performance. In
	this paper, we propose a probabilistic method for computing the weight
	distribution of LDPC codes. The proposed method efficiently finds
	low-weight codewords in a given LDPC code by using Stern's algorithm,
	and stochastically computes the low part of the weight distribution
	from the frequency of the found codewords. It is based on a relation
	between the number of codewords with a given weight and the rate
	of generating the codewords in Stern's algorithm. In the numerical
	results for LDPC codes of length 504, 1008 and 4896, we could compute
	the weight distribution by the proposed method with greater accuracy
	than by conventional methods.},
  file = {:PDF\\e92-a_7_1677.pdf:PDF},
  timestamp = {2013.02.05}
}

@ARTICLE{Massey,
  author = {J. Massey},
  title = {Shift-Register Synthesis and BCH Decoding},
  journal = {IEEE Transactions on Information Theory},
  year = {1969},
  volume = {15},
  pages = {122-127},
  file = {:PDF\\Shift-Regiser_Synthesis_and_BCH_Decoding.pdf:PDF},
  issue = {1}
}

@INPROCEEDINGS{4036370,
  author = {Matsui, H. and Mita, S.},
  title = {Inverse-Free Implementation of Berlekamp-Massey-Sakata Algorithm
	for Decoding Codes on Algebraic Curves},
  booktitle = {Information Theory, 2006 IEEE International Symposium on},
  year = {2006},
  pages = {2250-2254},
  month = {July},
  abstract = {The authors have already proposed small-scale decoder for codes on
	algebraic curves, which updates decoding data serially and has the
	same number of calculators for the finite-field computations as the
	decoders for Reed-Solomon codes except for one calculator for inverse.
	In this research, we eliminate divisions of the finite field from
	error-location algorithm, and propose an inverse-free architecture
	for the error-location of codes on algebraic curves which reduces
	Kotter's architecture},
  doi = {10.1109/ISIT.2006.261967},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\04036370.pdf:PDF},
  keywords = {Reed-Solomon codes;algebraic codes;decoding;Berlekamp-Massey-Sakata
	algorithm;Reed-Solomon codes;algebraic curves;decoding codes;error-location;error-location
	algorithm;inverse-free architecture;small-scale decoder;Calculators;Circuits;Clocks;Computer
	architecture;Costs;Decoding;Equations;Estimation error;Galois fields;Information
	science},
  timestamp = {2014.03.25}
}

@ARTICLE{1522645,
  author = {Matsui, H. and Sakata, S. and Kurihara, M. and Mita, S.},
  title = {Systolic array architecture implementing Berlekamp-Massey-Sakata
	algorithm for decoding codes on a class of algebraic curves},
  journal = {Information Theory, IEEE Transactions on},
  year = {2005},
  volume = {51},
  pages = {3856-3871},
  number = {11},
  month = {Nov},
  doi = {10.1109/TIT.2005.856950},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\01522645.pdf:PDF},
  issn = {0018-9448},
  keywords = {algebraic codes;decoding;error correction codes;parallel algorithms;systolic
	arrays;BMS;Berlekamp-Massey-Sakata algorithm;Grobner basis;algebraic
	curve code;circuit scale;decoding;error-locator polynomial;parallel
	algorithm;systolic array architecture implementation;Circuits;Equations;Galois
	fields;Iterative algorithms;Iterative decoding;Logic;Materials science
	and technology;Polynomials;Systolic arrays;Two dimensional displays;Berlekamp–Massey–Sakata
	(BMS) algorithm;GrÖbner basis;codes on algebraic curves;parallel
	decoding;systolic array},
  timestamp = {2014.03.25}
}

@BOOK{ECCforNVRAM,
  title = {Error Correction Codes for Non-Volatile Memories},
  publisher = {Springer},
  year = {2008},
  author = {R. Micheloni and A. Marelli and R. Ravasio},
  month = {June}
}

@INPROCEEDINGS{4558857,
  author = {Mielke, N. and Marquart, T. and Ning Wu and Kessenich, J. and Belgal,
	H. and Schares, E. and Trivedi, F. and Goodness, E. and Nevill, L.R.},
  title = {Bit error rate in NAND Flash memories},
  booktitle = {Reliability Physics Symposium, 2008. IRPS 2008. IEEE International},
  year = {2008},
  pages = {9 -19},
  month = {27 2008-may 1},
  abstract = {NAND flash memories have bit errors that are corrected by error-correction
	codes (ECC). We present raw error data from multi-level-cell devices
	from four manufacturers, identify the root-cause mechanisms, and
	estimate the resulting uncorrectable bit error rates (UBER). Write,
	retention, and read-disturb errors all contribute. Accurately estimating
	the UBER requires care in characterization to include all write errors,
	which are highly erratic, and guardbanding for variation in raw bit
	error rate. NAND UBER values can be much better than 10-15, but UBER
	is a strong function of program/erase cycling and subsequent retention
	time, so UBER specifications must be coupled with maximum specifications
	for these quantities.},
  doi = {10.1109/RELPHY.2008.4558857},
  file = {:PDF\\04558857.pdf:PDF},
  keywords = {NAND flash memories;error-correction codes;multilevel-cell devices;read-disturb
	errors;root-cause mechanisms;uncorrectable bit error rates;write
	errors;NAND circuits;error correction codes;error statistics;flash
	memories;},
  timestamp = {2011.04.22}
}

@ARTICLE{959254,
  author = {Miller, G. and Burshtein, D.},
  title = {Bounds on the maximum-likelihood decoding error probability of low-density
	parity-check codes},
  journal = {Information Theory, IEEE Transactions on},
  year = {2001},
  volume = {47},
  pages = {2696 -2710},
  number = {7},
  month = {nov},
  abstract = {We derive both upper and lower bounds on the decoding error probability
	of maximum-likelihood (ML) decoded low-density parity-check (LDPC)
	codes. The results hold for any binary-input symmetric-output channel.
	Our results indicate that for various appropriately chosen ensembles
	of LDPC codes, reliable communication is possible up to channel capacity.
	However, the ensemble averaged decoding error probability decreases
	polynomially, and not exponentially. The lower and upper bounds coincide
	asymptotically, thus showing the tightness of the bounds. However,
	for ensembles with suitably chosen parameters, the error probability
	of almost all codes is exponentially decreasing, with an error exponent
	that can be set arbitrarily close to the standard random coding exponent},
  doi = {10.1109/18.959254},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\00959254.pdf:PDF},
  issn = {0018-9448},
  keywords = {Gallager's bound;LDPC codes;average spectrum;binary-input symmetric-output
	channel;channel capacity;discrete memoryless channel;ensemble averaged
	decoding error probability;error exponent;linear codes;low-density
	parity-check codes;lower bounds;maximum-likelihood decoding error
	probability;random coding exponent;reliable communication;upper bounds;channel
	capacity;error detection codes;error statistics;linear codes;maximum
	likelihood decoding;memoryless systems;},
  timestamp = {2011.11.15}
}

@ARTICLE{5467625,
  author = {Tai Min and Qiang Chen and Beach, R. and Jan, G. and Cheng Horng
	and Kula, W. and Torng, T. and Tong, R. and Zhong, T. and Tang, D.
	and Pokang Wang and Mao-min Chen and Sun, J.Z. and Debrosse, J.K.
	and Worledge, D.C. and Maffitt, T.M. and Gallagher, W.J.},
  title = {A Study of Write Margin of Spin Torque Transfer Magnetic Random Access
	Memory Technology},
  journal = {Magnetics, IEEE Transactions on},
  year = {2010},
  volume = {46},
  pages = {2322 -2327},
  number = {6},
  month = {june },
  abstract = {Key design parameters of 64 Mb STT-MRAM at 90-nm technology node are
	discussed. A design point was developed with adequate TMR for fast
	read operation, enough energy barrier for data retention and against
	read disturbs, a write voltage satisfying the long term reliability
	against dielectric breakdown and a write bit error rate below 10-9.
	A direct experimental method was developed to determine the data
	retention lifetime that avoids the discrepancy in the energy barrier
	values obtained with spin current- and field-driven switching measurements.
	Other parameters detrimental to write margins such as backhopping
	and the existence of a low breakdown population are discussed. At
	low bit-error regime, new phenomenon emerges, suggestive of a bifurcation
	of switching modes. The dependence of the bifurcated switching threshold
	on write pulse width, operating temperature, junction dimensions
	and external field were studied. These show bifurcated switching
	to be strongly influenced by thermal fluctuation related to the spatially
	inhomogeneous free layer magnetization. An external field along easy
	axis direction assisting switching was shown to be effective for
	significantly reducing the percentage of MTJs showing bifurcated
	switching.},
  doi = {10.1109/TMAG.2010.2043069},
  file = {:PDF\\05467625.pdf:PDF},
  issn = {0018-9464},
  keywords = {MTJ;STT-MRAM;TMR;bifurcated switching threshold;bit error regime;dielectric
	breakdown;magnetic random access memory technology;magnetic tunneling
	junction;magnetization;nanotechnology node;reliability;size 90 nm;spin
	torque transfer;storage capacity 64 Mbit;thermal fluctuation;tunneling
	magnetoresistance;write margin;write pulse width;MRAM devices;electric
	breakdown;nanotechnology;reliability;tunnelling magnetoresistance;},
  timestamp = {2011.06.03}
}

@ARTICLE{817571,
  author = {Miranda, E. and Sune, J. and Rodriguez, R. and Nafria, M. and Aymerich,
	X. and Fonseca, L. and Campabadal, F.},
  title = {Soft breakdown conduction in ultrathin (3-5 nm) gate dielectrics
	},
  journal = {Electron Devices, IEEE Transactions on},
  year = {2000},
  volume = {47},
  pages = {82 -89},
  number = {1},
  month = jan,
  abstract = {Prior to any attempt to model a charge transport mechanism, a precise
	knowledge of the parameters on which the current depends is essential.
	In this work, the soft breakdown (SBD) failure mode of ultrathin
	(3-5 nm) SiO2 layers in polysilicon-oxide-semiconductor structures
	is investigated. This conduction regime is characterized by a large
	leakage current and by multilevel current fluctuations, both at low
	applied voltages. In order to obtain a general picture of SBD, room-temperature
	current-voltage (I-V) measurements have been performed on samples
	with different gate areas, oxide thicknesses and substrate types.
	An astounding matching between some of these I-V characteristics
	has been found. The obtained results and the comparison with the
	final breakdown regime suggest that the current flow through a SBD
	spot is largely influenced by its atomic-scale dimensions as occurs
	in a point contact configuration. Experimental data are also presented
	which demonstrate that specific current fluctuations can be ascribed
	to a blocking behavior of unstable SBD conduction channels},
  doi = {10.1109/16.817571},
  file = {:PDF\\Soft_Breakdown_Conduction_in_Ultrathin(3-5nm)_Gate_Dielectrics.pdf:PDF},
  issn = {0018-9383},
  keywords = {3 to 5 nm;I-V characteristics;SiO2;atomic-scale dimensions;breakdown
	regime;charge transport mechanism;current fluctuations;failure mode;gate
	areas;leakage current;multilevel current fluctuations;oxide thicknesses;point
	contact configuration;polysilicon-oxide-semiconductor structures;room-temperature
	current-voltage measurements;soft breakdown conduction;ultrathin
	gate dielectrics;MIS devices;dielectric thin films;failure analysis;leakage
	currents;semiconductor device breakdown;semiconductor device reliability;silicon
	compounds;}
}

@ARTICLE{1519184,
  author = {Mita, S. and Matsui, H.},
  title = {An effective error correction using a combination of algebraic geometric
	codes and Parity codes for HDD},
  journal = {Magnetics, IEEE Transactions on},
  year = {2005},
  volume = {41},
  pages = { 2992 - 2994},
  number = {10},
  month = {oct},
  abstract = {This paper describes the performance of an efficient error-correcting
	system for hard disk drives. The performance of the codes on algebraic
	curves, such as Hermitian codes over GF(28), elliptic codes over
	GF(29), and Fermat codes over GF(210) is compared with that of conventional
	Reed-Solomon (RS) codes. In particular, an adoption of Hermitian
	codes can reduce the redundant part by approximately 800 bits more
	than the RS codes when an error-correcting capability of 240 bytes
	is adopted for a long sector size. Moreover, we propose an error-correcting
	system based on a combination of algebraic geometric codes and parity
	codes. This combination system can cover a bit-error rate of approximately
	10-2 under a condition of EEPR4 channel and additive Gaussian noise.},
  doi = {10.1109/TMAG.2005.854451},
  file = {:PDF\\01519184.pdf:PDF},
  issn = {0018-9464},
  keywords = { Bahl-Cocke-Jelinek-Raviv algorithm; EEPR4 channel; Fermat codes;
	Hermitian codes; Reed-Solomon codes; additive Gaussian noise; algebraic
	curves; algebraic geometric codes; belief propagation algorithm;
	bit-error rate; elliptic codes; error correction; hard disk drives;
	magnetic recording channels; parity codes; Reed-Solomon codes; algebraic
	geometric codes; error correction codes; error statistics; hard discs;
	parity check codes;},
  timestamp = {2011.06.14}
}

@INPROCEEDINGS{4339706,
  author = {Miura, K. and Kawahara, T. and Takemura, R. and Hayakawa, J. and
	Ikeda, S. and Sasaki, R. and Takahashi, H. and Matsuoka, H. and Ohno,
	H.},
  title = {A novel SPRAM (SPin-transfer torque RAM) with a synthetic ferrimagnetic
	free layer for higher immunity to read disturbance and reducing write-current
	dispersion},
  booktitle = {VLSI Technology, 2007 IEEE Symposium on},
  year = {2007},
  pages = {234 -235},
  month = {june},
  abstract = {A novel SPRAM (spin-transfer torque RAM) consisting of MgO-barrier-based
	magnetic tunnel junctions (MTJs) with a synthetic ferrimagnetic (SyF)
	structure in a free layer was demonstrated for both higher immunity
	to read disturbance and a sufficient margin between the read and
	write currents. Since magnetization of the free layer becomes stable
	against thermal fluctuation with increasing thermal-stability factor
	E/kBT, the SyF free layer of the MTJs realized a magnetic information
	retention of over 10 years due to its high E/kBT of 67. Furthermore,
	it was found that the SyF free layer has an advantage of reducing
	dispersion of write-current density Jc, which is necessary for securing
	an adequate margin between the read and write currents.},
  doi = {10.1109/VLSIT.2007.4339706},
  file = {:PDF\\04339706.pdf:PDF},
  keywords = {MgO;SPRAM;magnetic information retention;magnetic tunnel junctions;read
	disturbance;spin-transfer torque RAM;synthetic ferrimagnetic free
	layer;thermal fluctuation;thermal-stability;write-current density;write-current
	dispersion;ferrimagnetic materials;magnetic tunnelling;magnetisation;random-access
	storage;},
  timestamp = {2011.06.03}
}

@INPROCEEDINGS{1309947,
  author = {Modelli, A. and Visconti, A. and Bez, R.},
  title = {Advanced flash memory reliability},
  booktitle = {Integrated Circuit Design and Technology, 2004. ICICDT '04. International
	Conference on},
  year = {2004},
  pages = { 211 - 218},
  abstract = { With reference to the mainstream technology, the most relevant failure
	mechanisms that affect reliability of Flash memory are reviewed,
	showing the primary role played by tunnel oxide defects. The degradation
	of device. performance induced by program/erase cycling is discussed,
	specifically for what concern the leakage that affect a very small
	fraction of memory cells after cycling. The dependence of the leakage
	on tunnel oxide thickness, number of cycles, and temperature is analyzed.
	The leakage current is explained by trap-assisted tunneling involving
	one, two or more traps, with decreasing occurrence probability. Finally,
	data are presented showing the robustness of scaled Flash memory
	to alpha particles and electromagnetic radiation.},
  doi = {10.1109/ICICDT.2004.1309947},
  file = {:PDF\\Advanced_Flash_Memory_Reliability.pdf:PDF},
  issn = { },
  keywords = { Fowler-Nordheim tunneling; advanced flash memory reliability; alpha
	particles robustness; channel hot-electron injection; data retention;
	device. performance degradation; electromagnetic radiation robustness;
	electron-hole pairs; failure mechanisms; leakage current; memory
	endurance; number of cycles; program-erase cycling; single floating-gate
	transistor; temperature dependence; trap-assisted tunneling; tunnel
	oxide defects; tunnel oxide thickness; flash memories; hot carriers;
	integrated circuit reliability; leakage currents; radiation hardening
	(electronics); tunnelling;}
}

@INPROCEEDINGS{Monagan:2006:RSM:1145768.1145809,
  author = {Monagan, Michael and Pearce, Roman},
  title = {Rational simplification modulo a polynomial ideal},
  booktitle = {Proceedings of the 2006 international symposium on Symbolic and algebraic
	computation},
  year = {2006},
  series = {ISSAC '06},
  pages = {239--245},
  address = {New York, NY, USA},
  publisher = {ACM},
  acmid = {1145809},
  doi = {10.1145/1145768.1145809},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\p239-monagan.pdf:PDF},
  isbn = {1-59593-276-3},
  keywords = {Groebner Bases, field of fractions, quotient ring, rational expression,
	side relations, simplification},
  location = {Genoa, Italy},
  numpages = {7},
  timestamp = {2012.05.16},
  url = {http://doi.acm.org/10.1145/1145768.1145809}
}

@BOOK{1088886,
  title = {Error Correction Coding: Mathematical Methods and Algorithms},
  publisher = {Wiley-Interscience},
  year = {2005},
  author = {Moon, Todd K.},
  isbn = {0471648000}
}

@INPROCEEDINGS{6167471,
  author = {Motwani, R. and Chong Ong},
  title = {Robust decoder architecture for multi-level flash memory storage
	channels},
  booktitle = {Computing, Networking and Communications (ICNC), 2012 International
	Conference on},
  year = {2012},
  pages = {492 -496},
  month = {30 2012-feb. 2},
  abstract = {Multi-level-cell (MLC) flash memory comprises of cells which can be
	programmed to multiple levels. Recent MLC flash memory systems support
	3 bits per cell to 4 bits per cell which means that the individual
	cells are programmed to 8 or 16 distinct levels respectively. MLC
	flash has higher raw bit error rate (rber) than the single bit per
	cell flash. This calls for the use of sophisticated error control
	coding (ECC) schemes like LDPC codes. The flash memory channel is
	usually modeled with level distributions having Gaussian probability
	density function. However, the Gaussian distribution is not a good
	fit for practical NAND channels. Even for the beginning of life of
	the flash memory device, this model has to be replaced by a more
	realistic model. With erases and re-writes, the channel towards the
	end of life of the device is far from Gaussian and floating-gate
	to floating-gate coupling and charge loss not only increases the
	raw bit error rate but causes the level distributions to become asymmetric.
	Given such channel impairments, if the LDPC decoder assumes Gaussian
	level distributions, its performance can degrade considerably. Hence,
	modifications are required in the decoding algorithm to cope up with
	the channel distortions. In order to keep the hardware costs within
	limits, simple schemes with minimal hardware increase are desirable
	to keep up the performance. In this paper, the flash channel degradations
	are first enlisted and then simple solutions are proposed which keep
	the performance in check as the flash memory transits from a channel
	with moderate impairments to the end of life condition, where the
	level distributions for the different levels are highly asymmetric.},
  doi = {10.1109/ICCNC.2012.6167471},
  file = {:PDF\\06167471.pdf:PDF},
  keywords = {Gaussian level distributions;Gaussian probability density function;LDPC
	decoder;MLC flash memory systems;NAND channels;bit error rate;channel
	distortions;charge loss;error control coding schemes;flash channel
	degradations;flash memory channel transition;floating-gate coupling
	device;individual cells;multilevel cell flash memory storage channels;robust
	decoder architecture;Gaussian distribution;channel coding;error statistics;flash
	memories;parity check codes;},
  timestamp = {2012.10.22}
}

@ARTICLE{1506715,
  author = {Seho Myung and Kyeongcheol Yang},
  title = {A combining method of quasi-cyclic LDPC codes by the Chinese remainder
	theorem},
  journal = {Communications Letters, IEEE},
  year = {2005},
  volume = {9},
  pages = { 823 - 825},
  number = {9},
  month = {sep},
  abstract = { In this paper we propose a method of constructing quasi-cyclic low-density
	parity-check (QC-LDPC) codes of large length by combining QC-LDPC
	codes of small length as their component codes, via the Chinese remainder
	theorem. The girth of the QC-LDPC codes obtained by the proposed
	method is always larger than or equal to that of each component code.
	By applying the method to array codes, we present a family of high-rate
	regular QC-LDPC codes with no 4-cycles. Simulation results show that
	they have almost the same performance as random regular LDPC codes.},
  doi = {10.1109/LCOMM.2005.1506715},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\01506715.pdf:PDF},
  issn = {1089-7798},
  keywords = { Chinese remainder theorem; QC-LDPC; array codes; component code;
	quasi-cyclic low-density parity-check code; random code; cyclic codes;
	parity check codes; random codes;},
  timestamp = {2011.12.06}
}

@ARTICLE{1468309,
  author = {Myung, S. and Yang, K. and Kim, J.},
  title = {Quasi-cyclic LDPC codes for fast encoding},
  journal = {Information Theory, IEEE Transactions on},
  year = {2005},
  volume = {51},
  pages = {2894 -2901},
  number = {8},
  month = aug,
  abstract = {In this correspondence we present a special class of quasi-cyclic
	low-density parity-check (QC-LDPC) codes, called block-type LDPC
	(B-LDPC) codes, which have an efficient encoding algorithm due to
	the simple structure of their parity-check matrices. Since the parity-check
	matrix of a QC-LDPC code consists of circulant permutation matrices
	or the zero matrix, the required memory for storing it can be significantly
	reduced, as compared with randomly constructed LDPC codes. We show
	that the girth of a QC-LDPC code is upper-bounded by a certain number
	which is determined by the positions of circulant permutation matrices.
	The B-LDPC codes are constructed as irregular QC-LDPC codes with
	parity-check matrices of an almost lower triangular form so that
	they have an efficient encoding algorithm, good noise threshold,
	and low error floor. Their encoding complexity is linearly scaled
	regardless of the size of circulant permutation matrices.},
  doi = {10.1109/TIT.2005.851753},
  file = {:PDF\\Quasi-Cyclic_LDPC_Codes_for_Fast_Encoding.pdf:PDF},
  issn = {0018-9448},
  keywords = {QC-LDPC;block-type LDPC code;circulant permutation matrix;encoding
	algorithm;quasicyclic low-density parity-check code;AWGN channels;block
	codes;channel coding;cyclic codes;matrix algebra;parity check codes;}
}

@ARTICLE{5257335,
  author = {Nakamura, Y. and Okamoto, Y. and Osawa, H. and Aoi, H. and Muraoka,
	H.},
  title = {A Study of LDPC Coding and Iterative Decoding System in Magnetic
	Recording System Using Bit-Patterned Medium With Write Error},
  journal = {Magnetics, IEEE Transactions on},
  year = {2009},
  volume = {45},
  pages = {3753 -3756},
  number = {10},
  month = oct,
  abstract = {In this paper, low-density parity-check (LDPC) coding and iterative
	decoding system is studied in the magnetic recording system using
	a bit-patterned medium (BPM). The magnetic recording system using
	BPM has a serious problem of requiring precise write synchronization
	to correctly write the recording sequence on each intended island.
	In this paper, an error locator using the parity-check matrix of
	LDPC code is utilized to curb the influence of write error on the
	iterative decoding process. The performance of the proposed system
	with the error locator is evaluated by computer simulation for magnetic
	recording system using BPM with synchronization write error at a
	recording density of 2 Tb/in2, and it is compared with a conventional
	Reed-Solomon (RS) encoding and decoding system. The results show
	that the proposed system provides better performance.},
  doi = {10.1109/TMAG.2009.2022331},
  file = {:PDF\\A_Study_of_LDPC_Coding_and_Iterative_Decoding_System_in_Magnetic_Recording_System_Using_Bit-Patterned_Medium_with_Write_Error.pdf:PDF},
  issn = {0018-9464},
  keywords = {LDPC coding;bit-patterned medium;iterative decoding system;low-density
	parity-check coding;magnetic recording system;recording sequence;write
	error;write synchronization;error statistics;iterative decoding;magnetic
	recording;parity check codes;synchronisation;}
}

@INPROCEEDINGS{5474157,
  author = {Byung-Woo Nam and Gap-Joo Na and Sang-Won Lee},
  title = {A Hybrid Flash Memory SSD Scheme for Enterprise Database Applications},
  booktitle = {Web Conference (APWEB), 2010 12th International Asia-Pacific},
  year = {2010},
  pages = {39 -44},
  month = {april},
  abstract = {Flash memory has many advantages such as high performance, low electronic
	power, non-volatile storage and physical stability, over hard-disks.
	For this reason, flash memory has been deployed as data storage for
	mobile devices, including PDAs, MP3 players, laptop-computers and
	database systems. According to the cell type, flash memory can be
	divided into SLC(Single Level Chip) and MLC(Multi Level Chip). In
	general, SLC is known to have high performance and longer lifetime
	(i.e. more than 100 K wear-leveling) while MLC is to offer larger
	capacity and with low price but have wear leveling of not longer
	than 10 K. In this paper, we show that it is possible to design a
	fast and cost-efficient storage by combining two types of flash memories
	in a hybrid fashion. Specifically, we propose a hybrid flash memory
	solid state disk(SSD) scheme using FAST FTL for enterprise applications,
	where SLC chip is used as the log space for FAST while MLC chips
	store the normal data blocks. SLC chips allow fast and durable performance
	for write while MLC chips provide the large capacity. And, this is
	mainly due to the FAST FTL algorithm's characteristics: it tends
	to direct the random writes to SLC chips and direct the other most
	random read to MLC chips. By taking the advantages of both chip types,
	we can find an economically desirable flash SSD design option. Experimental
	results show that our hybrid flash SSD scheme outperforms MLC-only
	flash scheme by far both in terms of performance and price.},
  doi = {10.1109/APWeb.2010.70},
  file = {:PDF\\05474157.pdf:PDF},
  keywords = {data storage;enterprise database applications;flash memory solid state
	disk;flash translation layer;hybrid flash memory SSD scheme;multilevel
	chip;nonvolatile storage;physical stability;single level chip;wear
	leveling;business data processing;database management systems;flash
	memories;},
  timestamp = {2011.05.20}
}

@ARTICLE{springerlink:10.1023/A:1004614130865,
  author = {Newman, M.E.J. and Palmer, R.G.},
  title = {Error Estimation in the Histogram Monte Carlo Method},
  journal = {Journal of Statistical Physics},
  year = {1999},
  volume = {97},
  pages = {1011-1026},
  note = {10.1023/A:1004614130865},
  abstract = {We examine the sources of error in the histogram reweighting method
	for Monte Carlo data analysis. We demonstrate that, in addition to
	the standard statistical error which has been studied elsewhere,
	there are two other sources of error, one arising through correlations
	in the reweighted samples, and one arising from the finite range
	of energies sampled by a simulation of finite length. We demonstrate
	that while the former correction is usually negligible by comparison
	with statistical fluctuations, the latter may not be, and give criteria
	for judging the range of validity of histogram extrapolations based
	on the size of this latter correction.},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\springerlink_10_1023_A_100461413865.pdf:PDF},
  issn = {0022-4715},
  issue = {5},
  keyword = {Physics and Astronomy},
  publisher = {Springer Netherlands},
  timestamp = {2012.03.12},
  url = {http://dx.doi.org/10.1023/A:1004614130865}
}

@ARTICLE{6169192,
  author = {Nguyen, D.V. and Chilappagari, S.K. and Marcellin, M.W. and Vasic,
	B.},
  title = {On the Construction of Structured LDPC Codes Free of Small Trapping
	Sets},
  journal = {Information Theory, IEEE Transactions on},
  year = {2012},
  volume = {58},
  pages = {2280-2302},
  number = {4},
  abstract = {We present a method to construct low-density parity-check (LDPC) codes
	with low error floors on the binary symmetric channel. Codes are
	constructed so that their Tanner graphs are free of certain small
	trapping sets. These trapping sets are selected from the trapping
	set ontology for the Gallager A/B decoder. They are selected based
	on their relative harmfulness for a given decoding algorithm. We
	evaluate the relative harmfulness of different trapping sets for
	the sum-product algorithm by using the topological relations among
	them and by analyzing the decoding failures on one trapping set in
	the presence or absence of other trapping sets. We apply this method
	to construct structured LDPC codes. To facilitate the discussion,
	we give a new description of structured LDPC codes whose parity-check
	matrices are arrays of permutation matrices. This description uses
	Latin squares to define a set of permutation matrices that have disjoint
	support and to derive a simple necessary and sufficient condition
	for the Tanner graph of a code to be free of four cycles.},
  doi = {10.1109/TIT.2011.2173733},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\06169192.pdf:PDF},
  issn = {0018-9448},
  keywords = {binary codes;decoding;matrix algebra;parity check codes;Gallager A-B
	decoder;Latin squares;Tanner graphs;binary symmetric channel;decoding
	failures;low error floors;low-density parity-check codes;parity-check
	matrices;permutation matrices;small trapping sets;structured LDPC
	code;sum-product algorithm;trapping set ontology;Algorithm design
	and analysis;Charge carrier processes;Databases;Decoding;Iterative
	decoding;Merging;Error floor;Latin squares;structured low-density
	parity-check codes;trapping sets},
  timestamp = {2013.11.08}
}

@BOOK{nishimori_spinglass,
  title = {Statistical Physics of Spin Glasses and Information Processing: An
	Introduction},
  publisher = {Oxford Univ Pr on Demand},
  year = {2001},
  author = {Hidetoshi Nishimori},
  month = {September},
  abstract = {Spin glasses are magnetic materials. Statistical mechanics, a subfield
	of physics, has been a powerful tool to theoretically analyse various
	unique properties of spin glasses. A number of new analytical techniques
	have been developed to establish a theory of spin glasses. Surprisingly,
	these techniques have turned out to offer new tools and viewpoints
	for the understanding of information processing problems, including
	neural networks, error-correcting codes, image restoration, and optimization
	problems. This book is one of the first publications of the past
	ten years that provide a broad overview of this interdisciplinary
	field. Most of the book is written in a self-contained manner, assuming
	only a general knowledge of statistical mechanics and basic probability
	theory. It provides the reader with a sound introduction to the field
	and to the analytical techniques necessary to follow its most recent
	developments.},
  isbn = {978-0198509417},
  timestamp = {2012.03.16},
  totalpages = {243}
}

@ARTICLE{1580807,
  author = {O'Sullivan, M.E.},
  title = {Algebraic construction of sparse matrices with large girth},
  journal = {Information Theory, IEEE Transactions on},
  year = {2006},
  volume = {52},
  pages = {718 -727},
  number = {2},
  month = {feb. },
  abstract = {In this correspondence, we present a method for constructing sparse
	matrices that have a compact description and whose associated bipartite
	graphs have large girth. Based on an arbitrary seed matrix of nonnegative
	integers a new matrix is constructed which replaces each entry of
	the seed matrix with a sum of permutation matrices. Algebraic conditions
	that lead to short cycles in the associated bipartite graph are analyzed
	and methods to achieve large girth in two special cases are presented.
	In one, all the permutation matrices are circulants; in the other
	they are all affine permutation matrices. When used to define a low-density
	parity-check (LDPC) code the compact description should lead to efficient
	implementation and the large girth to good error correction performance.
	The method is adaptable to a variety of rates, and a variety of row
	and column degrees},
  doi = {10.1109/TIT.2005.862120},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\01580807.pdf:PDF},
  issn = {0018-9448},
  keywords = {LDPC;algebraic construction;arbitrary seed matrix;associated bipartite
	graph;circulant permutation matrix;error correction performance;large
	girth;low-density parity-check code;nonnegative integer;sparse matrix;error
	correction codes;graph theory;parity check codes;sparse matrices;},
  timestamp = {2011.12.06}
}

@ARTICLE{4860408,
  author = {Ogawa, Shigeo and Shiono, Noboru and Shimaya, Masakazu},
  title = {Neutral electron trap generation in SiO2 by hot holes},
  journal = {Applied Physics Letters},
  year = {1990},
  volume = {56},
  pages = {1329 -1331},
  number = {14},
  month = apr,
  abstract = {Experimental evidence for neutral electron trap generation in SiO2
	caused by injected holes is presented. The neutral electron traps
	are detected by Fowler #x2013;Nordheim (FN) tunneling electron injection
	after avalanche hole injection. The density of generated neutral
	traps increases with the number of injected holes, but does not saturate
	with that of the trapped holes. The centroid of generated neutral
	traps is found to be in the middle of the oxide. These results suggest
	that neutral traps are generated by the holes and not only by the
	recombination of electrons with trapped holes. The origin of neutral
	traps is considered to be associated with dipolar defects formed
	by SiO bond breaking under hole transport in the oxide.},
  doi = {10.1063/1.103200},
  file = {:PDF\\APL_Vol56_P1329.pdf:PDF},
  issn = {0003-6951}
}

@ARTICLE{Ogita05accuratesum,
  author = {Takeshi Ogita and Siegfried M. Rump and Shin'ichi Oishi},
  title = {Accurate Sum and Dot Product},
  journal = {SIAM J. Sci. Comput},
  year = {2005},
  volume = {26},
  pages = {2005},
  abstract = {Algorithms for summation and dot product of floating point numbers
	are presented which are fast in terms of measured computing time.
	We show that the computed results are as accurate as if computed
	in twice or K-fold working precision, K 3. For twice the working
	precision our algorithms for summation and dot product are some 40
	% faster than the corresponding XBLAS routines while sharing similar
	error estimates. Our algorithms are widely applicable because they
	require only addition, subtraction and multiplication of floating
	point numbers in the same working precision as the given data. Higher
	precision is unnecessary, algorithms are straight loops without branch,
	and no access to mantissa or exponent is necessary.},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\Ogita05accuratesum.pdf:PDF},
  timestamp = {2011.08.27}
}

@ARTICLE{4806138,
  author = {Daesun Oh and Parhi, K.K.},
  title = {Low-Complexity Switch Network for Reconfigurable LDPC Decoders},
  journal = {Very Large Scale Integration (VLSI) Systems, IEEE Transactions on},
  year = {2010},
  volume = {18},
  pages = {85-94},
  number = {1},
  abstract = {In this paper, we propose an efficient low-complexity switch network
	design for reconfigurable low-density parity-check (LDPC) decoders.
	The proposed architecture leads to significant reductions in hardware
	complexity. Since the structured quasi-cyclic (QC) LDPC codes for
	most modern wireless communication systems include multiple code
	rates, various block lengths, and different sizes of submatrices,
	a reconfigurable LDPC decoder is desirable and the barrel shifter
	needs to be programmable. The Benes network cannot be optimized as
	the barrel shifter for a reconfigurable LDPC decoder when the input
	size of barrel shifter is not a power of 2. Also, it is not trivial
	to generate all the control signals on-the-fly for numerous 2 ??
	2 switches in the switch network. In this paper, a novel low-complexity
	switch network design is proposed, which can be used efficiently
	when the input size of barrel shifters is not a power of 2. Furthermore,
	we propose a novel algorithm to generate all the control signals,
	which can be implemented with a small size of lookup table (LUT)
	or a simple combination logic on-the-fly, using the properties that
	both the full-size switch network can be broken into two half-size
	switch networks and the barrel shifters for the structured QC LDPC
	decoders require only cyclic shifts. Compared with conventional Benes
	networks using a dedicated LUT or a complicated signal generating
	algorithm, the proposed architectures achieve significant hardware
	reductions in implementing the barrel shifters for reconfigurable
	LDPC decoders. In synthesis result using the TSMC 0.18-??m standard
	cell CMOS technology, the proposed switch network for a reconfigurable
	LDPC decoder of IEEE 802.16e and IEEE 802.11n can be implemented
	with an area of 0.772 mm2, which leads to a significant area reduction.},
  doi = {10.1109/TVLSI.2008.2007736},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\04806138.pdf:PDF},
  issn = {1063-8210},
  keywords = {communication complexity;cyclic codes;parity check codes;table lookup;telecommunication
	switching;Benes network;IEEE 802.11n;IEEE 802.16e;TSMC 0.18-??m standard
	cell CMOS technology;barrel shifter;combination logic on-the-fly;complicated
	signal generating algorithm;hardware complexity;lookup table;low-complexity
	switch network;modern wireless communication systems;reconfigurable
	low-density parity-check decoders;size 0.18 mum;structured quasi-cyclic
	low-density parity-check decoders codes;Barrel shifter;Benes network;VLSI;low-density
	parity-check (LDPC) decoder;reconfigurable;switch network},
  timestamp = {2013.05.28}
}

@INPROCEEDINGS{4253023,
  author = {Daesun Oh and Parhi, K.K.},
  title = {Efficient Highly-Parallel Decoder Architecture for Quasi-Cyclic Low-Density
	Parity-Check Codes},
  booktitle = {Circuits and Systems, 2007. ISCAS 2007. IEEE International Symposium
	on},
  year = {2007},
  pages = {1855 -1858},
  month = may,
  abstract = {In this paper, the authors propose an efficient highly-parallel decoder
	architecture using partially overlapped decoding scheme for quasi-cyclic
	(QC) low-density parity-check (LDPC) codes, which leads to reduction
	in hardware complexity and power consumption. Generally, due to the
	regularly structured parity-check matrix H of QC LDPC codes, the
	message updating computations in the check node unit (CNU) and the
	variable node unit (VNU) can be efficiently overlapped, which increases
	the decoding throughput by maximizing the hardware utilization efficiency
	(HUE). However, the partially overlapped decoding scheme cannot be
	used to design a highly-parallel decoding architecture for high-throughput
	applications. For (3, 5)-regular QC LDPC codes, our proposed method
	could reduce the hardware complexity by approximately 33% for the
	CNU and 20% for the VNU in the highly-parallel decoder architecture
	without any performance degradation. In addition, the power consumption
	can be minimized by reducing the total number of memory accesses
	for updated messages.},
  doi = {10.1109/ISCAS.2007.378276},
  file = {:PDF\\Efficient_Highly-Parallel_Decoder_Architecture_for_Quasi-Cyclic_Low-Density_Parity_Check_Codes.pdf:PDF},
  keywords = {CNU;HUE;LDPC;VNU;check node unit;hardware utilization efficiency;highly-parallel
	decoder;low-density parity-check codes;quasi-cyclic codes;variable
	node unit;cyclic codes;decoding;parity check codes;}
}

@ARTICLE{PhysRev.65.117,
  author = {Onsager, Lars},
  title = {Crystal Statistics. I. A Two-Dimensional Model with an Order-Disorder
	Transition},
  journal = {Phys. Rev.},
  year = {1944},
  volume = {65},
  pages = {117--149},
  month = {Feb},
  doi = {10.1103/PhysRev.65.117},
  file = {:PDF\\PhysRev.65.117.pdf:PDF},
  issue = {3-4},
  publisher = {American Physical Society},
  timestamp = {2012.08.21},
  url = {http://link.aps.org/doi/10.1103/PhysRev.65.117}
}

@INPROCEEDINGS{5746286,
  author = {Otsuka, W. and Miyata, K. and Kitagawa, M. and Tsutsui, K. and Tsushima,
	T. and Yoshihara, H. and Namise, T. and Terao, Y. and Ogata, K.},
  title = {A 4Mb conductive-bridge resistive memory with 2.3GB/s read-throughput
	and 216MB/s program-throughput},
  booktitle = {Solid-State Circuits Conference Digest of Technical Papers (ISSCC),
	2011 IEEE International},
  year = {2011},
  pages = {210 -211},
  month = {feb.},
  abstract = {The growing demand for higher performance in the storage and access
	of data in various consumer electronic and computing devices has
	driven the development of nonvolatile memory (NVM) technologies.
	The promising candidates for future NVM such as FeRAM and PCM have
	demonstrated shorter access time, faster programming and wide read/write
	bandwidth in the chip and the memory macro. Resistive memory (ReRAM)
	is also one of alternative NVMs, because of its low operating voltage,
	high speed and good scalability. Several types of ReRAM characteristics
	have been investigated on memory array. However, most are limited
	in terms of memory array performance because of not having suitable
	read/write circuit for ReRAM. In this work, we present a 4Mb conductive
	bridge ReRAM test macro realizing 2.3GB/S read-throughput, 216MB/S
	program-throughput and robust reliability results by using read/write
	fully functional device technology with direct sense in programming
	(DSIP) method.},
  doi = {10.1109/ISSCC.2011.5746286},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05746286.pdf:PDF},
  issn = {0193-6530},
  keywords = {computing device;conductive bridge ReRAM test macro;conductive-bridge
	resistive memory;consumer electronic;memory array performance;memory
	macro;nonvolatile memory technology;program-throughput;read-throughput;read/write
	circuit;read/write fully functional device technology;robust reliability;storage
	capacity 4 Mbit;memory architecture;random-access storage;},
  timestamp = {2011.08.24}
}

@ARTICLE{796391,
  author = {Ozbudak, F. and Stichtenoth, H.},
  title = {Constructing codes from algebraic curves},
  journal = {Information Theory, IEEE Transactions on},
  year = {1999},
  volume = {45},
  pages = {2502 -2505},
  number = {7},
  month = nov,
  abstract = {We discuss some previous constructions of codes from algebraic curves
	due to Xing, Niederreiter and Lam (see ibid., vol.45, no.7, p.2498-2501,
	1999), and we investigate their relations with Goppa's (1981) algebraic-geometric
	codes},
  doi = {10.1109/18.796391},
  file = {:PDF\\00796391.pdf:PDF},
  issn = {0018-9448},
  keywords = {Goppa algebraic-geometric codes;algebraic curves;algebraic function
	fields;code construction;linear codes;Goppa codes;algebraic geometric
	codes;linear codes;}
}

@ARTICLE{springerlink:10.1007/s00200-007-0048-7,
  author = {O’Keeffe, Henry and Fitzpatrick, Patrick},
  title = {Gr\"{o}bner basis approach to list decoding of algebraic geometry
	codes},
  journal = {Applicable Algebra in Engineering, Communication and Computing},
  year = {2007},
  volume = {18},
  pages = {445-466},
  note = {10.1007/s00200-007-0048-7},
  abstract = {We show how our Gröbner basis algorithm, which was previously applied
	to list decoding of Reed Solomon codes, can be used in the hard and
	soft decision list decoding of Algebraic Geometry codes. In addition,
	we present a linear functional version of our Gröbner basis algorithm
	in order to facilitate comparisons with methods based on duality.},
  affiliation = {National University of Ireland Boole Centre for Research in Informatics
	Cork Ireland},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\springerlink_10.1007_s00200-007-0048-7.pdf:PDF},
  issn = {0938-1279},
  keyword = {Computer Science},
  publisher = {Springer Berlin / Heidelberg},
  timestamp = {2012.05.15},
  url = {http://dx.doi.org/10.1007/s00200-007-0048-7}
}

@INPROCEEDINGS{613165,
  author = {Paar, C.},
  title = {Optimized arithmetic for Reed-Solomon encoders},
  year = {1997},
  pages = {250},
  month = {jun.},
  doi = {10.1109/ISIT.1997.613165},
  file = {:PDF\\Optimized_arithmetic_for_Reed-Solomon_encoders.pdf:PDF},
  journal = {Information Theory. 1997. Proceedings., 1997 IEEE International Symposium
	on},
  keywords = {Galois fields;Reed-Solomon encoders;arithmetic operation;low complexity
	constant multipliers;multiplication;optimization algorithms;optimized
	arithmetic;Galois fields;Reed-Solomon codes;arithmetic codes;computational
	complexity;matrix multiplication;optimisation;}
}

@INPROCEEDINGS{5873231,
  author = {Papandreou, N. and Pozidis, H. and Mittelholzer, T. and Close, G.F.
	and Breitwisch, M. and Lam, C. and Eleftheriou, E.},
  title = {Drift-Tolerant Multilevel Phase-Change Memory},
  booktitle = {Memory Workshop (IMW), 2011 3rd IEEE International},
  year = {2011},
  pages = {1 -4},
  month = {may},
  abstract = {Multilevel-cell (MLC) storage is a typical way for achieving increased
	capacity and thus lower cost-per-bit in memory technologies. In phase-change
	memory (PCM), however, MLC storage is seriously hampered by the phenomenon
	of resistance drift. Reference cells may be used to offer some relief,
	however their effectiveness is limited due to the stochastic nature
	of drift. In this paper, an alternative way to cope with drift in
	PCM is introduced, based on modulation coding. The new drift tolerant
	coding technique encodes information in the relative order of resistance
	levels in a codeword. Experimental results from a 90-nm PCM prototype
	chip demonstrate the effectiveness of the proposed method in offering
	high resilience to drift. Most notably, 4 levels/cell storage with
	raw bit-error-rates in the order of 10-5 is achieved in a 200 kcell
	array and maintained for over 30 days after programming at room temperature.},
  doi = {10.1109/IMW.2011.5873231},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05873231.pdf:PDF},
  keywords = {MLC storage;bit-error-rates;drift tolerant coding technique;drift-tolerant
	multilevel phase-change memory;modulation coding;multilevel-cell
	storage;reference cells;resistance drift;room temperature;size 90
	nm;temperature 293 K to 298 K;error statistics;modulation coding;phase
	change memories;},
  timestamp = {2011.07.09}
}

@INPROCEEDINGS{5341269,
  author = {Kwanghee Park and Dong-Hwan Lee and Youngjoo Woo and Geunhyung Lee
	and Ju-Hong Lee and Deok-Hwan Kim},
  title = {Reliability and performance enhancement technique for SSD array storage
	system using RAID mechanism},
  booktitle = {Communications and Information Technology, 2009. ISCIT 2009. 9th
	International Symposium on},
  year = {2009},
  pages = {140 -145},
  month = {sept.},
  abstract = {Recently solid state drive (SSD) based on NAND flash memory chips
	becomes popular in the consumer electronics market because it is
	tough on shock and its I/O performance is better than that of conventional
	hard disk drive. However, as the density of the semiconductor grows
	higher, the distance between its wires narrows down, their interferences
	are frequently occurred, and the bit error rate of semiconductor
	increases. Such frequent error occurrence and short life cycle in
	NAND flash memory reduce the reliability of SSD. In this paper, we
	present reliability and performance enhancement technique on new
	RAID system based on SSD. First, we analyze the existing RAID mechanism
	in the environment of SSD array and then develop a new RAID methodology
	adaptable to SSD array storage system. Via trace-driven simulation,
	we evaluated the performance of our new optimized SSD array storage
	using RAID mechanism. The proposed method enhances the reliability
	of SSD array 2% higher than that of existing RAID system and improves
	the I/O performance of SSD array 28% higher than that of existing
	RAID system.},
  doi = {10.1109/ISCIT.2009.5341269},
  file = {:PDF\\05341269.pdf:PDF},
  keywords = {I/O performance;NAND flash memory chip;RAID mechanism;SSD array storage
	system reliability;bit error rate;consumer electronics market;solid
	state drive;trace-driven simulation;RAID;disc drives;error statistics;flash
	memories;storage management chips;},
  timestamp = {2011.05.19}
}

@INPROCEEDINGS{5564941,
  author = {Sang-Shin Park and Sang-Won Lee},
  title = {Hash join in commercial database with flash memory SSD},
  booktitle = {Computer Science and Information Technology (ICCSIT), 2010 3rd IEEE
	International Conference on},
  year = {2010},
  volume = {4},
  pages = {265 -268},
  month = {july},
  abstract = {Hash join is one of important operations in database system, and its
	performance may slow down because of disk I/O in hash table overflow
	phenomenon. In that phenomenon, the more overflow of hash table occurs,
	the more disk I/O arise, so join performance go from bad to worse.
	Dominant disk I/O patterns of hash join are sequential writes and
	random reads, then flash memory SSD is in a more advantageous position
	than magnetic disk on those I/O patterns. Therefore, using a flash
	memory SSD instead of magnetic disk as a temporary storage of database
	will prevent existing performance degradation. In this paper, we
	show calculated costs by query optimizer in some test cases first.
	And then, we also show empirically hash join performance in these
	test cases on temporary table space created on magnetic disk and
	flash memory SSD. Consequently, the average response time with flash
	memory SSD was about nine times, minimum and twenty times, max faster
	than that with magnetic disk. And you might encounter problems that
	estimated cost by query optimizer and real performance do not correspond
	on magnetic disk. But, that problem was solved on flash memory SSD.},
  doi = {10.1109/ICCSIT.2010.5564941},
  file = {:PDF\\05564941.pdf:PDF},
  keywords = {commercial database system;disk I/O pattern;flash memory SSD;hash
	join;hash table overflow phenomenon;magnetic disk;query optimizer;random
	read;sequential write;flash memories;magnetic disc storage;},
  timestamp = {2011.05.19}
}

@ARTICLE{678579,
  author = {Young-Bog Park and Schroder, D.K.},
  title = {Degradation of thin tunnel gate oxide under constant Fowler-Nordheim
	current stress for a flash EEPROM},
  journal = {Electron Devices, IEEE Transactions on},
  year = {1998},
  volume = {45},
  pages = {1361 -1368},
  number = {6},
  month = {jun},
  abstract = {The degradation of thin tunnel gate oxide under constant Fowler-Nordheim
	(FN) current stress was studied using flash EEPROM structures. The
	degradation is a strong function of the amount of injected charge
	density (Qinj), oxide thickness, and the direction of stress. Positive
	charge trapping is usually dominant at low Qinj followed by negative
	charge trapping at high Qinj , causing a turnaround of gate voltage
	and threshold voltage. Interface trap generation continues to increase
	with increasing stress, as evidenced by subthreshold slope and transconductance.
	Gate injection stress creates more positive charge traps and interface
	traps than does substrate injection stress. Oxide degradation gets
	more severe for thicker oxide, due to more oxide charge trapping
	and interface trap generation by impact ionization. A simple model
	of oxide degradation and breakdown was established based on the experimental
	results. It indicates that the damage in the oxide is more serious
	near the anode interface by impact ionization and oxide breakdown
	is also closely related to surface roughness at the cathode interface.
	When all the damage sites in the oxide connect and a conductive path
	between cathode and anode is formed, oxide breakdown occurs. The
	damage is more serious for thicker oxide because a thicker oxide
	is more susceptible to impact ionization},
  doi = {10.1109/16.678579},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\00678579.pdf:PDF},
  issn = {0018-9383},
  keywords = {constant Fowler-Nordheim current stress;damage sites;flash EEPROM;gate
	injection stress;impact ionization;injected charge density;negative
	charge trapping;oxide degradation;oxide thickness;positive charge
	trapping;substrate injection stress;subthreshold slope;surface roughness;transconductance;tunnel
	gate oxide;EPROM;MOS memory circuits;electric breakdown;electron
	traps;hole traps;impact ionisation;insulating thin films;integrated
	circuit reliability;internal stresses;leakage currents;surface topography;}
}

@ARTICLE{45279,
  author = {Pellikaan, R.},
  title = {On a decoding algorithm for codes on maximal curves},
  journal = {Information Theory, IEEE Transactions on},
  year = {1989},
  volume = {35},
  pages = {1228 -1232},
  number = {6},
  month = {nov},
  abstract = {A decoding algorithm for algebraic geometric codes that was given
	by A.N. Skorobogatov and S.G. Vladut (preprint, Inst. Problems of
	Information Transmission, 1988) is considered. The author gives a
	modified algorithm, with improved performance, which he obtains by
	applying the above algorithm a number of times in parallel. He proves
	the existence of the decoding algorithm on maximal curves by showing
	the existence of certain divisors. However, he has so far been unable
	to give an efficient procedure of finding these divisors},
  doi = {10.1109/18.45279},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\00045279.pdf:PDF},
  issn = {0018-9448},
  keywords = {algebraic geometric codes;decoding algorithm;divisors;maximal curves;codes;decoding;},
  timestamp = {2011.11.10}
}

@BOOK{book_peterson_weldon,
  title = {ERROR-CORRECTING CODES Second Edition},
  publisher = {THE MIT PRESS},
  year = {1972},
  author = {W. W. Petersen and E. J. Weldon, Jr.},
  timestamp = {2011.11.11}
}

@ARTICLE{Piyas19991677,
  author = {Piyas and Samanta},
  title = {Calculation of the probability of hole injection from polysilicon
	gate into silicon dioxide in MOS structures under high-field stress},
  journal = {Solid-State Electronics},
  year = {1999},
  volume = {43},
  pages = {1677 - 1687},
  number = {9},
  abstract = {A theoretical study of the mechanism for hole injection from the degenerately
	doped poly-Si gate into the gate oxide in metal-oxide Silicon (MOS)
	structures under high-field stress (5-15 MV/cm) is presented. Our
	theoretical model for the anode hole injection mechanism (AHI) is
	due to the injection of holes generated by impact ionization of energetic
	electrons entering into the conduction band of poly-Si gate from
	the oxide conduction band under the applied electric field. An analytical
	approach is proposed on the basis of the above injection mechanism
	to estimate the hole injection probability per injected electron
	in order to utilize these data in MOS device simulation under high-field
	Fowler-Nordheim (FN) stress. The computed values of anode hole injection
	probability data are compared with experimental results. Using the
	anode hole injection probability from the present model, we have
	predicted thin tunnel gate oxide degradation under constant current
	and voltage FN stress.},
  doi = {10.1016/S0038-1101(99)00144-6},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\Piyas19991677.pdf:PDF},
  issn = {0038-1101},
  timestamp = {2011.09.23},
  url = {http://www.sciencedirect.com/science/article/pii/S0038110199001446}
}

@INPROCEEDINGS{4290561,
  author = {Prall, K.},
  title = {Scaling Non-Volatile Memory Below 30nm},
  booktitle = {Non-Volatile Semiconductor Memory Workshop, 2007 22nd IEEE},
  year = {2007},
  pages = {5 -10},
  month = {aug},
  abstract = {The future scaling challenges of non-volatile memories for 32 Gb+
	using 30 nm and below feature sizes are discussed. The key challenges
	reviewed include structural integrity, floating gate scaling, floating
	gate replacement, noise and variation. Future trends are discussed.},
  doi = {10.1109/NVSMW.2007.4290561},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\04290561.pdf:PDF},
  keywords = {floating gate replacement;floating gate scaling;nonvolatile memory
	scaling;nanoelectronics;random-access storage;},
  timestamp = {2011.07.16}
}

@INPROCEEDINGS{5351232,
  author = {Presman, N. and Litsyn, S.},
  title = {On upper bounds for the achievable rates of LDPC codes},
  booktitle = {Information Theory Workshop, 2009. ITW 2009. IEEE},
  year = {2009},
  pages = {21 -25},
  month = {oct},
  abstract = {Upper bounds on the mutual entropy of syndrome components of low-density
	parity-check (LDPC) codes are developed. Using these bounds, an upper
	bound is derived on the rates of LDPC codes for which reliable communication
	over a memoryless binary-input symmetric-output (MBIOS) channel is
	achievable. This bound improves on the earlier known bounds due to
	Gallager, Burshtein et al., and Weichmann-Sason.},
  doi = {10.1109/ITW.2009.5351232},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05351232.pdf:PDF},
  keywords = {LDPC codes;low-density parity-check;memoryless binary-input symmetric-output
	channel;mutual entropy;upper bounds;entropy;parity check codes;},
  timestamp = {2011.11.15}
}

@BOOK{NumericalRecipesInC,
  title = {Numerical Recipes with Source Code CD-ROM 3rd Edition : The Art of
	Scientific Computing},
  publisher = { Cambridge University Press},
  year = {2007},
  author = {William H. Press and Saul A. Teukolsky and William T. Vetterling
	and Brian P. Flannery},
  abstract = {This book/CD bundle of the greatly expanded third edition of Numerical
	Recipes now has wider coverage than ever before, many new, expanded
	and updated sections, and two completely new chapters. Co-authored
	by four leading scientists from academia and industry, Numerical
	Recipes starts with basic mathematics and computer science and proceeds
	to complete, working routines. The informal, easy-to-read style that
	made earlier editions so popular is kept throughout. Highlights of
	the new material include: a new chapter on classification and inference,
	Gaussian mixture models, HMMs, hierarchical clustering, and SVMs;
	a new chapter on computational geometry, covering KD trees, quad-
	and octrees, Delaunay triangulation, and algorithms for lines, polygons,
	triangles, and spheres; interior point methods for linear programming;
	MCMC; an expanded treatment of ODEs with completely new routines;
	and many new statistical distributions. For support or further licence
	information please visit www.nr.com.},
  isbn = {978-0521884075}
}

@ARTICLE{Rajwar:2002:TLE:635508.605399,
  author = {Rajwar, Ravi and Goodman, James R.},
  title = {Transactional lock-free execution of lock-based programs},
  journal = {SIGOPS Oper. Syst. Rev.},
  year = {2002},
  volume = {36},
  pages = {5--17},
  month = {oct},
  acmid = {605399},
  address = {New York, NY, USA},
  doi = {http://doi.acm.org/10.1145/635508.605399},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\p5-rajwar.pdf:PDF},
  issn = {0163-5980},
  issue = {5},
  numpages = {13},
  publisher = {ACM},
  timestamp = {2011.11.25},
  url = {http://doi.acm.org/10.1145/635508.605399}
}

@ARTICLE{Rajwar:2005:VTM:1080695.1070011,
  author = {Rajwar, Ravi and Herlihy, Maurice and Lai, Konrad},
  title = {Virtualizing Transactional Memory},
  journal = {SIGARCH Comput. Archit. News},
  year = {2005},
  volume = {33},
  pages = {494--505},
  month = {May},
  acmid = {1070011},
  address = {New York, NY, USA},
  doi = {http://doi.acm.org/10.1145/1080695.1070011},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\Rajwar_2005_VTM_1080695_1070011.pdf:PDF},
  issn = {0163-5964},
  issue = {2},
  numpages = {12},
  publisher = {ACM},
  timestamp = {2011.11.25},
  url = {http://doi.acm.org/10.1145/1080695.1070011}
}

@INPROCEEDINGS{1365107,
  author = {Ramamoorthy, A. and Wesel, R.D.},
  title = {Analysis of an algorithm for irregular LDPC code construction},
  booktitle = {Information Theory, 2004. ISIT 2004. Proceedings. International Symposium
	on},
  year = {2004},
  pages = { 69},
  month = {june-2 july},
  abstract = { This work presents a rigorous analysis of an algorithm proposed by
	Tian et al. (2003) for the construction of irregular LDPC codes with
	reduced stopping sets and low error floors. Computation of the expected
	number of stopping sets of a given size proves that the algorithm
	significantly outperforms a random construction. We show that the
	algorithm provably reduces the expected number of stopping sets up
	to a certain size (based on the input parameters). The expected number
	of cycles of a given size is computed for both constructions.},
  doi = {10.1109/ISIT.2004.1365107},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\01365107.pdf:PDF},
  keywords = { error floor; irregular LDPC code construction; stopping set; parity
	check codes;},
  timestamp = {2012.04.10}
}

@ARTICLE{ReedandShih,
  author = {Reed, T. S. and Shih, M. T. and Truong, T. K.},
  title = {VLSI design of inverse-free Berlekamp-Massey algorithm},
  journal = {Computer and Digital Techniques, IEE Proceedings E},
  year = {1991},
  volume = {138},
  pages = {295-298},
  month = {September},
  file = {:PDF\\VLSI_design_of_inverse-freeBerlekamp-Massey_argorithm.pdf:PDF},
  issue = {5}
}

@ARTICLE{701488,
  author = {Ricco, B. and Gozzi, G. and Lanzoni, M.},
  title = {Modeling and simulation of stress-induced leakage current in ultrathin
	SiO2 films},
  journal = {Electron Devices, IEEE Transactions on},
  year = {1998},
  volume = {45},
  pages = {1554 -1560},
  number = {7},
  month = {jul},
  abstract = {This paper presents a new model fur stress-induced leakage current
	(SILC) in ultrathin SiO2 films, that is able to explain and accurately
	represent the experimental data obtained with MOS capacitors fabricated
	with different technologies and oxide thickness in the 3-7 nm range},
  doi = {10.1109/16.701488},
  file = {:PDF\\00701488.pdf:PDF},
  issn = {0018-9383},
  keywords = {Conductive films;Conductivity;Electrons;Leakage current;MOS capacitors;Nonvolatile
	memory;Stress;Substrates;Tunneling;Voltage;MOS capacitors;insulating
	thin films;leakage currents;silicon compounds;3 to 7 nm;MOS capacitor;SILC;SiO2;modeling;simulation;stress
	induced leakage current;ultrathin SiO2 film;},
  timestamp = {2013.02.13}
}

@INPROCEEDINGS{ErrorFloorsofLDPCCodes_Richardson,
  author = {T.J. Richardson},
  title = {Error floor of LDPC codes},
  booktitle = {Proceedings of Allerton Conference of Communications, Control and
	Computing},
  year = {2003},
  pages = {1426-1435},
  month = {oct},
  file = {:PDF\\ErrorFloorsofLDPCCodes_Richardson.pdf:PDF},
  timestamp = {2013.01.25}
}

@ARTICLE{910578,
  author = {Richardson, T.J. and Shokrollahi, M.A. and Urbanke, R.L.},
  title = {Design of capacity-approaching irregular low-density parity-check
	codes},
  journal = {Information Theory, IEEE Transactions on},
  year = {2001},
  volume = {47},
  pages = {619 -637},
  number = {2},
  month = {feb},
  abstract = {We design low-density parity-check (LDPC) codes that perform at rates
	extremely close to the Shannon capacity. The codes are built from
	highly irregular bipartite graphs with carefully chosen degree patterns
	on both sides. Our theoretical analysis of the codes is based on
	the work of Richardson and Urbanke (see ibid., vol.47, no.2, p.599-618,
	2000). Assuming that the underlying communication channel is symmetric,
	we prove that the probability densities at the message nodes of the
	graph possess a certain symmetry. Using this symmetry property we
	then show that, under the assumption of no cycles, the message densities
	always converge as the number of iterations tends to infinity. Furthermore,
	we prove a stability condition which implies an upper bound on the
	fraction of errors that a belief-propagation decoder can correct
	when applied to a code induced from a bipartite graph with a given
	degree distribution. Our codes are found by optimizing the degree
	structure of the underlying graphs. We develop several strategies
	to perform this optimization. We also present some simulation results
	for the codes found which show that the performance of the codes
	is very close to the asymptotic theoretical bounds.},
  doi = {10.1109/18.910578},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\00910578.pdf:PDF},
  issn = {0018-9448},
  keywords = {Shannon capacity;additive white Gaussian noise channel;asymptotic
	theoretical bounds;belief-propagation decoder;binary-input AWGN channel;bipartite
	graph;capacity-approaching irregular codes;code design;decoding;degree
	distribution;degree structure optimisation;irregular bipartite graphs;low-density
	parity-check codes;message densities;message nodes;probability densities;simulation
	results;stability condition;symmetric communication channel;symmetry
	property;upper bound;AWGN channels;channel capacity;decoding;error
	detection codes;optimisation;probability;stability;},
  timestamp = {2012.04.10}
}

@ARTICLE{910577,
  author = {Richardson, T.J. and Urbanke, R.L.},
  title = {The capacity of low-density parity-check codes under message-passing
	decoding},
  journal = {Information Theory, IEEE Transactions on},
  year = {2001},
  volume = {47},
  pages = {599 -618},
  number = {2},
  month = {feb},
  abstract = {We present a general method for determining the capacity of low-density
	parity-check (LDPC) codes under message-passing decoding when used
	over any binary-input memoryless channel with discrete or continuous
	output alphabets. Transmitting at rates below this capacity, a randomly
	chosen element of the given ensemble will achieve an arbitrarily
	small target probability of error with a probability that approaches
	one exponentially fast in the length of the code. (By concatenating
	with an appropriate outer code one can achieve a probability of error
	that approaches zero exponentially fast in the length of the code
	with arbitrarily small loss in rate.) Conversely, transmitting at
	rates above this capacity the probability of error is bounded away
	from zero by a strictly positive constant which is independent of
	the length of the code and of the number of iterations performed.
	Our results are based on the observation that the concentration of
	the performance of the decoder around its average performance, as
	observed by Luby et al. in the case of a binary-symmetric channel
	and a binary message-passing algorithm, is a general phenomenon.
	For the particularly important case of belief-propagation decoders,
	we provide an effective algorithm to determine the corresponding
	capacity to any desired degree of accuracy. The ideas presented in
	this paper are broadly applicable and extensions of the general method
	to low-density parity-check codes over larger alphabets, turbo codes,
	and other concatenated coding schemes are outlined},
  doi = {10.1109/18.910577},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\00910577.pdf:PDF},
  issn = {0018-9448},
  keywords = {belief-propagation decoders;binary message-passing algorithm;binary-input
	memoryless channel;binary-symmetric channel;code capacity;code length;code
	rate;concatenated coding;continuous output alphabet;decoder performance;discrete
	output alphabet;error probability;iterative decoding;low-density
	parity-check codes;message-passing decoding;outer code;turbo codes;concatenated
	codes;error detection codes;error statistics;iterative decoding;memoryless
	systems;telecommunication channels;turbo codes;},
  timestamp = {2011.12.02}
}

@ARTICLE{910579,
  author = {Richardson, T.J. and Urbanke, R.L.},
  title = {Efficient encoding of low-density parity-check codes},
  journal = {Information Theory, IEEE Transactions on},
  year = {2001},
  volume = {47},
  pages = {638-656},
  number = {2},
  abstract = {Low-density parity-check (LDPC) codes can be considered serious competitors
	to turbo codes in terms of performance and complexity and they are
	based on a similar philosophy: constrained random code ensembles
	and iterative decoding algorithms. We consider the encoding problem
	for LDPC codes. More generally we consider the encoding problem for
	codes specified by sparse parity-check matrices. We show how to exploit
	the sparseness of the parity-check matrix to obtain efficient encoders.
	For the (3,6)-regular LDPC code, for example, the complexity of encoding
	is essentially quadratic in the block length. However, we show that
	the associated coefficient can be made quite small, so that encoding
	codes even of length n≃100000 is still quite practical. More importantly,
	we show that “optimized” codes actually admit linear time encoding},
  doi = {10.1109/18.910579},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\00910579.pdf:PDF},
  issn = {0018-9448},
  keywords = {computational complexity;error detection codes;iterative decoding;optimisation;random
	codes;sparse matrices;binary erasure channel;block length;code length;coefficient;complexity;constrained
	random code ensembles;efficient encoding;iterative decoding algorithms;linear
	time encoding;low-density parity-check codes;optimized codes;performance;regular
	LDPC code;sparse parity-check matrices;turbo codes;Algorithm design
	and analysis;Channel capacity;Encoding;Iterative algorithms;Iterative
	decoding;Parity check codes;Physics;Sparse matrices;Technological
	innovation;Turbo codes},
  timestamp = {2013.10.15}
}

@INPROCEEDINGS{5451825,
  author = {Rizvi, S.S. and Tae-Sun Chung},
  title = {Flash memory SSD based DBMS for data warehouses and data marts},
  booktitle = {Computer and Automation Engineering (ICCAE), 2010 The 2nd International
	Conference on},
  year = {2010},
  volume = {3},
  pages = {557 -559},
  month = {feb.},
  abstract = {Flash memory based high capacity SSDs open the doors for large enterprise
	applications for better performance and high reliability. Flash memory
	hardware characteristics not allow disk based schemes implication
	directly. For employing such schemes, we need to revise them on some
	level to make them effective for flash media storage. In this paper,
	we aim to implement DBMS on flash memory SSD based large enterprise
	applications. This paper presents the relevancy of SSD characteristics
	with storage features of data warehouses and data marts and proposes
	the architecture of data storage for variable-length records in and
	data retrieval using virtual sequential access method by multilevel
	indexing from flash memory based SSDs for such applications. We prove
	less overhead with high reliability, and more throughput compare
	to hard disk drives.},
  doi = {10.1109/ICCAE.2010.5451825},
  file = {:PDF\\05451825.pdf:PDF},
  keywords = {DBMS;data marts;data retrieval;data warehouse;flash memory SSD;large
	enterprise application;multilevel indexing;solid-state-drive;variable-length
	record;virtual sequential access method;data warehouses;flash memories;storage
	management;},
  timestamp = {2011.05.20}
}

@INPROCEEDINGS{5674850,
  author = {Rizvi, S.S. and Tae-Sun Chung},
  title = {Flash memory SSD based DBMS for high performance computing embedded
	and multimedia systems},
  booktitle = {Computer Engineering and Systems (ICCES), 2010 International Conference
	on},
  year = {2010},
  pages = {183 -188},
  month = {30 2010-dec. 2},
  abstract = {Flash memory based large capacity SSDs open the doors on high performance
	computing applications by offering remarkable throughput and amazing
	reliability. However, flash memory hardware characteristics like
	erase-before-write and limited endurance cycles do not allow disk
	based schemes implication directly. For employing such schemes, we
	need to revise them on some level to make them effective for flash
	media. Regarding, the researchers have devoted considerable efforts
	to implement the DBMSs on flash for fast retrieval of data that is
	bit indolent using file systems. However, previous techniques require
	huge main memory space and high computational power for data processing.
	This paper proposes an advanced DBMS architecture using key sequenced
	data set, virtual sequential access method and multilevel indexing
	for flash memory SSD based performance oriented embedded and multimedia
	applications. Basic database operations plus space reclamation and
	memory wear-leveling are achieved by taking flash characteristics
	into account carefully. Main memory based buffer management is implemented
	to increase throughput and for efficient media utilization. Comprehensive
	performance evaluations using two modern benchmarks prove less overhead
	with high reliability and outstanding throughput for flash SSD based
	DBMSs compare to HDD.},
  doi = {10.1109/ICCES.2010.5674850},
  file = {:PDF\\05674850.pdf:PDF},
  keywords = {DBMS architecture;buffer management;data retrieval;embedded systems;endurance
	cycles;erase-before-write;file systems;flash media;flash memory SSD;flash
	memory hardware characteristics;flash memory solid-state-drive;high
	performance computing;key sequenced data set;large capacity SSDs;memory
	wear-leveling;multilevel indexing;multimedia systems;performance
	evaluations;reliability;space reclamation;virtual sequential access;buffer
	storage;database indexing;embedded systems;flash memories;information
	retrieval;multimedia computing;multimedia databases;performance evaluation;},
  timestamp = {2011.05.20}
}

@INPROCEEDINGS{6125326,
  author = {Romano, G. and Drago, A. and Ciuonzo, D.},
  title = {Sub-optimal importance sampling for fast simulation of linear block
	codes over BSC channels},
  booktitle = {Wireless Communication Systems (ISWCS), 2011 8th International Symposium
	on},
  year = {2011},
  pages = {141 -145},
  month = {nov.},
  abstract = {Estimation of very low word-error probability of hard-decoded linear
	block codes can be performed through Monte-Carlo simulation. The
	computational complexity of the standard method however increases
	as the probability of error to be estimated decreases. In this paper
	we propose a general algorithm for fast estimation of probability
	of error of linear block codes on BSC channels based on the importance
	sampling and the cross-entropy method for rare-events that can be
	employed for any hard-decision decoder. When optimal decoding is
	used the algorithm reduces to a single simulation run that can estimate,
	with a given accuracy, performances for a whole range of sufficiently
	high signal-to-noise ratios.},
  doi = {10.1109/ISWCS.2011.6125326},
  file = {:PDF\\06125326.pdf:PDF},
  issn = {2154-0217},
  keywords = {BSC channels;Monte-Carlo simulation;computational complexity;cross-entropy
	method;hard-decision decoder;hard-decoded linear block codes;optimal
	decoding;signal-to-noise ratios;suboptimal importance sampling;word-error
	probability;Monte Carlo methods;block codes;channel coding;computational
	complexity;decoding;linear codes;probability;},
  timestamp = {2012.09.04}
}

@ARTICLE{1053994,
  author = {Rudolph, L.},
  title = {A class of majority logic decodable codes (Corresp.)},
  journal = {Information Theory, IEEE Transactions on},
  year = {1967},
  volume = {13},
  pages = {305 -307},
  number = {2},
  month = {april },
  abstract = {Not available},
  doi = {10.1109/TIT.1967.1053994},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\01053994.pdf:PDF},
  issn = {0018-9448},
  keywords = {Cyclic codes;Majority logic decoding;},
  timestamp = {2011.10.24}
}

@INPROCEEDINGS{5394793,
  author = {Saeedi, H. and Pishro-Nik, H. and Banihashemi, A.H.},
  title = {On systematic design of universally capacity approaching rate-compatible
	sequences of LDPC code ensembles over binary-input output-symmetric
	memoryless channels},
  booktitle = {Communication, Control, and Computing, 2009. Allerton 2009. 47th
	Annual Allerton Conference on},
  year = {2009},
  pages = {400 -407},
  month = {30 2009-oct 2},
  abstract = {Despite tremendous amount of research on the design of low-density
	parity-check (LDPC) codes with belief propagation decoding over different
	types of binary-input output-symmetric memoryless (BIOSM) channels,
	most results on this topic are based on numerical methods and optimization
	which do not provide much insight into the design process. In particular,
	systematic design of provably capacity achieving sequences of LDPC
	code ensembles over the general class of BIOSM channels, has remained
	a fundamental open problem. For the case of the binary erasure channel,
	explicit construction of capacity achieving sequences have been proposed
	based on a property called the flatness condition. In this paper,
	we propose a systematic method to design universally capacity approaching
	rate-compatible LDPC code ensemble sequences over BIOSM channels.
	This is achieved by interpreting the flatness condition over the
	BEC, as a successive maximization (SM) principle that is generalized
	to other BIOSM channels to design a sequence of capacity approaching
	ensembles called the parent sequence. The SM principle is then applied
	to each ensemble within the parent sequence, this time to design
	rate-compatible puncturing schemes. As part of our results, we extend
	the stability condition which was previously derived for degree-2
	variable nodes to other variable node degrees as well as to the case
	of rate-compatible codes. Consequently, we rigorously prove that
	using the SM principle, one is able to design universally capacity
	achieving rate-compatible LDPC code ensemble sequences over the BEC.
	Unlike the previous results on such schemes over the BEC in the literature,
	the proposed SM approach is naturally extendable to other BIOSM channels.
	The performance of the rate-compatible schemes designed based on
	our systematic method is comparable to those designed by optimization.},
  doi = {10.1109/ALLERTON.2009.5394793},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05394793.pdf:PDF},
  keywords = {LDPC code;binary-input output-symmetric memoryless channel;low-density
	parity-check;rate-compatible puncturing scheme;rate-compatible sequences;successive
	maximization;binary codes;channel capacity;memoryless systems;optimisation;parity
	check codes;},
  timestamp = {2011.12.02}
}

@ARTICLE{476246,
  author = {Saints, K. and Heegard, C.},
  title = {Algebraic-geometric codes and multidimensional cyclic codes: a unified
	theory and algorithms for decoding using Grobner bases},
  journal = {Information Theory, IEEE Transactions on},
  year = {1995},
  volume = {41},
  pages = {1733 -1751},
  number = {6},
  month = {nov},
  abstract = {It is proved that any algebraic-geometric (AG) code can be expressed
	as a cross section of an extended multidimensional cyclic code. Both
	AG codes and multidimensional cyclic codes are described by a unified
	theory of linear block codes defined over point sets: AG codes are
	defined over the points of an algebraic curve, and an m-dimensional
	cyclic code is defined over the points in m-dimensional space. The
	power of the unified theory is in its description of decoding techniques
	using Grobner bases. In order to fit an AG code into this theory,
	a change of coordinates must be applied to the curve over which the
	code is defined so that the curve is in special position. For curves
	in special position, all computations can be performed with polynomials
	and this also makes it possible to use the theory of Grobner bases.
	Next, a transform is defined for AG codes which generalizes the discrete
	Fourier transform. The transform is also related to a Grobner basis,
	and is useful in setting up the decoding problem. In the decoding
	problem, a key step is finding a Grobner basis for an error locator
	ideal. For AG codes, multidimensional cyclic codes, and indeed, any
	cross section of an extended multidimensional cyclic code, Sakata's
	algorithm can be used to find linear recursion relations which hold
	on the syndrome array. In this general context, the authors give
	a self-contained and simplified presentation of Sakata's algorithm,
	and present a general framework for decoding algorithms for this
	family of codes, in which the use of Sakata's algorithm is supplemented
	by a procedure for extending the syndrome array},
  doi = {10.1109/18.476246},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\00476246.pdf:PDF},
  issn = {0018-9448},
  keywords = {Grobner bases;Sakata's algorithm;algebraic curve;algebraic-geometric
	codes;algorithms;decoding;discrete Fourier transform;error locator;linear
	block codes;linear recursion relations;m-dimensional space;multidimensional
	cyclic codes;point set;polynomial;syndrome array;unified theory;algebraic
	geometric codes;block codes;cyclic codes;decoding;discrete Fourier
	transforms;error correction codes;linear codes;polynomials;},
  timestamp = {2012.05.15}
}

@ARTICLE{585557,
  author = {Sakakibara, K. and Ajika, N. and Eikyu, K. and Ishikawa, K. and Miyoshi,
	H.},
  title = {A quantitative analysis of time-decay reproducible stress-induced
	leakage current in SiO2 films},
  journal = {Electron Devices, IEEE Transactions on},
  year = {1997},
  volume = {44},
  pages = {1002 -1008},
  number = {6},
  month = jun,
  abstract = {In the cases of both Fowler-Nordheim (FN) stress and substrate hot-hole
	stress, three reproducible stress-induced leakage current (SILC)
	components have been found for the repeated unipolar gate-voltage
	scans in 9.2 nm wet oxides. To clarify the mechanisms of these current
	components, a quantitative analysis has been developed. By precisely
	modeling the phonon assisted tunneling process, it has been shown
	that the E-J and t-J characteristics of the reproducible current
	components can be completely simulated as electron tunneling processes
	into the neutral traps, each with a single trap level. From this
	analysis, the physical parameters of the traps have been estimated
	with a reasonable degree of accuracy. Furthermore, the increase in
	distribution of the neutral trap density toward both the SiO2 interfaces
	has also been estimated},
  doi = {10.1109/16.585557},
  file = {:PDF\\A_Quatitative_Analysis_of_Time-Decay_Reproducible_Stress-Induced_Leakage_Current_in_SiO2_Films.pdf:PDF},
  issn = {0018-9383},
  keywords = {9.2 nm;Fowler-Nordheim stress;SiO2 films;SiO2-Si;current components;electron
	tunneling processes;modeling;neutral trap density;phonon assisted
	tunneling process;quantitative analysis;stress-induced leakage current;substrate
	hot-hole stress;time-decay reproducible SILC;unipolar gate-voltage
	scans;MOSFET;dielectric thin films;electric breakdown;electron traps;hot
	carriers;interface states;leakage currents;semiconductor device models;semiconductor-insulator
	boundaries;silicon compounds;tunnelling;}
}

@ARTICLE{585555,
  author = {Sakakibara, K. and Ajika, N. and Hatanaka, M. and Miyoshi, H. and
	Yasuoka, A.},
  title = {Identification of stress-induced leakage current components and the
	corresponding trap models in SiO2 films},
  journal = {Electron Devices, IEEE Transactions on},
  year = {1997},
  volume = {44},
  pages = {986 -992},
  number = {6},
  month = {jun},
  abstract = {Time-decay stress-induced leakage current (SILC) has been systematically
	investigated for the cases of both Fowler-Nordheim (FN) stress and
	substrate hot-hole stress. From the three viewpoints of the reproducibility
	of the-current component for the gate voltage scan, the change of
	oxide charge during the gate voltage scan, and the resistance of
	the current component to thermal annealing, it has been found that
	time-decay stress-induced leakage current is composed of five current
	components, regardless of stress type. Trap models corresponding
	to each current component have been proposed. In addition, it has
	also been proven that holes generate the electron traps related to
	one of those current components},
  doi = {10.1109/16.585555},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\00585555.pdf:PDF},
  issn = {0018-9383},
  keywords = {Fowler-Nordheim stress;MOS transistors;SiO2 films;SiO2-Si;electron
	traps;gate voltage scan;oxide charge;stress-induced leakage current
	components;substrate hot-hole stress;thermal annealing resistance;time-decay
	stress-induced leakage current;trap models;MOSFET;dielectric thin
	films;electric breakdown;electron traps;hot carriers;leakage currents;semiconductor
	device models;semiconductor-insulator boundaries;silicon compounds;tunnelling;},
  timestamp = {2011.09.16}
}

@ARTICLE{Sakata1991191,
  author = {Shojiro Sakata},
  title = {Two-dimensional shift register synthesis and Grﾃｶbner bases for polynomial
	ideals over an integer residue ring },
  journal = {Discrete Applied Mathematics },
  year = {1991},
  volume = {33},
  pages = {191 - 203},
  number = {1-3},
  doi = {http://dx.doi.org/10.1016/0166-218X(91)90115-D},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\Sakata1991191.pdf:PDF},
  issn = {0166-218X},
  timestamp = {2014.03.26},
  url = {http://www.sciencedirect.com/science/article/pii/0166218X9190115D}
}

@ARTICLE{Sakata1990207,
  author = {Shojiro Sakata},
  title = {Extension of the Berlekamp-Massey algorithm to N dimensions },
  journal = {Information and Computation },
  year = {1990},
  volume = {84},
  pages = {207 - 239},
  number = {2},
  doi = {http://dx.doi.org/10.1016/0890-5401(90)90039-K},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\Sakata1990207.pdf:PDF},
  issn = {0890-5401},
  timestamp = {2014.03.26},
  url = {http://www.sciencedirect.com/science/article/pii/089054019090039K}
}

@ARTICLE{Sakata1988321,
  author = {Shojiro Sakata},
  title = {Finding a minimal set of linear recurring relations capable of generating
	a given finite two-dimensional array },
  journal = {Journal of Symbolic Computation },
  year = {1988},
  volume = {5},
  pages = {321 - 337},
  number = {3},
  doi = {http://dx.doi.org/10.1016/S0747-7171(88)80033-6},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\Sakata1988321.pdf:PDF},
  issn = {0747-7171},
  timestamp = {2014.03.26},
  url = {http://www.sciencedirect.com/science/article/pii/S0747717188800336}
}

@ARTICLE{476248,
  author = {Sakata, S. and Jensen, H.E. and Hoholdt, T.},
  title = {Generalized Berlekamp-Massey decoding of algebraic-geometric codes
	up to half the Feng-Rao bound},
  journal = {Information Theory, IEEE Transactions on},
  year = {1995},
  volume = {41},
  pages = {1762-1768},
  number = {6},
  month = {Nov},
  abstract = {We treat a general class of algebraic-geometric codes and show how
	to decode these up to half the Feng-Rao bound, using an extension
	and modification of the Sakata algorithm (1990). The Sakata algorithm
	is a generalization to N dimensions of the classical Berlekamp-Massey
	algorithm},
  doi = {10.1109/18.476248},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\00476248.pdf:PDF},
  issn = {0018-9448},
  keywords = {algebraic geometric codes;decoding;Feng-Rao bound;Sakata algorithm;algebraic-geometric
	codes;generalized Berlekamp-Massey decoding;Decoding;Differential
	equations;Error correction;Polynomials;Vectors},
  timestamp = {2014.03.25}
}

@ARTICLE{SakataJensenHoholdt1995,
  author = {Sakata, S. and Jensen, H.E. and Hoholdt, T.},
  title = {Generalized Berlekamp-Massey decoding of algebraic-geometric codes
	up to half the Feng-Rao bound},
  journal = {Information Theory, IEEE Transactions on},
  year = {1995},
  volume = {41},
  pages = {1762 -1768},
  number = {6},
  month = nov,
  abstract = {We treat a general class of algebraic-geometric codes and show how
	to decode these up to half the Feng-Rao bound, using an extension
	and modification of the Sakata algorithm (1990). The Sakata algorithm
	is a generalization to N dimensions of the classical Berlekamp-Massey
	algorithm},
  doi = {10.1109/18.476248},
  file = {:PDF\\Generalized_Berlekamp-Massey_Decoding_of_Algebraic-Geometric_Codes_up_to_Half_the_Feng-Rao_Bound.pdf:PDF},
  issn = {0018-9448},
  keywords = {Feng-Rao bound;Sakata algorithm;algebraic-geometric codes;generalized
	Berlekamp-Massey decoding;algebraic geometric codes;decoding;}
}

@ARTICLE{476240,
  author = {Sakata, S. and Justesen, J. and Madelung, Y. and Jensen, H.E. and
	Hoholdt, T.},
  title = {Fast decoding of algebraic-geometric codes up to the designed minimum
	distance},
  journal = {Information Theory, IEEE Transactions on},
  year = {1995},
  volume = {41},
  pages = {1672 -1677},
  number = {6},
  month = nov,
  abstract = {We present a decoding algorithm for algebraic-geometric codes from
	regular plane curves, in particular the Hermitian curve, which corrects
	all error patterns of weight less than d*/2 with low complexity.
	The algorithm is based on the majority scheme of Feng and Rao (1993)
	and uses a modified version of Sakata's (1988) generalization of
	the Berlekamp-Massey algorithm},
  doi = {10.1109/18.476240},
  file = {:PDF\\Fast_Decoding_of_Algebraic-Geometric_Codes_up_to_the_Designed_Minimum_Distance.pdf:PDF},
  issn = {0018-9448},
  keywords = {Berlekamp-Massey algorithm;Hermitian curve;algebraic-geometric codes;error
	patterns correction;fast decoding algorithm;low complexity;majority
	scheme;minimum distance;regular plane curves;algebraic geometric
	codes;computational complexity;decoding;}
}

@BOOK{Sala:1412032,
  title = {Gr\"{o}bner Bases, Coding, and Cryptography},
  publisher = {Springer},
  year = {2009},
  author = {Sala, Massimiliano and Mora, Teo and Perret, Ludovic},
  address = {Dordrecht},
  abstract = {Coding theory and cryptography allow secure and reliable data transmission,
	which is at the heart of modern communication. Nowadays, it is hard
	to find an electronic device without some code inside.
	
	
	Gröbner bases have emerged as the main tool in computational algebra,
	permitting numerous applications, both in theoretical contexts and
	in practical situations.
	
	
	This book is the first book ever giving a comprehensive overview on
	the application of commutative algebra to coding theory and cryptography.
	For example, all important properties of algebraic/geometric coding
	systems (including encoding, construction, decoding, list decoding)
	are individually analysed, reporting all significant approaches appeared
	in the literature. Also, stream ciphers, PK cryptography, symmetric
	cryptography and Polly Cracker systems deserve each a separate chapter,
	where all the relevant literature is reported and compared. While
	many short notes hint at new exciting directions, the reader will
	find that all chapters fit nicely within a unified notation.},
  isbn = {978-3-540-93805-7},
  keywords = {Coding Theory, Cryptography, Groebner bases, computer algebra},
  timestamp = {2012.06.06},
  url = {http://www.springer.com/mathematics/algebra/book/978-3-540-93805-7}
}

@INPROCEEDINGS{6033740,
  author = {Sason, I. and Eshel, R.},
  title = {On concentration of measures for LDPC code ensembles},
  booktitle = {Information Theory Proceedings (ISIT), 2011 IEEE International Symposium
	on},
  year = {2011},
  pages = {1268-1272},
  abstract = {This work considers the concentration of measures for low-density
	parity-check (LDPC) code ensembles. The two results derived in this
	paper follow from Azuma's inequality for Doob martingales with bounded
	differences. The first result is a tightened concentration inequality
	for the conditional entropy (originally derived by Méasson et al.),
	and the second result is a concentration inequality for the cardinality
	of the fundamental systems of cycles of a bipartite graph from the
	ensemble.},
  doi = {10.1109/ISIT.2011.6033740},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\06033740.pdf:PDF},
  issn = {2157-8095},
  keywords = {entropy;graph theory;parity check codes;Azuma inequality;Doob martingales;LDPC
	code ensembles;bipartite graph;cardinality;conditional entropy;fundamental
	systems;low-density parity-check code ensembles;tightened concentration
	inequality;Bipartite graph;Channel capacity;Entropy;Iterative decoding;Upper
	bound;Azuma's inequality;concentration of measures;low-density parity-check
	(LDPC) codes;martingales},
  timestamp = {2013.12.17}
}

@ARTICLE{285029,
  author = {Schuegraf, K.F. and Chenming Hu},
  title = {Hole injection SiO2 breakdown model for very low voltage lifetime
	extrapolation},
  journal = {Electron Devices, IEEE Transactions on},
  year = {1994},
  volume = {41},
  pages = {761 -767},
  number = {5},
  month = {may},
  abstract = {In this paper, we present a model for silicon dioxide breakdown characterization,
	valid for a thickness range between 25 Aring; and 130 Aring;, which
	provides a method for predicting dielectric lifetime for reduced
	power supply voltages and aggressively scaled oxide thicknesses.
	This model, based on hole injection from the anode, accurately predicts
	QBD and tBD behavior including a fluence in excess of 107 C/cm2 at
	an oxide voltage of 2.4 V for a 25 Aring; oxide. Moreover, this model
	is a refinement of and fully complementary with the well known 1/E
	model, while offering the ability to predict oxide reliability for
	low voltages},
  doi = {10.1109/16.285029},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\00285029.pdf:PDF},
  issn = {0018-9383},
  keywords = {1/E model;25 to 130 A;SiO2;SiO2 breakdown model;breakdown characterization;dielectric
	lifetime;hole injection;low voltage lifetime extrapolation;oxide
	reliability prediction;scaled oxide thicknesses;dielectric thin films;electric
	breakdown of solids;reliability;semiconductor device models;semiconductor-insulator
	boundaries;silicon compounds;},
  timestamp = {2011.09.30}
}

@INPROCEEDINGS{996606,
  author = {Schuler, F. and Degraeve, R. and Hendrickx, P. and Wellekens, D.},
  title = {Physical description of anomalous charge loss in floating gate based
	NVM's and identification of its dominant parameter},
  booktitle = {Reliability Physics Symposium Proceedings, 2002. 40th Annual},
  year = {2002},
  pages = { 26 - 33},
  abstract = { A model for anomalous charge loss is presented based on the physical
	description of charge transport through the tunnel oxide. This physics
	based model considers phonon-assistance as well as arbitrary 3-dimensional
	distributions of oxide defects (electron traps). After identifying
	the trap-trap distance as the most important parameter the 3-dimensional
	model can be simplified to a tunneling model, which describes one
	tunneling step only. The consistency of the simplified 1-step model
	with the percolation model for anomalous charge loss description
	is shown. Also the accelerated testing method for anomalous charge
	loss is confirmed and validated by these models.},
  doi = {10.1109/RELPHY.2002.996606},
  file = {:PDF\\Physical_Description_of_Anomalous_Charge_Loss_in_Floating_Gate_Based_NVM's_and_Identification_of_its_Dominant_Parameter.pdf:PDF},
  issn = { },
  keywords = { accelerated testing; charge loss; charge transport; electron trap;
	floating gate nonvolatile memory; one-step model; percolation model;
	phonon assistance; three-dimensional defect distribution; tunnel
	oxide; tunneling model; electron traps; electron-phonon interactions;
	integrated memory circuits; life testing; percolation; tunnelling;}
}

@ARTICLE{RevModPhys.36.856,
  author = {SCHULTZ, T. D. and MATTIS, D. C. and LIEB, E. H.},
  title = {Two-Dimensional Ising Model as a Soluble Problem of Many Fermions},
  journal = {Rev. Mod. Phys.},
  year = {1964},
  volume = {36},
  pages = {856--871},
  month = {Jul},
  doi = {10.1103/RevModPhys.36.856},
  file = {:PDF\\RevModPhys.36.856.pdf:PDF},
  issue = {3},
  publisher = {American Physical Society},
  timestamp = {2012.11.13},
  url = {http://link.aps.org/doi/10.1103/RevModPhys.36.856}
}

@ARTICLE{59946,
  author = {Seguin, G.E. and Drolet, G.},
  title = {The trace description of irreducible quasi-cyclic codes},
  journal = {Information Theory, IEEE Transactions on},
  year = {1990},
  volume = {36},
  pages = {1463 -1466},
  number = {6},
  month = {nov},
  abstract = {The notion of a q-ary irreducible quasi-cyclic code of block length
	n and index r is introduced. A trace description of such a code is
	provided in a fashion similar to the trace description of irreducible
	cyclic codes. In particular, it is shown that an irreducible quasi-cyclic
	code of dimension k is completely described by an irreducible cyclic
	code and r elements from a field of cardinality qk. Using this fact,
	a number of binary irreducible quasi-cyclic codes of index 2 are
	constructed and their weight spectra obtained},
  doi = {10.1109/18.59946},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\00059946.pdf:PDF},
  issn = {0018-9448},
  keywords = {binary codes;irreducible quasi-cyclic code;q-ary codes;trace description;weight
	spectra;error correction codes;},
  timestamp = {2011.11.10}
}

@INPROCEEDINGS{5746281,
  author = {Shyh-Shyuan Sheu and Meng-Fan Chang and Ku-Feng Lin and Che-Wei Wu
	and Yu-Sheng Chen and Pi-Feng Chiu and Chia-Chen Kuo and Yih-Shan
	Yang and Pei-Chia Chiang and Wen-Pin Lin and Che-He Lin and Heng-Yuan
	Lee and Pei-Yi Gu and Sum-Min Wang and Chen, F.T. and Keng-Li Su
	and Chen-Hsin Lien and Kuo-Hsing Cheng and Hsin-Tun Wu and Tzu-Kun
	Ku and Ming-Jer Kao and Ming-Jinn Tsai},
  title = {A 4Mb embedded SLC resistive-RAM macro with 7.2ns read-write random-access
	time and 160ns MLC-access capability},
  booktitle = {Solid-State Circuits Conference Digest of Technical Papers (ISSCC),
	2011 IEEE International},
  year = {2011},
  pages = {200 -202},
  month = {feb.},
  abstract = {This work proposes process/resistance variation-insensitive read schemes
	for embedded RRAM to achieve fast read speeds with high yields. An
	embedded mega-bit scale (4Mb), single-level-cell (SLC) RRAM macro
	with sub-8ns read-write random access time is presented. Multi-level-cell
	(MLC) operation with 160ns write-verify operation is demonstrated.},
  doi = {10.1109/ISSCC.2011.5746281},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05746281.pdf:PDF},
  issn = {0193-6530},
  keywords = {MLC-access capability;embedded RRAM;embedded SLC resistive-RAM macro;read-write
	random-access time;single-level-cell RRAM macro;embedded systems;random-access
	storage;},
  timestamp = {2011.08.24}
}

@ARTICLE{5590231,
  author = {Shyh-Shyuan Sheu and Kuo-Hsing Cheng and Meng-Fan Chang and Pei-Chia
	Chiang and Wen-Pin Lin and Heng-Yuan Lee and Pang-Shiu Chen and Yu-Sheng
	Chen and Tai-Yuan Wu and Chen, F.T. and Keng-Li Su and Ming-Jer Kao
	and Ming-Jinn Tsai},
  title = {Fast-Write Resistive RAM (RRAM) for Embedded Applications},
  journal = {Design Test of Computers, IEEE},
  year = {2011},
  volume = {28},
  pages = {64 -71},
  number = {1},
  month = {jan.-feb. },
  abstract = {Especially for microcontroller and mobile applications, embedded nonvolatile
	memory is an important technology offering to reduce power and provide
	local persistent storage. This article describes a new resistive
	RAM device with fast write operation to improve the speed of embedded
	nonvolatile memories.},
  doi = {10.1109/MDT.2010.96},
  file = {:PDF\\05590231.pdf:PDF},
  issn = {0740-7475},
  keywords = {embedded applications;embedded nonvolatile memory;fast-write resistive
	RAM;microcontroller;mobile applications;storage;microcontrollers;random-access
	storage;},
  timestamp = {2011.05.23}
}

@ARTICLE{shimizuIEICE_volE89-A_no4,
  author = {Kazunori Shimizu and Tatsuyuki Ishikawa and Nozomu Togawa and Takeshi
	Ikenaga Satoshi Goto},
  title = {Partially-Parallel LDPC Decoder Achieving High-Efficiency Message-Passing
	Schedule},
  journal = {IEICE TRANSACTIONS on Fundamentals of Electronics, Communications
	and Computer Sciences},
  year = {2006},
  volume = {E89-A},
  pages = {969-978},
  number = {4},
  month = {April},
  abstract = {In this paper, we propose a partially-parallel LDPC decoder which
	achieves a high-efficiency message-passing schedule. The proposed
	LDPC decoder is characterized as follows: (i) The column operations
	follow the row operations in a pipelined architecture to ensure that
	the row and column operations are performed concurrently. (ii) The
	proposed parallel pipelined bit functional unit enables the column
	operation module to compute every message in each bit node which
	is updated by the row operations. These column operations can be
	performed without extending the single iterative decoding delay when
	the row and column operations are performed concurrently. Therefore,
	the proposed decoder performs the column operations more frequently
	in a single iterative decoding, and achieves a high-efficiency message-passing
	schedule within the limited decoding delay time. Hardware implementation
	on an FPGA and simulation results show that the proposed partially-parallel
	LDPC decoder improves the decoding throughput and bit error performance
	with a small hardware overhead.},
  file = {:PDF\\shimizuIEICE_volE89-A_no4.pdf:PDF},
  keywords = {low-density parity-check codes, partially-parallel LDPC decoder, message-passing
	algorithm, FPGA},
  timestamp = {2011.04.26},
  url = {http://search.ieice.org/bin/summary.php?id=e89-a_4_969&category=A&lang=E&year=2006}
}

@ARTICLE{1411019,
  author = { Min-Ho Shin and Joon-Sung Kim and Hong-Yeop Song},
  title = {Generalization of Tanner's minimum distance bounds for LDPC codes},
  journal = {Communications Letters, IEEE},
  year = {2005},
  volume = {9},
  pages = { 240 - 242},
  number = {3},
  month = march,
  abstract = { Tanner derived minimum distance bounds of regular codes in terms
	of the eigenvalues of the adjacency matrix by using some graphical
	analysis on the associated graph of the code. In this letter, we
	generalize Tanner's results by deriving a bit-oriented bound and
	a parity-oriented bound on the minimum distances of both regular
	and block-wise irregular LDPC codes.},
  doi = {10.1109/LCOMM.2005.03002},
  file = {:PDF\\Generalization_of_Tanner's_Minimum_Distance_Bounds_for_LDPC_Codes.pdf:PDF},
  issn = {1089-7798},
  keywords = { LDPC codes; Tanner derived minimum distance bounds; adjacency matrix;
	bit-oriented bound; block-wise irregular LDPC codes; code graph;
	eigenvalues; graphical analysis; low-density parity check codes;
	minimum distances; parity-oriented bound; regular LDPC codes; regular
	codes; block codes; eigenvalues and eigenfunctions; error correction
	codes; graph theory; matrix algebra; parity check codes;}
}

@ARTICLE{10.1.1.89.2279,
  author = {I rfan Siap and Nilgun Kulhan},
  title = {The Structure of Generalized Quasi-Cyclic Codes},
  journal = {Applied Mathematics Journal E-Notes},
  year = {2005},
  volume = {5},
  pages = {24-30},
  month = {March},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\10.1.1.89.2279.pdf:PDF},
  timestamp = {2011.10.12},
  url = {http://www.math.nthu.edu.tw/~amen/}
}

@BOOK{silverman,
  title = {はじめての数論 原著第3版},
  publisher = {ピアソンエデュケーション},
  year = {2007},
  author = {Joseph H. Silverman}
}

@ARTICLE{Simmons1986287,
  author = {J.G. Simmons and G.W. Taylor},
  title = {Concepts of gain at an oxide-semiconductor interface and their application
	to the TETRAN-A tunnel emitter transistornd-And to the MIS switching
	device},
  journal = {Solid-State Electronics},
  year = {1986},
  volume = {29},
  pages = {287 - 303},
  number = {3},
  abstract = {The idea of current gain at an insulator interface is discussed using
	the tunnel-oxide as an example upon which to base calculations. A
	novel approach to the calculation of tunnel components is introduced
	which accurately describes the electron tunnel component when the
	metal Fermi level is both above and below the semiconductor conduction
	band edge. A new form of bipolar amplifier (the TETRAN) is proposed
	and its performance is compared from a logic and memory point of
	view with existing transistors. As a logic element the device does
	not perform well when compared with MOS transistors because the current
	densities that one can obtain are too low to charge the relatively
	large tunnel-oxide capacitance. For the memory comparison, the concepts
	of gain necessary for TETRAN operation are applied to the MIS switching
	device. Preliminary experimental verification of the concept is reported.},
  doi = {10.1016/0038-1101(86)90207-8},
  issn = {0038-1101},
  timestamp = {2011.09.15},
  url = {http://www.sciencedirect.com/science/article/pii/0038110186902078}
}

@ARTICLE{556667,
  author = {Sipser, M. and Spielman, D.A.},
  title = {Expander codes},
  journal = {Information Theory, IEEE Transactions on},
  year = {1996},
  volume = {42},
  pages = {1710-1722},
  number = {6},
  abstract = {Using expander graphs, we construct a new family of asymptotically
	good, linear error-correcting codes. These codes have linear time
	sequential decoding algorithms and logarithmic time parallel decoding
	algorithms that use a linear number of processors. We present both
	randomized and explicit constructions of these codes. Experimental
	results demonstrate the good performance of the randomly chosen codes},
  doi = {10.1109/18.556667},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\00556667.pdf:PDF},
  issn = {0018-9448},
  keywords = {error correction codes;graph theory;linear codes;random processes;sequential
	decoding;asymptotically good linear error-correcting codes;expander
	codes;expander graphs;explicit constructions;linear time sequential
	decoding algorithms;logarithmic time parallel decoding algorithms;randomized
	constructions;randomly chosen codes;Algorithm design and analysis;Bipartite
	graph;Circuits;Computational modeling;Decoding;Error correction codes;Fault
	tolerance;Graph theory;Linear code;Parity check codes},
  timestamp = {2013.11.11}
}

@INPROCEEDINGS{1644559,
  author = {Smith, B. and Kschischang, F.R. and Yu, W.},
  title = {Low-Density Parity-Check Codes for Discretized Min-Sum Decoding},
  booktitle = {Communications, 2006 23rd Biennial Symposium on},
  year = {2006},
  pages = {14 -17},
  month = {0-0},
  abstract = {The performance of low-density parity-check (LDPC) codes transmitted
	over a memoryless binary-input continuous output additive white Gaussian
	noise (AWGN) channel and decoded with quantized min-sum decoding
	is strongly influenced by the decoder's quantization scheme. This
	paper presents an efficient algorithm that determines the best uniform
	scalar quantizer for a particular code. To maximize performance,
	it is necessary to determine degree distributions that best match
	the characteristics of the quantized min-sum decoder. Toward this
	end, an iterative optimization framework that jointly optimizes the
	degree distributions and the quantizer is presented},
  doi = {10.1109/BSC.2006.1644559},
  file = {:PDF\\Memory_System_Optimization_for_FPGA-Based_Implementation_of_Quaso-Cyclic_LDPC_Codes_Decoder.pdf:PDF;:PDF\\01644559.pdf:PDF},
  keywords = {AWGN channel;LDPC codes;additive white Gaussian noise;discretized
	min-sum decoding;iterative optimization;low-density parity-check
	codes;memoryless binary-input;quantization scheme;AWGN channels;binary
	codes;iterative decoding;memoryless systems;parity check codes;quantisation
	(signal);}
}

@ARTICLE{KJC1969122,
  author = {K. J. C. Smith},
  title = {On the p-rank of the incidence matrix of points and hyperplanes in
	a finite projective geometry},
  journal = {Journal of Combinatorial Theory},
  year = {1969},
  volume = {7},
  pages = {122 - 129},
  number = {2},
  abstract = {The rank over GF(p), or p-rank, of the incidence matrix of points
	and hyperplanes in the finite projective geometry $PG(t, p^n)$ is
	shown to be equal to $\binom{p+t-1}{t}^{n}+1$. This result is obtained
	by representing the points of the geometry by elements of the field
	$K=GF(p^{n(t+1)})$ and by representing hyperplanes in terms of linear
	functionals on K expressed in terms of the trace from $K$ to $GF(p^n)$.},
  doi = {10.1016/S0021-9800(69)80046-3},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\KJC1969122.pdf:PDF},
  issn = {0021-9800},
  timestamp = {2011.11.10},
  url = {http://www.sciencedirect.com/science/article/pii/S0021980069800463}
}

@BOOK{Snygg2012,
  title = {A New Approach to Differential Geometry Using Clifford's Geometric
	Algebra},
  publisher = {Birkhauser},
  year = {2012},
  author = {John Snygg},
  timestamp = {2012.09.05}
}

@ARTICLE{908351,
  author = {Hongzin Song and Todd, R.M. and Cruz, J.R.},
  title = {Low density parity check codes for magnetic recording channels},
  journal = {Magnetics, IEEE Transactions on},
  year = {2000},
  volume = {36},
  pages = {2183 -2186},
  number = {5},
  month = sep,
  abstract = {We propose a system for magnetic recording, using a low density parity
	check (LDPC) code as the error-correcting-code, in conjunction with
	a rate 16/17 quasi-maximum-transition-run channel code and a modified
	E2PR4-equalized channel. Iterative decoding between the partial response
	channel and the LDPC code is performed. Simulations show that this
	system can achieve a 5.9 dB gain over uncoded EPR4. The algorithms
	used to design this LDPC code are also discussed},
  doi = {10.1109/20.908351},
  file = {:PDF\\Low_Density_Parity_Check_Codes_for_Magnetic_Recording_Channels.pdf:PDF},
  issn = {0018-9464},
  keywords = {5.9 dB;coding gain;error-correcting-code;iterative decoding;low density
	parity check code;magnetic recording channels;modified E2PR4-equalized
	channel;partial response channel;rate 16/17 quasi-maximum-transition-run
	channel code;channel coding;digital magnetic recording;equalisers;error
	correction codes;iterative decoding;partial response channels;}
}

@ARTICLE{1046102,
  author = {Leilei Song and Meng-Lin Yu and Shaffer, M.S.},
  title = {10- and 40-Gb/s forward error correction devices for optical communications},
  journal = {Solid-State Circuits, IEEE Journal of},
  year = {2002},
  volume = {37},
  pages = { 1565 - 1573},
  number = {11},
  month = {nov.},
  doi = {10.1109/JSSC.2002.803931},
  issn = {0018-9200},
  keywords = { 0.16 micron; 1.5 V; 2.5 to 40 Gbit/s; 343 mW; 360 mW; CMOS technology;
	RS decoder blocks; Reed-Solomon codes; asynchronous channels; complexity
	reduction; forward error correction devices; low-power parallel implementation;
	modified Euclidean algorithm; optical communication systems; parallel
	processing; performance monitoring functions; standard FEC devices;
	standard compliant framing; CMOS digital integrated circuits; Reed-Solomon
	codes; VLSI; circuit complexity; decoding; digital communication;
	digital signal processing chips; forward error correction; low-power
	electronics; monitoring; optical communication equipment; parallel
	algorithms; parallel architectures; telecommunication computing;}
}

@ARTICLE{4768575,
  author = {Shumei Song and Bo Zhou and Shu Lin and Abdel-Ghaffar, K.},
  title = {A unified approach to the construction of binary and nonbinary quasi-cyclic
	LDPC codes based on finite fields},
  journal = {Communications, IEEE Transactions on},
  year = {2009},
  volume = {57},
  pages = {84 -93},
  number = {1},
  month = {January},
  abstract = {A unified approach for constructing binary and nonbinary quasi-cyclic
	LDPC codes under a single framework is presented. Six classes of
	binary and nonbinary quasi-cyclic LDPC codes are constructed based
	on primitive elements, additive subgroups, and cyclic subgroups of
	finite fields. Numerical results show that the codes constructed
	perform well over the AWGN channel with iterative decoding.},
  doi = {10.1109/TCOMM.2009.0901.060129},
  file = {:PDF\\A_Unified_Approach_to_the_Construction_of_Binary_and_Nonbinary_Quasi-Cyclic_LDPC_Codes_Based_on_Finite_Field.pdf:PDF},
  issn = {0090-6778},
  keywords = {AWGN channel;additive white Gaussian noise channel;binary code;finite
	fields;iterative decoding;low density parity check code;nonbinary
	code;quasicyclic LDPC code;AWGN channels;binary codes;cyclic codes;iterative
	decoding;parity check codes;}
}

@ARTICLE{5281735,
  author = {Spagnol, C. and Marnane, W.},
  title = {A class of quasi-cyclic LDPC codes over GF(2m)},
  journal = {Communications, IEEE Transactions on},
  year = {2009},
  volume = {57},
  pages = {2524 -2527},
  number = {9},
  month = september,
  abstract = {Low Density Parity Check (LDPC) codes over GF(2m,)are an extension
	of binary LDPC codes. Performances of GF(2m,) LDPC codes have been
	shown to be higher than binary LDPC codes, but the complexity of
	the encoders/decoders increases. Hence there is a substantial lack
	of implementations for LDPC codes over GF(2m,)codes. This paper presents
	a class of quasi-cyclic LDPC codes over GF(2m,). These codes can
	alleviate the encoding/decoding complexity without excessive loss
	of performances. It is shown how, from a performance point of view,
	such codes are better than m times bigger binary codes and as good
	as (2m,)longer binary codes.},
  doi = {10.1109/TCOMM.2009.09.070644},
  file = {:PDF\\05281735.pdf:PDF},
  issn = {0090-6778},
  keywords = {binary codes;permutational codes;quasi-cyclic low density parity check
	codes;binary codes;cyclic codes;parity check codes;}
}

@INPROCEEDINGS{4036023,
  author = {Stepanov, M. and Chertkov, Michael},
  title = {Instanton analysis of Low-Density Parity-Check codes in the error-floor
	regime},
  booktitle = {Information Theory, 2006 IEEE International Symposium on},
  year = {2006},
  pages = {552-556},
  month = {July},
  abstract = {In this paper we develop instanton method introduced in V. Chernyak
	et al. (2004), M.G. Stepanov et al. (2005) to analyze quantitatively
	performance of low-density parity-check (LDPC) codes decoded iteratively
	in the so-called error-floor regime. We discuss statistical properties
	of the numerical instanton-amoeba scheme focusing on detailed analysis
	and comparison of two regular LDPC codes: Tanner's [155,64,20] and
	Margulis' [672,336,16] codes. In the regime of moderate values of
	the signal-to-noise ratio we critically compare results of the instanton-amoeba
	evaluations against the standard Monte Carlo calculations of the
	frame-error-rate},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\04036023.pdf:PDF},
  keywords = {iterative decoding;parity check codes;statistical analysis;LDPC;error-floor
	regime;instanton analysis;iterative decoding;low-density parity-check
	codes;numerical instanton-amoeba scheme;signal-to-noise ratio;statistical
	properties;AWGN;Automation;Iterative decoding;Laboratories;Maximum
	likelihood decoding;Monte Carlo methods;Parity check codes;Performance
	analysis;Signal to noise ratio;Testing},
  timestamp = {2014.03.05}
}

@INCOLLECTION{10.1007BFb0019850,
  author = {Stern, Jacques},
  title = {A method for finding codewords of small weight},
  booktitle = {Coding Theory and Applications},
  publisher = {Springer Berlin Heidelberg},
  year = {1989},
  editor = {Cohen, Gerard and Wolfmann, Jacques},
  volume = {388},
  series = {Lecture Notes in Computer Science},
  pages = {106-113},
  doi = {10.1007/BFb0019850},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\10.1007BFb0019850.pdf:PDF},
  isbn = {978-3-540-51643-9},
  timestamp = {2014.03.20},
  url = {http://dx.doi.org/10.1007/BFb0019850}
}

@ARTICLE{EuclidOnECC,
  author = {Yasuo Sugiyama and Masao Kasahara and Shigeichi Hirasawa and Toshihiko
	Namekawa},
  title = {A method for solving key equation for decoding goppa codes},
  journal = {Information and Control},
  year = {1975},
  volume = {27},
  pages = {87-99},
  month = {January},
  file = {:PDF\\A_method_for_solving_key_equation_for_decoding_Goppa_codes.pdf:PDF},
  issue = {1}
}

@INPROCEEDINGS{5167041,
  author = {Jinsun Suk and Jaechun No},
  title = {Performance Analysis of NAND Flash-Based SSD for Designing a Hybrid
	Filesystem},
  booktitle = {High Performance Computing and Communications, 2009. HPCC '09. 11th
	IEEE International Conference on},
  year = {2009},
  pages = {539 -544},
  month = {june},
  abstract = {As density doubles with the rapidly dropping price each year for the
	past seven years (currently 32 Gbits/chip), NAND flash memory has
	virtually replaced HDDs (hard disk drives) in battery-operated consumer
	devices such as cellular phones, PMPs, and PDAs. This trend has also
	enabled the introduction of so-called flash memory SSDs (solid state
	disks) that have an interface identical to that of HDDs but use NAND
	flash memory inside as storage media. The ordinary filesystems designed
	for HDDs are no longer suitable for SSDs because SSDs have many different
	features from HDDs. We designed a hybrid filesystem, called HybridFS,
	that uses two kinds of storages - HDDs and SSDs. This is accomplished
	by distributing data into two partitions based on their type. In
	HybridFS, the data blocks of a large regular file are stored in a
	data partition in HDDs, while the metadata being stored in the partition
	of SSDs. Separating data into the different storages of disk partitions
	makes it possible to produce high I/O performance by taking appropriate
	I/O approach,according to the data characteristics.},
  doi = {10.1109/HPCC.2009.69},
  file = {:PDF\\05167041.pdf:PDF},
  keywords = {HybridFS;I/O performance;NAND flash memory;NAND flash-based SSD;data
	block;data distribution;data partition;disk partition;hard disk drive;hybrid
	filesystem;memory SSD;metadata;solid state disk;storage media;disc
	drives;flash memories;hard discs;meta data;storage management;},
  timestamp = {2011.05.20}
}

@INPROCEEDINGS{4253085,
  author = {Yang Sun and Karkooti, M. and Cavallaro, J.R.},
  title = {VLSI Decoder Architecture for High Throughput, Variable Block-size
	and Multi-rate LDPC Codes},
  booktitle = {Circuits and Systems, 2007. ISCAS 2007. IEEE International Symposium
	on},
  year = {2007},
  pages = {2104 -2107},
  month = {may},
  abstract = {A low-density parity-check (LDPC) decoder architecture that supports
	variable block sizes and multiple code rates is presented. The proposed
	architecture is based on the structured quasi-cyclic (QC-LDPC) codes
	whose performance compares favorably with that of randomly constructed
	LDPC codes for short to moderate block sizes. The main contribution
	of this work is to address the variable block-size and multi-rate
	decoder hardware complexity that stems from the irregular LDPC codes.
	The overall decoder, which was synthesized, placed and routed on
	TSMC 0.13-micron CMOS technology with a core area of 4.5 square millimeters,
	supports variable code lengths from 360 to 4200 bits and multiple
	code rates between frac14 and 9/10. The average throughput can achieve
	1 Gbps at 2.2 dB SNR.},
  doi = {10.1109/ISCAS.2007.378514},
  file = {:PDF\\04253085.pdf:PDF},
  keywords = {0.13 micron;1 Gbit/s;360 to 4200 bit;CMOS technology;TSMC;VLSI decoder;low-density
	parity-check decoder;multirate LDPC codes;structured quasicyclic
	codes;variable block-size;variable code lengths;CMOS integrated circuits;VLSI;parity
	check codes;},
  timestamp = {2012.10.22}
}

@ARTICLE{PhysRevLett.105.120603,
  author = {Suwa, Hidemaro and Todo, Synge},
  title = {Markov Chain Monte Carlo Method without Detailed Balance},
  journal = {Phys. Rev. Lett.},
  year = {2010},
  volume = {105},
  pages = {120603},
  month = {Sep},
  doi = {10.1103/PhysRevLett.105.120603},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\PhysRevLett.105.120603.pdf:PDF},
  issue = {12},
  numpages = {4},
  publisher = {American Physical Society},
  timestamp = {2012.04.03},
  url = {http://link.aps.org/doi/10.1103/PhysRevLett.105.120603}
}

@INPROCEEDINGS{5634934,
  author = {Tai, K. and Kitakami, M.},
  title = {Prolongation of Lifetime and the Evaluation Method of Dependable
	SSD},
  booktitle = {Defect and Fault Tolerance in VLSI Systems (DFT), 2010 IEEE 25th
	International Symposium on},
  year = {2010},
  pages = {373 -381},
  month = {oct},
  abstract = {Since high-density flash memory has high error rate, strong error
	control is necessary for the solid-state drive (SSD). The number
	of erasure cycles of each memory cell is limited, where the cell
	should be erased before writing. Wear-leveling is used for leveling
	the erasure cycles in a flash memory. Since the existing wear-leveling
	is executed in a chip, it is not effective if write operations are
	concentrated into specified chips. This paper proposes wear-leveling
	and error control method by using redundant flash memories in order
	to improve reliability and lifetime of the SSD. In the proposed method,
	error control is usually executed, and wear-leveling among the chips
	is executed when the bias in the erasure cycles is large. The execution
	frequency of wear-leveling is adjusted considering deterioration
	of the cell. Evaluations of bit error rate and lifetime show that
	the proposed method has high reliability and durability.},
  doi = {10.1109/DFT.2010.60},
  file = {:PDF\\05634934.pdf:PDF},
  issn = {1550-5774},
  keywords = {SSD;bit error rate;erasure cycle;error control method;high-density
	flash memory;redundant flash memories;solid-state drive;wear-leveling;disc
	drives;error statistics;flash memories;},
  timestamp = {2011.05.20}
}

@ARTICLE{5437480,
  author = {Takemura, R. and Kawahara, T. and Miura, K. and Yamamoto, H. and
	Hayakawa, J. and Matsuzaki, N. and Ono, K. and Yamanouchi, M. and
	Ito, K. and Takahashi, H. and Ikeda, S. and Hasegawa, H. and Matsuoka,
	H. and Ohno, H.},
  title = {A 32-Mb SPRAM With 2T1R Memory Cell, Localized Bi-Directional Write
	Driver and `1'/`0' Dual-Array Equalized Reference Scheme},
  journal = {Solid-State Circuits, IEEE Journal of},
  year = {2010},
  volume = {45},
  pages = {869 -879},
  number = {4},
  month = april,
  abstract = {A 32-Mb SPin-transfer torque RAM (SPRAM) chip was demonstrated with
	an access time of 32 ns and a cell write-time of 40 ns at a supply
	voltage of 1.8 V. The chip was fabricated with 150-nm CMOS and a
	100 Ã 200-nm tunnel magneto-resistive (TMR) device element. A required
	thermal stability of 67 of the TMR device was estimated by taking
	into account the disturbances during read operations and data retention
	periods of 10 years for nonvolatile operation. The 32-Mb SPRAM chip
	features three circuit technologies suitable for a large-scale array:
	1) a two-transistor, one-resistor (2T1R) type memory cell for achieving
	a sufficiently large write current despite the small cell size, 2)
	a compact read/write separated hierarchy bit/source-line structure
	with a localized bi-directional write driver for efficiently distributing
	write current, and 3) a '1'/'0' dual-array equalized reference scheme
	for stable read operation.},
  doi = {10.1109/JSSC.2010.2040120},
  file = {:PDF\\A_32-Mb_SPRAM_With_2T1R_Memory_Cell_Localized_Bi-Directional_Write_Driver_and_1_0_Dual-Array_Equalized_Reference_Scheme.pdf:PDF},
  issn = {0018-9200},
  keywords = {2T1R memory cell;CMOS integrated circuit;SPRAM chip;bit/source-line
	structure;data retention periods;dual-array equalized reference scheme;localized
	bi-directional write driver;nonvolatile operation;one-resistor type
	memory cell;size 150 nm;spin-transfer torque RAM;storage capacity
	32 Mbit;thermal stability;time 32 ns;time 40 ns;tunnel magneto-resistive
	device element;two-transistor type memory cell;voltage 1.8 V;CMOS
	integrated circuits;driver circuits;random-access storage;thermal
	stability;write-once storage;}
}

@INPROCEEDINGS{5993667,
  author = {Takeuchi, Ken},
  title = {Green high performance storage class memory amp; NAND flash memory
	hybrid SSD system},
  booktitle = {Low Power Electronics and Design (ISLPED) 2011 International Symposium
	on},
  year = {2011},
  pages = {369 -370},
  month = {aug},
  abstract = {SSDs and emerging storage class non-volatile semiconductor memories
	such as PCRAM, FeRAM, RRAM and MRAM have enabled innovations in various
	nano-scale VLSI memory systems for personal computers, multimedia
	applications and enterprise servers [1,2]. This paper provides a
	comprehensive review on various state-of-the-art memory system architectures
	and related memory circuits for the green high performance computing.},
  doi = {10.1109/ISLPED.2011.5993667},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05993667.pdf:PDF},
  issn = {Pending},
  timestamp = {2011.08.28}
}

@ARTICLE{Takeuchi_co-design,
  author = {Ken Takeuchi},
  title = {Novel co-design of NAND Flash Memory and NAND Flash Controller Circuits
	for sub-30 nm Low-Power High-Speed Solid State Drives (SSD)},
  journal = {IEEE Journal of Solid-State Circuits},
  year = {2009},
  volume = {44},
  pages = {1227-1234},
  number = {4},
  file = {:PDF\\Novel_co-design_of_NAND_Flash_Controller_Circuits_for_Sub-30nm_Low-Power_High-Speed_Solid-State_Drives.pdf:PDF},
  publisher = {IEEE}
}

@INPROCEEDINGS{6177074,
  author = {Tanakamaru, S. and Yanagihara, Y. and Takeuchi, K.},
  title = {Over-10X-extended-lifetime 76%-reduced-error solid-state drives (SSDs)
	with error-prediction LDPC architecture and error-recovery scheme},
  booktitle = {Solid-State Circuits Conference Digest of Technical Papers (ISSCC),
	2012 IEEE International},
  year = {2012},
  pages = {424 -426},
  month = {feb.},
  abstract = {This paper presents solid-state drives (SSDs) with two high reliability
	techniques. First, an error-prediction (EP) low-density-parity-check
	(LDPC) error-correcting code (ECC) that realizes an over 10X; extended
	lifetime. Second, an error-recovery (ER) scheme that decreases the
	program-disturb error rate and the data-retention error rate by 74%
	and 56%, respectively.},
  doi = {10.1109/ISSCC.2012.6177074},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\06177074.pdf:PDF},
  issn = {0193-6530},
  keywords = {data-retention error rate;error-prediction LDPC architecture;error-recovery
	scheme;high reliability techniques;low-density-parity-check error-correcting
	code;program-disturb error rate;solid-state drives;disc drives;error
	correction codes;integrated memory circuits;parity check codes;},
  timestamp = {2012.04.25}
}

@ARTICLE{1302304,
  author = {Heng Tang and Jun Xu and Yu Kou and Lin, S. and Abdel-Ghaffar, K.},
  title = {On algebraic construction of Gallager and circulant low-density parity-check
	codes},
  journal = {Information Theory, IEEE Transactions on},
  year = {2004},
  volume = {50},
  pages = { 1269 - 1279},
  number = {6},
  month = {june},
  abstract = { This correspondence presents three algebraic methods for constructing
	low-density parity-check (LDPC) codes. These methods are based on
	the structural properties of finite geometries. The first method
	gives a class of Gallager codes and a class of complementary Gallager
	codes. The second method results in two classes of circulant-LDPC
	codes, one in cyclic form and the other in quasi-cyclic form. The
	third method is a two-step hybrid method. Codes in these classes
	have a wide range of rates and minimum distances, and they perform
	well with iterative decoding.},
  doi = {10.1109/TIT.2004.828088},
  file = {:PDF\\01302304.pdf:PDF},
  issn = {0018-9448},
  keywords = { Euclidean geometry; Gallager codes; LDPC codes; algebraic construction;
	circulant low-density parity-check codes; cyclic code; finite geometries;
	iterative decoding; projective geometry; quasicyclic code; sum-product
	algorithm; two-step hybrid method; cyclic codes; iterative decoding;
	parity check codes;},
  timestamp = {2011.04.15}
}

@ARTICLE{1386528,
  author = {Tang, H. and Xu, J. and Lin, S. and Abdel-Ghaffar, K.A.S.},
  title = {Codes on finite geometries},
  journal = {Information Theory, IEEE Transactions on},
  year = {2005},
  volume = {51},
  pages = {572 -596},
  number = {2},
  month = feb,
  abstract = {New algebraic methods for constructing codes based on hyperplanes
	of two different dimensions in finite geometries are presented. The
	new construction methods result in a class of multistep majority-logic
	decodable codes and three classes of low-density parity-check (LDPC)
	codes. Decoding methods for the class of majority-logic decodable
	codes, and a class of codes that perform well with iterative decoding
	in spite of having many cycles of length 4 in their Tanner graphs,
	are presented. Most of the codes constructed can be either put in
	cyclic or quasi-cyclic form and hence their encoding can be implemented
	with linear shift registers.},
  doi = {10.1109/TIT.2004.840867},
  file = {:PDF\\Codes_on_Finite_Geometries.pdf:PDF},
  issn = {0018-9448},
  keywords = {Euclidean geometry;LDPC;Tanner graphs;finite geometry;iterative decoding;linear
	shift registers;low-density parity-check codes;multistep majority-logic
	decodable codes;projective geometry;quasicyclic codes;sum product
	algorithm;cyclic codes;geometric codes;geometry;graph theory;iterative
	decoding;majority logic;parity check codes;shift registers;}
}

@ARTICLE{910591,
  author = {Tanner, R.M.},
  title = {Minimum-distance bounds by graph analysis},
  journal = {Information Theory, IEEE Transactions on},
  year = {2001},
  volume = {47},
  pages = {808 -821},
  number = {2},
  month = feb,
  abstract = {The parity-check matrix of a linear code is used to define a bipartite
	code constraint (Tanner) graph in which bit nodes are connected to
	parity-check nodes. The connectivity properties of this graph are
	analyzed using both local connectivity and the eigenvalues of the
	associated adjacency matrix. A simple lower bound on the minimum
	distance of the code is expressed in terms of the two largest eigenvalues.
	For a more powerful bound, local properties of the subgraph corresponding
	to a minimum-weight word in the code are used to create an optimization
	problem whose solution is a lower bound on the code's minimum distance.
	Linear programming gives one bound. The technique is illustrated
	by applying it to sparse block codes with parameters [7,3,4] and
	[42,23,6]},
  doi = {10.1109/18.910591},
  file = {:PDF\\Minimum-Distance_Bounds_by_Graph_analysis.pdf:PDF},
  issn = {0018-9448},
  keywords = {Tanner graph;associated adjacency matrix;bipartite code constraint
	graph;bit nodes are;connectivity properties;eigenvalues;graph analysis;linear
	code;linear programming;local connectivity;lower bound;minimum-distance
	bounds;minimum-weight word;optimization problem;parity-check matrix;parity-check
	nodes;sparse block codes;subgraph;binary codes;block codes;eigenvalues
	and eigenfunctions;error correction codes;graph theory;linear codes;linear
	programming;matrix algebra;}
}

@BOOKLET{SAGE_Tutorial,
  title = {Sage Tutorial},
  author = {The Sage Development Team},
  month = {July},
  year = {2009}
}

@BOOK{978-1461269441,
  title = {Gr\"{o}bner Bases: A Computational Approach to Commutative Algebra},
  publisher = {Springer},
  year = {2012},
  author = {Thomas Becker,Volker Weispfenning},
  abstract = {This book provides a comprehensive treatment of Groebner bases theory
	embedded in an introduction to commutative algebra from a computational
	point of view. The centerpiece of Gr bner bases theory is the Buchberger
	algorithm, which provides a common generalization of the Euclidean
	algorithm and the Gaussian elimination algorithm to multivariate
	polynomial rings. The book explains how the Buchberger algorithm
	and the theory surrounding it are eminently important both for the
	mathematical theory and for computational applications. A number
	of results such as optimized version of the Buchberger algorithm
	are presented in textbook format for the first time. This book requires
	no prerequisites other than the mathematical maturity of an advanced
	undergraduate and is therefore well suited for use as a textbook.
	At the same time, the comprehensive treatment makes it a valuable
	source of reference on Groebner bases theory for mathematicians,
	computer scientists, and others. Placing a strong emphasis on algorithms
	and their verification, while making no sacrifices in mathematical
	rigor, the book spans a bridge between mathematics and computer science.},
  isbn = {978-1461269441},
  timestamp = {2013.05.23}
}

@ARTICLE{1327837,
  author = {Tao Tian and Jones, C.R. and Villasenor, J.D. and Wesel, R.D.},
  title = {Selective avoidance of cycles in irregular LDPC code construction},
  journal = {Communications, IEEE Transactions on},
  year = {2004},
  volume = {52},
  pages = { 1242 - 1247},
  number = {8},
  month = {aug},
  abstract = { This letter explains the effect of graph connectivity on error-floor
	performance of low-density parity-check (LDPC) codes under message-passing
	decoding. A new metric, called extrinsic message degree (EMD), measures
	cycle connectivity in bipartite graphs of LDPC codes. Using an easily
	computed estimate of EMD, we propose a Viterbi-like algorithm that
	selectively avoids small cycle clusters that are isolated from the
	rest of the graph. This algorithm is different from conventional
	girth conditioning by emphasizing the connectivity as well as the
	length of cycles. The algorithm yields codes with error floors that
	are orders of magnitude below those of random codes with very small
	degradation in capacity-approaching capability.},
  doi = {10.1109/TCOMM.2004.833048},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\01327837.pdf:PDF},
  issn = {0090-6778},
  keywords = { LDPC code construction; Viterbi-like algorithm; bipartite graphs;
	capacity-approaching capability; error-floor performance; extrinsic
	message degree; girth conditioning; graph connectivity; graph cycles;
	iterative decoding; low-density parity-check codes; message-passing
	decoding; small cycle cluster; stopping sets; unstructured graph
	construction; decoding; error statistics; graph theory; message passing;
	parity check codes;},
  timestamp = {2012.04.10}
}

@INPROCEEDINGS{5424408,
  author = {Yuan Heng Tseng and Chia-En Huang and Kuo, C.-H. and Chih, Y.-D.
	and Chrong Jung Lin},
  title = {High density and ultra small cell size of Contact ReRAM (CR-RAM)
	in 90nm CMOS logic technology and circuits},
  booktitle = {Electron Devices Meeting (IEDM), 2009 IEEE International},
  year = {2009},
  pages = {1 -4},
  month = {dec.},
  abstract = {A new contact RRAM cell realized by TiN/TiON layers stacked between
	the W-plug and n+ diffusion region inside a small 80 Ã 80 nm contact
	hole is demonstrated using 90nm CMOS logic technology. This work
	reports the first time a resistive switching characteristics of the
	TiON layers sandwiched between the metal and Si substrate. The new
	Contact ReRAM cell exhibits highly stable read window and very small
	cell size of 0.19Â¿m2. By limiting the active ReRAM film in a small
	contact hole region, the cell effectively operates under a very low
	set voltage of 4V and a reset current of 150Â¿A, while achieving
	fast set and reset speed of less than 100ns and 10us, respectively.
	Excellent endurance of more than 1000k cycles and stable data retention
	characteristics further support the new Contact ReRAM (CR-RAM) cell
	will be a superior NVM technology for the future.},
  doi = {10.1109/IEDM.2009.5424408},
  file = {:PDF\\05424408.pdf:PDF},
  keywords = {CMOS logic technology;Si;Si substrate;TiN-TiON;contact ReRAM;data
	retention;n+ diffusion region;resistive switching;size 90 nm;ultrasmall
	cell size;CMOS logic circuits;random-access storage;switching;titanium
	compounds;},
  timestamp = {2011.05.23}
}

@ARTICLE{breakVGbounds,
  author = {M. Tsfasman and S.Vladut and T.Zink},
  title = {On Goppa Codes Wihch Are Better than the Varshamov-Gilbert Bound},
  journal = {Math. Nachr.},
  year = {1982},
  volume = {109},
  pages = {21-28},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\MANA19821090103.pdf:PDF}
}

@INPROCEEDINGS{4480418,
  author = {Tsuchiya, Toshiaki},
  title = {Mechanism of Hot-Electron-Induced NMOSFET's Degradation},
  booktitle = {VLSI Technology, 1987. Digest of Technical Papers. Symposium on},
  year = {1987},
  pages = {53 -54},
  month = {may}
}

@INPROCEEDINGS{5351273,
  author = {Van, V.T. and Matsui, H. and Mita, S.},
  title = {Generalized quasi-cyclic low-density parity-check codes based on
	finite geometries},
  booktitle = {Information Theory Workshop, 2009. ITW 2009. IEEE},
  year = {2009},
  pages = {158 -162},
  month = oct,
  abstract = {In this study, we proved that several promising classes of codes based
	on finite geometries cannot be classified as quasi-cyclic (QC) codes
	but should be included in broader generalized quasi-cyclic (GQC)
	codes. Further, we proposed an algorithm (transpose algorithm) for
	the computation of the Grobner bases from the parity check matrices
	of GQC codes. Because of the GQC structure of such codes, they can
	be encoded systematically using GroÂ¿bner bases and their encoder
	can be implemented using simple feedback-shift registers. In order
	to demonstrate the efficiency of our encoder, we proved that the
	number of circuit elements in the encoder architecture is proportional
	to the code length for finite geometry (FG) LDPC codes. For codes
	constructed using points and lines of finite geometries, the hardware
	complexity of the serial-in serial-out encoder architecture of the
	codes is linear order O(n). To encode a binary codeword of length
	n, less than 2n adder and 3n memory elements are required.},
  doi = {10.1109/ITW.2009.5351273},
  file = {:PDF\\05351273.pdf:PDF},
  keywords = {binary codeword;circuit elements;code length;feedback-shift registers;finite
	geometry bLDPC codes;generalized quasi-cyclic low-density parity-check
	codes;hardware complexity;linear order code;parity check matrices;serial-in
	serial-out encoder architecture;communication complexity;cyclic codes;linear
	codes;matrix algebra;parity check codes;}
}

@INPROCEEDINGS{5683369,
  author = {Vo Tam Van and Matsui, H. and Mita, S.},
  title = {A Class of Generalized Quasi-Cyclic LDPC Codes: High-Rate and Low-Complexity
	Encoder for Data Storage Devices},
  booktitle = {GLOBECOM 2010, 2010 IEEE Global Telecommunications Conference},
  year = {2010},
  pages = {1 -6},
  month = dec,
  abstract = {In this paper, we study no 4-cycle, high-rate LDPC codes based on
	finite geometries for use in data storage devices and prove that
	these codes cannot be classified as quasi-cyclic (QC) codes but should
	be considered as broader generalized quasi-cyclic (GQC) codes. Because
	of the GQC structure of such codes, they can be systematically encoded
	using Groebner bases and their encoder can be implemented using simple
	feedback-shift registers. In order to demonstrate the efficiency
	of the encoder, we show that the hardware complexity of the serial-in
	serial-out encoder architecture of these codes is of linear order
	O(n). To encode a binary codeword of length n, less than 2n adders
	and 3n memory elements are required. Furthermore, we evaluated the
	error performances of these codes with sum product algorithm (SPA)
	decoding over additive white Gaussian noise (AWGN) channels. At a
	bit error rate (BER) of 10-5, they perform 1-dB away from the Shannon
	limit after 10 decoding iterations.},
  doi = {10.1109/GLOCOM.2010.5683369},
  file = {:PDF\\A_Class_of_generalized_quasi-cyclic_LDPC_codes_high-rate_and_low-complexity_encoder_for_data_storage_devices.pdf:PDF},
  issn = {1930-529X},
  keywords = {AWGN channel;additive white Gaussian noise channel;bit error rate;data
	storage device;low complexity encoder;quasi cyclic LDPC codes;quasi
	cyclic code;AWGN channels;cyclic codes;parity check codes;}
}

@ARTICLE{ieice_trans_A_volE92_No9_2009_pp2353-2359,
  author = {Vo TAM VAN and Hajime MATSUI and Seiichi MITA},
  title = {Computation of Gr\"{o}bner Basis for Systematic Encoding of Generalized
	Quasi-Cyclic Codes},
  journal = {IEICE TRANSACTIONS on Fundamentals of Electronics, Communications
	and Computer Sciences},
  year = {2009},
  volume = {E92},
  pages = {2345--2359},
  number = {9},
  month = {September},
  abstract = {Generalized quasi-cyclic (GQC) codes form a wide and useful class
	of linear codes that includes thoroughly quasi-cyclic codes, finite
	geometry (FG) low density parity check (LDPC) codes, and Hermitian
	codes. Although it is known that the systematic encoding of GQC codes
	is equivalent to the division algorithm in the theory of Grobner
	basis of modules, there has been no algorithm that computes Grobner
	basis for all types of GQC codes. In this paper, we propose two algorithms
	to compute Grobner basis for GQC codes from their parity check matrices;
	we call them echelon canonical form algorithm and transpose algorithm.
	Both algorithms require sufficiently small number of finite-field
	operations with the order of the third power of code-length. Each
	algorithm has its own characteristic. The first algorithm is composed
	of elementary methods and is appropriate for low-rate codes. The
	second algorithm is based on a novel formula and has smaller computational
	complexity than the first one for high-rate codes with the number
	of orbits (cyclic parts) less than half of the code length. Moreover,
	we show that a serial-in serial-out encoder architecture for FG LDPC
	codes is composed of linear feedback shift registers with the size
	of the linear order of code-length; to encode a binary codeword of
	length n, it takes less than 2n adder and 2n memory elements.},
  file = {:PDF\\ieice_trans_A_volE92_No9_2009_pp2345-2359.pdf:PDF},
  keywords = {automorphism group, Buchberger's algorithm, division algorithm, circulant
	matrix, finite geometry low density parity check (LDPC) codes},
  timestamp = {2012.06.19}
}

@ARTICLE{DBLP:journals/corr/abs-0811-4033,
  author = {Vo Tam Van and Hajime Matsui and Seiichi Mita},
  title = {Computation of Grobner basis for systematic encoding of generalized
	quasi-cyclic codes},
  journal = {CoRR},
  year = {2008},
  volume = {abs/0811.4033},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee = {http://arxiv.org/abs/0811.4033},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\0811.4033v1.pdf:PDF},
  timestamp = {2011.10.11}
}

@INPROCEEDINGS{1023584,
  author = {Vasic, B.},
  title = {Combinatorial constructions of low-density parity check codes for
	iterative decoding},
  booktitle = {Information Theory, 2002. Proceedings. 2002 IEEE International Symposium
	on},
  year = {2002},
  pages = {312-},
  abstract = {We introduce a combinatorial construction of regular low-density parity
	check (LDPC) codes based on balanced incomplete block designs, or
	more specifically on cyclic difference families of Abelian groups
	and affine geometries. Several constructions are presented, and the
	bounds on minimal distance are derived by using the concept of Pasch
	configurations.},
  doi = {10.1109/ISIT.2002.1023584},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\01023584.pdf:PDF},
  keywords = {combinatorial mathematics;cyclic codes;group codes;iterative decoding;parity
	check codes;Abelian groups;LDPC codes;Pasch configurations;affine
	geometries;balanced incomplete block designs;combinatorial construction;cyclic
	difference families;iterative decoding;low-density parity check codes;minimal
	distance;regular codes;Computational geometry;Hardware;Iterative
	decoding;Lattices;Magnetic recording;Optical design;Optical fiber
	communication;Parity check codes;Ultraviolet sources;Welding},
  timestamp = {2013.11.08}
}

@INPROCEEDINGS{5394825,
  author = {Vasić, B. and Chilappagari, S.K. and Nguyen, D.V. and Planjery,
	S.K.},
  title = {Trapping set ontology},
  booktitle = {Communication, Control, and Computing, 2009. Allerton 2009. 47th
	Annual Allerton Conference on},
  year = {2009},
  pages = {1-7},
  month = {Sept},
  abstract = {The failures of iterative decoders for low-density parity-check (LDPC)
	codes on the additive white Gaussian noise channel (AWGNC) and the
	binary symmetric channel (BSC) can be understood in terms of combinatorial
	objects known as trapping sets. In this paper, we derive a systematic
	method to identify the most relevant trapping sets for decoding over
	the BSC in the error floor region. We elaborate on the notion of
	the critical number of a trapping set and derive a classification
	of trapping sets. We then develop the trapping set ontology, a database
	of trapping sets that summarizes the topological relations among
	trapping sets. We elucidate the usefulness of the trapping set ontology
	in predicting the error floor as well as in designing better codes.},
  doi = {10.1109/ALLERTON.2009.5394825},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\05394825.pdf:PDF},
  keywords = {AWGN channels;channel coding;parity check codes;AWGNC;BSC;LDPC;additive
	white Gaussian noise channel;binary symmetric channel;error floor
	region;low-density parity-check codes;trapping set ontology;Additive
	white noise;Algorithm design and analysis;Databases;Degradation;Error
	analysis;H infinity control;Iterative algorithms;Iterative decoding;Ontologies;Parity
	check codes;Coding theory;iterative coding techniques},
  timestamp = {2014.03.04}
}

@ARTICLE{5106465,
  author = {Vincent, G. and Chantre, A. and Bois, D.},
  title = {Electric field effect on the thermal emission of traps in semiconductor
	junctions},
  journal = {Journal of Applied Physics},
  year = {1979},
  volume = {50},
  pages = {5484 -5487},
  number = {8},
  month = {aug },
  abstract = {Electric field effects on the thermal emission of traps in a diode
	have been studied. Calculations were performed and compared with
	experimental data on deep centers in GaAs. The results are consistent
	with a thermal equivalent of the optical Franz #x2010;Keldysh effect.},
  doi = {10.1063/1.326601},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05106465.pdf:PDF},
  issn = {0021-8979},
  timestamp = {2011.09.14}
}

@ARTICLE{springerlink:10.1007/BF01083182,
  author = {Vladut, S. G. and Drinfel'd, V. G.},
  title = {Number of points of an algebraic curve},
  journal = {Functional Analysis and Its Applications},
  year = {1983},
  volume = {17},
  pages = {53-54},
  note = {10.1007/BF01083182},
  file = {:PDF\\1007BF01083182.pdf:PDF},
  issn = {0016-2663},
  issue = {1},
  keyword = {Mathematics and Statistics},
  publisher = {Springer New York},
  timestamp = {2011.07.01},
  url = {http://dx.doi.org/10.1007/BF01083182}
}

@ARTICLE{1676616,
  author = {Wang, C.C. and Troung, T.K. and Shao, H.M. and Deutsch, L.J. and
	Omura, J.K. and Reed, I.S.},
  title = {VLSI Architectures for Computing Multiplications and Inverses in
	GF(2m)},
  journal = {Computers, IEEE Transactions on},
  year = {1985},
  volume = {C-34},
  pages = {709 -717},
  number = {8},
  month = aug,
  abstract = {Finite field arithmetic logic is central in the implementation of
	Reed-Solomon coders and in some cryptographic algorithms. There is
	a need for good multiplication and inversion algorithms that can
	be easily realized on VLSI chips. Massey and Omura [1] recently developed
	a new multiplication algorithm for Galois fields based on a normal
	basis representation. In this paper, a pipeline structure is developed
	to realize the Massey-Omura multiplier in the finite field GF(2m).
	With the simple squaring property of the normal basis representation
	used together with this multiplier, a pipeline architecture is also
	developed for computing inverse elements in GF(2m). The designs developed
	for the Massey-Omura multiplier and the computation of inverse elements
	are regular, simple, expandable, and therefore, naturally suitable
	for VLSI implementation.},
  doi = {10.1109/TC.1985.1676616},
  file = {:PDF\\VLSI_Architectures_for_Computing_Multiplications_and_Inverses_in_GF(2^m).pdf:PDF},
  issn = {0018-9340},
  keywords = {Finite field inverse;Massey-Omura multiplier;finite field multiplication;finite
	field multiplier;inverse;normal basis, normal basis multiplier;pipeline;systolic
	array;}
}

@ARTICLE{1542413,
  author = {Wang, C.-C. and Kulkarni, S.R. and Poor, H.V.},
  title = {Density evolution for asymmetric memoryless channels},
  journal = {Information Theory, IEEE Transactions on},
  year = {2005},
  volume = {51},
  pages = {4216 -4236},
  number = {12},
  month = {dec. },
  abstract = {Density evolution (DE) is one of the most powerful analytical tools
	for low-density parity-check (LDPC) codes and graph codes with message
	passing decoding algorithms. With channel symmetry as one of its
	fundamental assumptions, density evolution has been widely and successfully
	applied to different channels, including binary erasure channels
	(BECs), binary symmetric channels (BSCs), binary additive white Gaussian
	noise (BiAWGN) channels, etc. This paper generalizes density evolution
	for asymmetric memoryless channels, which in turn broadens the applications
	to general memoryless channels, e.g., z-channels, composite white
	Gaussian noise channels, etc. The central theorem underpinning this
	generalization is the convergence to perfect projection for any fixed-size
	supporting tree. A new iterative formula of the same complexity is
	then presented and the necessary theorems for the performance concentration
	theorems are developed. Several properties of the new density evolution
	method are explored, including stability results for general asymmetric
	memoryless channels. Simulations, code optimizations, and possible
	new applications suggested by this new density evolution method are
	also provided. This result is also used to prove the typicality of
	linear LDPC codes among the coset code ensemble when the minimum
	check node degree is sufficiently large. It is shown that the convergence
	to perfect projection is essential to the belief propagation (BP)
	algorithm even when only symmetric channels are considered. Hence,
	the proof of the convergence to perfect projection serves also as
	a completion of the theory of classical density evolution for symmetric
	memoryless channels.},
  doi = {10.1109/TIT.2005.858931},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\01542413.pdf:PDF},
  issn = {0018-9448},
  keywords = {BEC;BSC;BiAWGN;belief propagation algorithm;binary additive white
	Gaussian noise channel;binary erasure channel;binary symmetric channel;convergence;density
	evolution method;graph code;iterative formula;linear LDPC code;low-density
	parity-check code;memoryless channel;message passing decoding algorithm;sum-product
	algorithm;AWGN channels;binary codes;channel coding;convergence of
	numerical methods;iterative decoding;linear codes;message passing;parity
	check codes;},
  timestamp = {2012.02.29}
}

@ARTICLE{PhysRevE.64.056101,
  author = {Wang, Fugao and Landau, D. P.},
  title = {Determining the density of states for classical statistical models:
	A random walk algorithm to produce a flat histogram},
  journal = {Phys. Rev. E},
  year = {2001},
  volume = {64},
  pages = {056101},
  month = {Oct},
  doi = {10.1103/PhysRevE.64.056101},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\cond-mat0107006v1.pdf:PDF;:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\PhysRevE.64.056101.pdf:PDF},
  issue = {5},
  numpages = {16},
  publisher = {American Physical Society},
  timestamp = {2012.03.27},
  url = {http://link.aps.org/doi/10.1103/PhysRevE.64.056101}
}

@INPROCEEDINGS{6134417,
  author = {Jiadong Wang and Courtade, T. and Shankar, H. and Wesel, R.D.},
  title = {Soft Information for LDPC Decoding in Flash: Mutual-Information Optimized
	Quantization},
  booktitle = {Global Telecommunications Conference (GLOBECOM 2011), 2011 IEEE},
  year = {2011},
  pages = {1 -6},
  month = {dec.},
  abstract = {High-capacity NAND flash memory can achieve high density storage by
	using multi-level cells (MLC) to store more than one bit per cell.
	Although this larger storage capacity is certainly beneficial, the
	increased density also increases the raw bit error rate (BER), making
	powerful error correction coding necessary. Traditional flash memories
	employ simple algebraic codes, such as BCH codes, that can correct
	a fixed, specified number of errors. This paper investigates the
	application of low-density parity-check (LDPC) codes which are well
	known for their ability to approach capacity in the AWGN channel.
	We obtain soft information for the LDPC decoder by performing multiple
	cell reads with distinct word-line voltages. The values of the word-line
	voltages (also called reference voltages) are optimized by maximizing
	the mutual information between the input and output of the multiple-read
	channel. Our results show that using this soft information in the
	LDPC decoder provides a significant benefit and enables us to outperform
	BCH codes over a range of block error rates.},
  doi = {10.1109/GLOCOM.2011.6134417},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\06134417.pdf:PDF},
  issn = {1930-529X},
  keywords = {AWGN channel;BCH codes;BER;LDPC decoding;NAND flash memory;algebraic
	codes;block error rates;block length;comparable rate;error correction
	coding;high density storage;low-density parity-check codes;multilevel
	cells;multiple-read channel;mutual-information optimized quantization;raw
	bit error rate;reference voltages;soft information;word-line voltages;AWGN
	channels;BCH codes;NAND circuits;algebraic codes;decoding;error correction
	codes;error statistics;flash memories;parity check codes;},
  timestamp = {2012.03.29}
}

@INPROCEEDINGS{1493353,
  author = {Nuo Wang and Srinivasan, R.},
  title = {On importance sampling for iteratively decoded linear block codes},
  booktitle = {Communications, Circuits and Systems, 2005. Proceedings. 2005 International
	Conference on},
  year = {2005},
  volume = {1},
  pages = { 18 - 23 Vol. 1},
  month = {may},
  abstract = { We introduce an importance sampling (IS) scheme for fast performance
	evaluation of the linear block codes with message-passing decoding.
	This novel scheme overcomes the existing difficulties in IS to some
	extent when code length is large and requires codebook information.
	Experiments show very high IS gains for iteratively decoded linear
	block codes such as single parity-check (SPC) codes and block product
	codes, as well as low density parity-check (LDPC) codes. We also
	provide a scheme based on our IS method to find the minimum Hamming
	distance and pick up those codewords which are very useful when calculating
	asymptotic performance.},
  doi = {10.1109/ICCCAS.2005.1493353},
  file = {:PDF\\01493353.pdf:PDF},
  issn = { },
  keywords = { LDPC codes; SPC codes; asymptotic performance; block product codes;
	importance sampling; iterative decoding; linear block codes; low
	density parity-check codes; message-passing decoding; minimum Hamming
	distance; performance evaluation; single parity-check codes; Hamming
	codes; block codes; importance sampling; iterative decoding; linear
	codes; message passing; parity check codes; product codes;},
  timestamp = {2012.09.04}
}

@ARTICLE{4114369,
  author = {Zhongfeng Wang and Zhiqiang Cui},
  title = {Low-Complexity High-Speed Decoder Design for Quasi-Cyclic LDPC Codes},
  journal = {Very Large Scale Integration (VLSI) Systems, IEEE Transactions on},
  year = {2007},
  volume = {15},
  pages = {104 -114},
  number = {1},
  month = jan.,
  abstract = {This paper studies low-complexity high-speed decoder architectures
	for quasi-cyclic low density parity check (QC-LDPC) codes. Algorithmic
	transformation and architectural level optimization are incorporated
	to reduce the critical path. Enhanced partially parallel decoding
	architectures are proposed to linearly increase the throughput of
	conventional partially parallel decoders through introducing a small
	percentage of extra hardware. Based on the proposed architectures,
	a (8176, 7154) Euclidian geometry-based QC-LDPC code decoder is implemented
	on Xilinx field programmable gate array (FPGA) Virtex-II 6000, where
	an efficient nonuniform quantization scheme is employed to reduce
	the size of memories storing soft messages. FPGA implementation results
	show that the proposed decoder can achieve a maximum (source data)
	decoding throughput of 172 Mb/s at 15 iterations},
  doi = {10.1109/TVLSI.2007.891098},
  file = {:PDF\\Low-Complexity_High-Speed_Decoder_Design_for_Quasi-Cyclic_LDPC_Codes.pdf:PDF},
  issn = {1063-8210},
  keywords = {LDPC codes;error correction codes;field programmable gate arrays;low
	density parity check codes;parallel processing;quasi-cyclic codes;cyclic
	codes;error correction codes;field programmable gate arrays;parallel
	processing;parity check codes;quantisation (signal);}
}

@INPROCEEDINGS{5599244,
  author = {Qingsong Wei and Bozhao Gong and Pathak, S. and Tay, Y.C.},
  title = {FlashCoop: A Locality-Aware Cooperative Buffer Management for SSD-Based
	Storage Cluster},
  booktitle = {Parallel Processing (ICPP), 2010 39th International Conference on},
  year = {2010},
  pages = {634 -643},
  month = {sept.},
  abstract = {Random writes significantly limit the application of flash-based Solid
	State Drive (SSD) in enterprise environment due to its poor latency,
	negative impact on SSD lifetime and high garbage collection overhead.
	To release above limitations, we propose a locality-aware cooperative
	buffer scheme referred to as FlashCoop (Flash Cooperation), which
	leverages free memory of neighboring storage server to buffer writes
	over high speed network. Both temporal and sequential localities
	of access pattern are exploited in the design of cooperative buffer
	management. Leveraging the filtering effect of the cooperative buffer,
	FlashCoop can efficiently shape the I/O request stream and improve
	the sequentiality of the write accesses passed to the SSD. FlashCoop
	has been extensively evaluated under various enterprise workloads.
	Our benchmark results conclusively demonstrate that FlashCoop can
	achieve 52.3% performance improvement and 56.5% garbage collection
	overhead reduction compared to the system without FlashCoop.},
  doi = {10.1109/ICPP.2010.71},
  file = {:PDF\\05599244.pdf:PDF},
  issn = {0190-3918},
  keywords = {FlashCoop;I/O request stream;SSD based storage cluster;flash based
	solid state drive;flash cooperation;high garbage collection overhead;locality
	aware cooperative buffer scheme;neighboring storage server;random
	writes;buffer storage;flash memories;},
  timestamp = {2011.05.20}
}

@ARTICLE{1055987,
  author = { Welch, L. and Scholtz, R.},
  title = {Continued fractions and Berlekamp's algorithm},
  journal = {Information Theory, IEEE Transactions on},
  year = {1979},
  volume = {25},
  pages = { 19 - 27},
  number = {1},
  month = jan,
  abstract = { Theorems are presented concerning the optimality of rational approximations
	using non-Archimedean norms. The algorithm for developing the rational
	approximations is based on continued fraction techniques and is virtually
	equivalent to an algorithm employed by Berlekamp for decoding BCH
	codes. Several variations of the continued fraction technique and
	Berlekamp's algorithm are illustrated on a common example.},
  doi = {10.1109/TIT.1979.1055987},
  issn = {0018-9448},
  keywords = { Approximation methods; BCH codes; Continued fractions; Decoding;}
}

@ARTICLE{1054128,
  author = { Weldon, E., Jr.},
  title = {New generalizations of the Reed-Muller codes--II: Nonprimitive codes},
  journal = {Information Theory, IEEE Transactions on},
  year = {1968},
  volume = {14},
  pages = { 199 - 205},
  number = {2},
  month = mar,
  abstract = { In this paper a class of nonprimitive cyclic codes quite similar
	in structure to the original Reed-Muller codes is presented. These
	codes, referred to herein as nonprimitive Reed-Muller codes, are
	shown to possess many of the properties of the primitive codes. Specifically,
	two major results are presented. First the code length, number of
	information symbols, and minimum distance are shown to be related
	by means of a parameter known as the order of the code. These relationships
	show that for given values of code length and rate the codes have
	relatively large minimum distances. It is also shown that the codes
	are subcodes of the BCH codes of the same length and guaranteed minimum
	distance; thus in general the codes are not as powerful as the BCH
	codes. However, for most interesting values of code length and rate
	the difference between the two types of codes is slight. The second
	result is the observation that the codes can be decoded with a variation
	of the original algorithm proposed by Reed for the Reed-Muller codes.
	In other words, they areL-step orthogonalizable. Because of their
	large minimum distances and the simplicity of their decoders, nonprimitive
	Reed-Muller codes seem attractive for use in error-control systems
	requiring multiple random-error correction.},
  doi = {10.1109/TIT.1968.1054128},
  file = {:C\:\\Users\\Public\\Documents\\My eBooks\\refereces\\PDF\\01054128.pdf:PDF},
  issn = {0018-9448},
  keywords = { Cyclic codes; Reed-Muller codes;}
}

@ARTICLE{Wood1989277,
  author = {Jay A Wood},
  title = {Spinor groups and algebraic coding theory },
  journal = {Journal of Combinatorial Theory, Series A },
  year = {1989},
  volume = {51},
  pages = {277 - 313},
  number = {2},
  abstract = {The purpose of this paper is to explore the equivalence between the
	abelian subgroups of Ṽ(n), the “diagonal” extra-special 2-group of
	the compact, simple, simply-connected Lie group Spin(n), and the
	self-orthogonal linear binary codes of algebraic coding theory. In
	particular, the basic abstract structure theory of the abelian subgroups
	of Ṽ(n) is reflected in the distinction between ordinary self-orthogonal
	codes and even self-orthogonal codes. Work of Quillen on the equivariant
	cohomology of Spin(n) affects the classification of even self-orthogonal
	codes. },
  doi = {http://dx.doi.org/10.1016/0097-3165(89)90053-8},
  file = {:PDF\\Wood1989277.pdf:PDF},
  issn = {0097-3165},
  timestamp = {2013.06.25},
  url = {http://www.sciencedirect.com/science/article/pii/0097316589900538}
}

@ARTICLE{Wu:2010:DEH:1880037.1880040,
  author = {Wu, Xiaoxia and Li, Jian and Zhang, Lixin and Speight, Evan and Rajamony,
	Ram and Xie, Yuan},
  title = {Design exploration of hybrid caches with disparate memory technologies},
  journal = {ACM Trans. Archit. Code Optim.},
  year = {2010},
  volume = {7},
  pages = {15:1--15:34},
  month = {December},
  acmid = {1880040},
  address = {New York, NY, USA},
  articleno = {15},
  doi = {http://doi.acm.org/10.1145/1880037.1880040},
  file = {:PDF\\a15-wu.pdf:PDF},
  issn = {1544-3566},
  issue = {3},
  issue_date = {December 2010},
  keywords = {Hybrid cache architecture, Three-dimensional IC design, cache hierarchy,
	embedded DRAM, magnetic RAM, nonuniform cache architecture, phase-change
	RAM, power, thermal},
  numpages = {34},
  publisher = {ACM},
  timestamp = {2011.06.03},
  url = {http://doi.acm.org/10.1145/1880037.1880040}
}

@INPROCEEDINGS{1204566,
  author = {Bo Xia and Ryan, W.E.},
  title = {On importance sampling for linear block codes},
  booktitle = {Communications, 2003. ICC '03. IEEE International Conference on},
  year = {2003},
  volume = {4},
  pages = { 2904 - 2908 vol.4},
  month = {may},
  abstract = {We introduce an importance sampling scheme for linear block codes
	with message-passing decoding. This novel scheme overcomes an existing
	difficulty in the IS practice that requires codebook information.
	Experiments show large IS gains for single parity-check codes and
	short-length block codes. For medium-length block codes, IS gains
	in the order of 103 and higher are observed at high signal-to-noise
	ratio.},
  doi = {10.1109/ICC.2003.1204566},
  file = {:PDF\\01204566.pdf:PDF},
  keywords = { codebook information; linear block codes; medium-block length codes;
	message-passing decoding; sampling scheme; short-length block codes;
	signal-to-noise ratios; single-parity check codes; block codes; decoding;
	linear codes; message passing; noise; parity check codes; sampling
	methods;},
  timestamp = {2012.09.04}
}

@ARTICLE{5089484,
  author = {Hua Xiao and Banihashemi, A.H.},
  title = {Error rate estimation of low-density parity-check codes on binary
	symmetric channels using cycle enumeration},
  journal = {Communications, IEEE Transactions on},
  year = {2009},
  volume = {57},
  pages = {1550-1555},
  number = {6},
  abstract = {The performance of low-density parity-check (LDPC) codes decoded by
	hard-decision iterative decoding algorithms can be accurately estimated
	if the weight J and the number |EJ| of the smallest error patterns
	that cannot be corrected by the decoder are known. To obtain J and
	|EJ|, one would need to perform the direct enumeration of error patterns
	with weight i les J. The complexity of enumeration increases exponentially
	with J, essentially as nJ, where n is the code block length. This
	limits the application of direct enumeration to codes with small
	n and J. In this letter, we approximate J and |EJ | by enumerating
	and testing the error patterns that are subsets of short cycles in
	the code's Tanner graph. This reduces the computational complexity
	by several orders of magnitude compared to direct enumeration, making
	it possible to estimate the error rates for almost any practical
	LDPC code. To obtain the error rate estimates, we propose an algorithm
	that progressively improves the estimates as larger cycles are enumerated.
	Through a number of examples, we demonstrate that the proposed method
	can accurately estimate both the bit error rate (BER) and the frame
	error rate (FER) of regular and irregular LDPC codes decoded by a
	variety of hard-decision iterative decoding algorithms.},
  doi = {10.1109/TCOMM.2009.06.070048},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\05089484.pdf:PDF},
  issn = {0090-6778},
  keywords = {binary codes;computational complexity;error statistics;iterative decoding;parity
	check codes;LDPC code;Tanner graph;binary symmetric channels;bit
	error rate;code block length;computational complexity;cycle enumeration;direct
	enumeration;error patterns;error rate estimation;frame error rate;hard
	decision iterative decoding;low density parity check codes;Bit error
	rate;Communications Society;Computational complexity;Error analysis;Error
	correction codes;Estimation error;Iterative algorithms;Iterative
	decoding;Parity check codes;Testing;Binary symmetric channels (BSC),
	low-density parity-check (LDPC) codes, finite-length LDPC codes,
	error rate estimation of finite-length LDPC codes, error floor, hard-decision
	decoding algorithms, iterative decoding, Tanner graph cycles.},
  timestamp = {2013.12.17}
}

@INPROCEEDINGS{4259758,
  author = {Hua Xiao and Banihashemi, A.H.},
  title = {Estimation of Bit and Frame Error Rates of Low-Density Parity-Check
	Codes on Binary Symmetric Channels},
  booktitle = {Information Theory, 2007. CWIT '07. 10th Canadian Workshop on},
  year = {2007},
  pages = {73-76},
  abstract = {A method for estimating the performance of low-density parity-check
	(LDPC) codes decoded by hard-decision iterative decoding algorithms
	on binary symmetric channels (BSC) is proposed. Based on the enumeration
	of the smallest weight error patterns that cannot be all corrected
	by the decoder, this method estimates both the frame error rate (FER)
	and the bit error rate (BER) of a given LDPC code with very good
	precision for all crossover probabilities of practical interest.
	Through a number of examples, we show that the proposed method can
	be effectively applied to both regular and irregular LDPC codes and
	to a variety of hard-decision iterative decoding algorithms. Compared
	with the conventional Monte Carlo simulation, the proposed method
	has a much smaller computational complexity, particularly for lower
	error rates.},
  doi = {10.1109/CWIT.2007.375704},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\04259758.pdf:PDF},
  keywords = {Monte Carlo methods;binary codes;channel coding;error statistics;iterative
	decoding;parity check codes;Monte Carlo simulation;binary symmetric
	channel;bit error rate;channel coding;frame error rate;iterative
	decoding algorithm;low-density parity-check codes;Bit error rate;Computational
	complexity;Error analysis;Error correction;Error correction codes;Iterative
	algorithms;Iterative decoding;Parity check codes;Performance analysis},
  timestamp = {2013.12.20}
}

@ARTICLE{5352236,
  author = {Wei Xu and Hongbin Sun and Xiaobin Wang and Yiran Chen and Tong Zhang},
  title = {Design of Last-Level On-Chip Cache Using Spin-Torque Transfer RAM
	(STT RAM)},
  journal = {Very Large Scale Integration (VLSI) Systems, IEEE Transactions on},
  year = {2011},
  volume = {19},
  pages = {483 -493},
  number = {3},
  month = {march },
  abstract = {Because of its high storage density with superior scalability, low
	integration cost and reasonably high access speed, spin-torque transfer
	random access memory (STT RAM) appears to have a promising potential
	to replace SRAM as last-level on-chip cache (e.g., L2 or L3 cache)
	for microprocessors. Due to unique operational characteristics of
	its storage device magnetic tunneling junction (MTJ), STT RAM is
	inherently subject to a write latency versus read latency tradeoff
	that is determined by the memory cell size. This paper first quantitatively
	studies how different memory cell sizing may impact the overall computing
	system performance, and shows that different computing workloads
	may have conflicting expectations on memory cell sizing. Leveraging
	MTJ device switching characteristics, we further propose an STT RAM
	architecture design method that can make STT RAM cache with relatively
	small memory cell size perform well over a wide spectrum of computing
	benchmarks. This has been well demonstrated using CACTI-based memory
	modeling and computing system performance simulations using SimpleScalar.
	Moreover, we show that this design method can also reduce STT RAM
	cache energy consumption by up to 30% over a variety of benchmarks.},
  doi = {10.1109/TVLSI.2009.2035509},
  file = {:PDF\\05352236.pdf:PDF},
  issn = {1063-8210},
  keywords = {CACTI-based memory modeling;STT RAM;SimpleScalar;high access speed;last-level
	on-chip cache;magnetic tunneling junction;random access memory;spin-torque
	transfer RAM;storage device;magnetic tunnelling;random-access storage;},
  timestamp = {2011.06.03}
}

@ARTICLE{5497218,
  author = {Xu, W. and Zhang, T.},
  title = {A Time-Aware Fault Tolerance Scheme to Improve Reliability of Multilevel
	Phase-Change Memory in the Presence of Significant Resistance Drift},
  journal = {Very Large Scale Integration (VLSI) Systems, IEEE Transactions on},
  year = {2010},
  volume = {PP},
  pages = {1 -11},
  number = {99},
  abstract = {Because of its promising scalability potential and support of multilevel
	per cell storage, phase-change memory has become a topic of great
	current interest. However, recent studies show that structural relaxation
	effect makes the resistance of phase-change material drift over the
	time, which can severely degrade multilevel per cell phase-change
	memory storage reliability. This makes powerful memory fault tolerance
	solutions indispensable, where error correction code (ECC) will play
	an essential role. This work aims to develop fault tolerance solutions
	that can effectively compensate memory cell resistance drift. First,
	based upon information-theoretical study, we show that conventional
	use of ECC, which is unaware of memory content lifetime, can only
	achieve the performance with a big gap from the information-theoretical
	bounds. This motivates us to study the potential of time-aware memory
	fault tolerance, where the basic idea is to keep track the memory
	content lifetime and use this lifetime information to accordingly
	adjust how memory cell resistance is quantized and interpreted for
	ECC decoding. Under this time-aware fault tolerance framework, we
	study the use of two types of ECCs, including classical codes such
	as BCH that only demand hard-decision input and advanced codes such
	as low-density parity-check (LDPC) codes that demand soft-decision
	probability input. Using hypothetical four-level per cell and eight-level
	per cell phase-change memory with BCH and LDPC codes as test vehicles,
	we carry out extensive analysis and simulations, which demonstrate
	very significant performance advantages of such time-aware memory
	fault tolerance strategy in the presence of significant memory cell
	resistance drift.},
  doi = {10.1109/TVLSI.2010.2052640},
  file = {:PDF\\A_Time-Aware_Fault_Tolerance_Scheme_to_Improve_Reliability_of_Multilevel_Phase-Change_Memory_in_the_Presence_of_Significant_Resistance_Drift.pdf:PDF},
  issn = {1063-8210}
}

@INPROCEEDINGS{843915,
  author = {Yamada, R. and Mori, Y. and Okuyama, Y. and Yugami, J. and Nishimoto,
	T. and Kume, H.},
  title = {Analysis of detrap current due to oxide traps to improve flash memory
	retention},
  booktitle = {Reliability Physics Symposium, 2000. Proceedings. 38th Annual 2000
	IEEE International},
  year = {2000},
  pages = {200 -204},
  doi = {10.1109/RELPHY.2000.843915},
  keywords = {1 year;Fowler-Nordheim stressing;MOS capacitors;MOSFET;Si-SiO2;conduction
	mechanism;constant current stress;constant voltage stress;deep traps;detrap
	current;direct tunneling;electron injection;flash memory retention
	characteristics;hole injection;metal-oxide-semiconductor structures;oxide
	conduction band;oxide traps;shallow traps;thermally excited electron
	tunneling;threshold voltage shift;MOS capacitors;MOSFET;electron
	traps;flash memories;hole traps;integrated circuit reliability;interface
	states;tunnelling;}
}

@INPROCEEDINGS{934976,
  author = {Yamada, R. and Sekiguchi, T. and Okuyama, Y. and Yugami, J. and Kume,
	H.},
  title = {A novel analysis method of threshold voltage shift due to detrap
	in a multi-level flash memory},
  booktitle = {VLSI Technology, 2001. Digest of Technical Papers. 2001 Symposium
	on},
  year = {2001},
  pages = {115 -116},
  doi = {10.1109/VLSIT.2001.934976},
  keywords = {charge detrapping;detrap;detrap centroid;detrapping;detrapping parameter;electron
	detrapping;flash-memory retention characteristics;hole detrapping;multi-level
	flash memory;multi-level flash memory design;programmed memory cell;threshold
	voltage shift;tunnel oxide;tunnel-oxide degradation;carrier lifetime;electron
	traps;flash memories;hole traps;integrated circuit design;integrated
	memory circuits;tunnelling;}
}

@INPROCEEDINGS{1577854,
  author = {Lei Yang and Hui Liu and Shi, C.-J.R.},
  title = {VLSI implementation of a low-error-floor and capacity-approaching
	low-density parity-check code decoder with multi-rate capacity},
  booktitle = {Global Telecommunications Conference, 2005. GLOBECOM '05. IEEE},
  year = {2005},
  volume = {3},
  pages = { 6 pp.},
  month = nov.-2 # dec.,
  abstract = {With the superior error correction capability, low-density parity-check
	(LDPC) codes have initiated wide scale interests in wireless communication
	and storage fields. In the past, various structures of single code-rate
	LDPC decoders have been reported. However, to cover a wide range
	of service requirements and diverse interference conditions in wireless
	applications, LDPC decoders that can operate at both high and low
	code rates are desirable. In this paper, a 9k code length multi-rate
	LDPC decoder architecture is presented and implemented on a Xilinx
	FPGA device. Using pin selection, three operating modes, namely,
	the irregular 1/2 code, the regular 5/8 code and the regular 7/8
	code, are supported. Furthermore, to suppress the error floor level,
	a characterization on the conditions for short cycles in a LDPC code
	matrix expanded from a small base matrix is presented, and a cycle
	elimination algorithm is developed to detect and break such short
	cycles. The effectiveness of the cycle elimination algorithm has
	been verified by both simulation and hardware measurements, which
	show that the error floor is suppressed to a much lower level without
	incurring any performance penalty. The implemented decoder is tested
	in an experimental LDPC-OFDM system and achieves the superior measured
	performance of block error rate below 10-7 at SNR 1.8 dB.},
  doi = {10.1109/GLOCOM.2005.1577854},
  file = {:PDF\\VLSI_Implementation_of_a_Low-Error-Floor_and_Capacity-Approaching_Low-Density_Parity-Check-Code_Decoder_with_Multi-Rate_Capacity.pdf:PDF},
  keywords = { LDPC; OFDM; VLSI implementation; Xilinx FPGA device; capacity-approaching
	decoder; cycle elimination algorithm; low-density parity-check code
	decoder; low-error-floor decoder; multi-rate capacity; small base
	matrix; OFDM modulation; VLSI; decoding; field programmable gate
	arrays; parity check codes;}
}

@ARTICLE{1291797,
  author = {Yang, M. and Ryan, W.E. and Yan Li},
  title = {Design of efficiently encodable moderate-length high-rate irregular
	LDPC codes},
  journal = {Communications, IEEE Transactions on},
  year = {2004},
  volume = {52},
  pages = { 564 - 571},
  number = {4},
  month = april,
  abstract = {This paper presents a new class of irregular low-density parity-check
	(LDPC) codes of moderate length (103 le;n le;104) and high rate (R
	ge;3/4). Codes in this class admit low-complexity encoding and have
	lower error-rate floors than other irregular LDPC code-design approaches.
	It is also shown that this class of LDPC codes is equivalent to a
	class of systematic serial turbo codes and is an extension of irregular
	repeat-accumulate codes. A code design algorithm based on the combination
	of density evolution and differential evolution optimization with
	a modified cost function is presented. Moderate-length, high-rate
	codes with no error-rate floors down to a bit-error rate of 10-9
	are presented. Although our focus is on moderate-length, high-rate
	codes, the proposed coding scheme is applicable to irregular LDPC
	codes with other lengths and rates.},
  doi = {10.1109/TCOMM.2004.826367},
  file = {:PDF\\Design_of_Efficiently_Encodable_Moderate-Length_High-Rate_Irregular_LDPC_Codes.pdf:PDF},
  issn = {0090-6778},
  keywords = { error rate floor; irregular repeat-accumulate codes; low-density
	parity-check codes; serial turbo codes; error statistics; parity
	check codes; turbo codes;}
}

@INPROCEEDINGS{5749736,
  author = {Qing Yang and Jin Ren},
  title = {I-CASH: Intelligently Coupled Array of SSD and HDD},
  booktitle = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International
	Symposium on},
  year = {2011},
  pages = {278 -289},
  month = {feb.},
  abstract = {This paper presents a new disk I/O architecture composed of an array
	of a flash memory SSD (solid state disk) and a hard disk drive (HDD)
	that are intelligently coupled by a special algorithm. We call this
	architecture I-CASH: Intelligently Coupled Array of SSD and HDD.
	The SSD stores seldom-changed and mostly read reference data blocks
	whereas the HDD stores a log of deltas between currently accessed
	I/O blocks and their corresponding reference blocks in the SSD so
	that random writes are not performed in SSD during online I/O operations.
	High speed delta compression and similarity detection algorithms
	are developed to control the pair of SSD and HDD. The idea is to
	exploit the fast read performance of SSDs and the high speed computation
	of modern multi-core CPUs to replace and substitute, to a great extent,
	the mechanical operations of HDDs. At the same time, we avoid runtime
	SSD writes that are slow and wearing. An experimental prototype I-CASH
	has been implemented and is used to evaluate I-CASH performance as
	compared to existing SSD/HDD I/O architectures. Numerical results
	on standard benchmarks show that I-CASH reduces the average I/O response
	time by an order of magnitude compared to existing disk I/O architectures
	such as RAID and SSD/HDD storage hierarchy, and provides up to 2.8
	speedup over state-of-the-art pure SSD storage. Furthermore, I-CASH
	reduces random writes to SSD implying reduced wearing and prolonged
	life time of the SSD.},
  doi = {10.1109/HPCA.2011.5749736},
  file = {:PDF\\05749736.pdf:PDF},
  issn = {1530-0897},
  keywords = {I-CASH architecture;delta compression algorithm;disk input-output
	architecture;flash memory SSD;hard disk drive;intelligently coupled
	array of SSD and HDD;multicore CPU;similarity detection algorithm;solid
	state disk;disc drives;flash memories;hard discs;},
  timestamp = {2011.05.19}
}

@ARTICLE{1459044,
  author = {Yedidia, J.S. and Freeman, W.T. and Weiss, Y.},
  title = {Constructing free-energy approximations and generalized belief propagation
	algorithms},
  journal = {Information Theory, IEEE Transactions on},
  year = {2005},
  volume = {51},
  pages = {2282 - 2312},
  number = {7},
  month = {july},
  abstract = { Important inference problems in statistical physics, computer vision,
	error-correcting coding theory, and artificial intelligence can all
	be reformulated as the computation of marginal probabilities on factor
	graphs. The belief propagation (BP) algorithm is an efficient way
	to solve these problems that is exact when the factor graph is a
	tree, but only approximate when the factor graph has cycles. We show
	that BP fixed points correspond to the stationary points of the Bethe
	approximation of the free energy for a factor graph. We explain how
	to obtain region-based free energy approximations that improve the
	Bethe approximation, and corresponding generalized belief propagation
	(GBP) algorithms. We emphasize the conditions a free energy approximation
	must satisfy in order to be a "valid" or "maxent-normal" approximation.
	We describe the relationship between four different methods that
	can be used to generate valid approximations: the "Bethe method",
	the "junction graph method", the "cluster variation method", and
	the "region graph method". Finally, we explain how to tell whether
	a region-based approximation, and its corresponding GBP algorithm,
	is likely to be accurate, and describe empirical results showing
	that GBP can significantly outperform BP.},
  doi = {10.1109/TIT.2005.850085},
  file = {:PDF\\01459044.pdf:PDF},
  issn = {0018-9448},
  keywords = { Bethe approximation; GBP algorithm; Kikuchi free energy; cluster
	variation method; factor graphs; free energy approximation; generalized
	belief propagation; inference problem; junction graph method; message
	passing; region graph method; sum-product algorithm; backpropagation;
	belief networks; graph theory; inference mechanisms; message passing;},
  timestamp = {2012.10.15}
}

@INPROCEEDINGS{965981,
  author = {Yeo, E. and Pakzad, P. and Nikolic, B. and Anantharam, V.},
  title = {High throughput low-density parity-check decoder architectures},
  booktitle = {Global Telecommunications Conference, 2001. GLOBECOM '01. IEEE},
  year = {2001},
  volume = {5},
  pages = {3019 -3024 vol.5},
  abstract = {Two decoding schedules and the corresponding serialized architectures
	for low-density parity-check (LDPC) decoders are presented. They
	are applied to codes with parity-check matrices generated either
	randomly or using geometric properties of elements in Galois fields.
	Both decoding schedules have low computational requirements. The
	original concurrent decoding schedule has a large storage requirement
	that is dependent on the total number of edges in the underlying
	bipartite graph, while a new, staggered decoding schedule which uses
	an approximation of the belief propagation, has a reduced memory
	requirement that is dependent only on the number of bits in the block.
	The performance of these decoding schedules is evaluated through
	simulations on a magnetic recording channel},
  doi = {10.1109/GLOCOM.2001.965981},
  file = {:PDF\\00965981.pdf:PDF},
  keywords = {Galois fields;LDPC decoders;belief propagation;bipartite graph;concurrent
	decoding;decoding schedules;low-density parity-check decoders;magnetic
	recording channel;parity check matrices;serialized architectures;staggered
	decoding schedule;Galois fields;belief maintenance;decoding;error
	detection codes;magnetic recording;},
  timestamp = {2011.04.22}
}

@ARTICLE{e96-a_12_2562,
  author = {Yichao LU,Gang HE,Guifen TIAN,Satoshi GOTO},
  title = {Hybrid Message-Passing Algorithm and Architecture for Decoding Cyclic
	Non-binary LDPC Codes},
  journal = {IEICE TRANSACTIONS on Fundamentals of Electronics, Communications
	and Computer Sciences},
  year = {2013},
  volume = {E96-A},
  pages = {2652-2659},
  number = {12},
  month = {01},
  abstract = {Recently, non-binary low-density parity-check (NB-LDPC) codes starts
	to show their superiority in achieving significant coding gains when
	moderate codeword lengths are adopted. However, the overwhelming
	decoding complexity keeps NB-LDPC codes from being widely employed
	in modern communication devices. This paper proposes a hybrid message-passing
	decoding algorithm which consumes very low computational complexity.
	It achieves competitive error performance compared with conventional
	Min-max algorithm. Simulation result on a (255,174) cyclic code shows
	that this algorithm obtains at least 0.5dB coding gain over other
	state-of-the-art low-complexity NB-LDPC decoding algorithms. A partial-parallel
	NB-LDPC decoder architecture for cyclic NB-LDPC codes is also developed
	based on this algorithm. Optimization schemes are employed to cut
	off hard decision symbols in RAMs and also to store only part of
	the reliability messages. In addition, the variable node units are
	redesigned especially for the proposed algorithm. Synthesis results
	demonstrate that about 24.3% gates and 12% memories can be saved
	over previous works.},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\e96-a_12_2652.pdf:PDF},
  timestamp = {2014.04.03}
}

@ARTICLE{887870,
  author = {Dian-Wu Yue and Guang-Zeng Feng},
  title = {Minimum cyclotomic coset representatives and their applications to
	BCH codes and Goppa codes},
  journal = {Information Theory, IEEE Transactions on},
  year = {2000},
  volume = {46},
  pages = {2625 -2628},
  number = {7},
  month = {nov},
  abstract = {We consider the minimum cyclotomic coset representatives and derive
	some of their properties. The results allow more precise estimates
	of the dimension of Bose-Chaudhuri-Hocquenghem (BCH) and classical
	Goppa codes of a given designed minimum distance, and more precise
	estimates of the true designed distance of BCH codes and the minimum
	distance of classical Goppa codes},
  doi = {10.1109/18.887870},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\00887870.pdf:PDF},
  issn = {0018-9448},
  keywords = {BCH codes;Bose-Chaudhuri-Hocquenghem codes;Goppa codes;code dimension;minimum
	cyclotomic coset representatives;minimum distance;true designed distance;BCH
	codes;Goppa codes;set theory;},
  timestamp = {2012.02.07}
}

@INPROCEEDINGS{1188418,
  author = {Zarkeshvari, F. and Banihashemi, A.H.},
  title = {On implementation of min-sum algorithm for decoding low-density parity-check
	(LDPC) codes},
  booktitle = {Global Telecommunications Conference, 2002. GLOBECOM '02. IEEE},
  year = {2002},
  volume = {2},
  pages = { 1349 - 1353 vol.2},
  month = nov.,
  abstract = { This paper is concerned with the implementation issues of the so-called
	min-sum algorithm (also referred to as max-sum or max-product) for
	the decoding of low-density parity-check (LDPC) codes. The effects
	of clipping threshold and the number of quantization bits on the
	performance of the min-sum algorithm at short and intermediate block
	lengths are studied. It is shown that min-sum is robust against quantization
	effects, and in many cases, only four quantization bits suffices
	to obtain close to ideal performance. We also propose modifications
	to the min-sum algorithm that improve the performance by a few tenths
	of a dB with just a small increase in decoding complexity.},
  doi = {10.1109/GLOCOM.2002.1188418},
  file = {:PDF\\On_Implementation_of_Min-Sum_Algorithm_for_Decoding_Low-Density_Parity-Check(LDPC)_Codes.pdf:PDF},
  issn = { },
  keywords = { LDPC codes; block lengths; clipping threshold; decoding; low-density
	parity-check codes; max-product algorithm; max-sum algorithm; min-sum
	algorithm; performance; quantization bits; block codes; error statistics;
	iterative decoding; parity check codes; product codes; quantisation
	(signal);}
}

@ARTICLE{4489627,
  author = {Lingqi Zeng and Lan Lan and Tai, Y.Y. and Shumei Song and Shu Lin
	and Abdel-Ghaffar, K.},
  title = {Transactions Papers - Constructions of Nonbinary Quasi-Cyclic LDPC
	Codes: A Finite Field Approach},
  journal = {Communications, IEEE Transactions on},
  year = {2008},
  volume = {56},
  pages = {545 -554},
  number = {4},
  month = april,
  abstract = {This paper is concerned with construction of efficiently encodable
	nonbinary quasi-cyclic LDPC codes based on finite fields. Four classes
	of nonbinary quasi-cyclic LDPC codes are constructed. Experimental
	results show that codes constructed perform well with iterative decoding
	using a fast Fourier transform based q-ary sum-product algorithm
	and they achieve significant coding gains over Reed-Solomon codes
	of the same lengths and rates decoded with either algebraic hard-
	decision Berlekamp-Massey algorithm or algebraic soft-decision Kotter-Vardy
	algorithm.},
  doi = {10.1109/TCOMM.2008.060024},
  file = {:PDF\\Constructions_of_Nonbinary_Quasi-Cyclic_LDPC_Codes_A_Finite_Field_Approach.pdf:PDF},
  issn = {0090-6778},
  keywords = {fast Fourier transform;finite field approach;iterative decoding;nonbinary
	quasi-cyclic LDPC codes;q-ary sum-product algorithm;fast Fourier
	transforms;iterative decoding;matrix algebra;parity check codes;}
}

@ARTICLE{4471933,
  author = {Lingqi Zeng and Lan Lan and Ying Yu Tai and Bo Zhou and Shu Lin and
	Abdel-Ghaffar, K.A.S.},
  title = {Construction of nonbinary cyclic, quasi-cyclic and regular LDPC codes:
	a finite geometry approach},
  journal = {Communications, IEEE Transactions on},
  year = {2008},
  volume = {56},
  pages = {378 -387},
  number = {3},
  month = march,
  abstract = {This paper presents five methods for constructing nonbinary LDPC codes
	based on finite geometries. These methods result in five classes
	of nonbinary LDPC codes, one class of cyclic LDPC codes, three classes
	of quasi-cyclic LDPC codes and one class of structured regular LDPC
	codes. Experimental results show that constructed codes in these
	classes decoded with iterative decoding based on belief propagation
	perform very well over the AWGN channel and they achieve significant
	coding gains over Reed-Solomon codes of the same lengths and rates
	with either algebraic hard-decision decoding or Kotter-Vardy algebraic
	soft-decision decoding at the expense of a larger decoding computational
	complexity.},
  doi = {10.1109/TCOMM.2008.060025},
  file = {:PDF\\04471933.pdf:PDF},
  issn = {0090-6778},
  keywords = {AWGN channel;Kotter-Vardy algebraic soft-decision decoding;Reed-Solomon
	codes;algebraic hard-decision decoding;belief propagation;decoding
	computational complexity;finite geometry approach;iterative decoding;low
	density parity check codes;quasicyclic LDPC codes;AWGN channels;iterative
	decoding;parity check codes;}
}

@INPROCEEDINGS{5496999,
  author = {Gong Zhang and Chiu, L. and Dickey, C. and Ling Liu and Muench, P.
	and Seshadri, S.},
  title = {Automated lookahead data migration in SSD-enabled multi-tiered storage
	systems},
  booktitle = {Mass Storage Systems and Technologies (MSST), 2010 IEEE 26th Symposium
	on},
  year = {2010},
  pages = {1 -6},
  month = {may},
  abstract = {The significant IO improvements of Solid State Disks (SSD) over traditional
	rotational hard disks makes it an attractive approach to integrate
	SSDs in tiered storage systems for performance enhancement. However,
	to integrate SSD into multi-tiered storage system effectively, automated
	data migration between SSD and HDD plays a critical role. In many
	real world application scenarios like banking and supermarket environments,
	workload and IO profile present interesting characteristics and also
	bear the constraint of workload deadline. How to fully release the
	power of data migration while guaranteeing the migration deadline
	is critical to maximizing the performance of SSD-enabled multi-tiered
	storage system. In this paper, we present an automated, deadline-aware,
	lookahead migration scheme to address the data migration challenge.
	We analyze the factors that may impact on the performance of lookahead
	migration efficiency and develop a greedy algorithm to adaptively
	determine the optimal lookahead window size to optimize the effectiveness
	of lookahead migration, aiming at improving overall system performance
	and resource utilization while meeting workload deadlines. We compare
	our lookahead migration approach with the basic migration model and
	validate the effectiveness and efficiency of our adaptive lookahead
	migration approach through a trace driven experimental study.},
  doi = {10.1109/MSST.2010.5496999},
  file = {:PDF\\05496999.pdf:PDF},
  keywords = {HDD;IO improvements;SSD-enabled multitiered storage systems;automated
	data migration;automated lookahead data migration;data migration
	challenge;greedy algorithm;lookahead migration efficiency;migration
	deadline;optimal lookahead window size;performance enhancement;resource
	utilization;solid state disks;traditional rotational hard disks;workload
	deadline;disc drives;greedy algorithms;hard discs;storage management;},
  timestamp = {2011.05.20}
}

@ARTICLE{1706484,
  author = {Zhang, H. and Hekstra, A.P. and Yin, B.},
  title = {Performance evaluation of LDPC codes in bliss scheme-based storage
	systems using density evolution},
  journal = {Consumer Electronics, IEEE Transactions on},
  year = {2006},
  volume = {52},
  pages = {879 -887},
  number = {3},
  month = {aug},
  abstract = {Density evolution algorithms for scaled min-sum decoding of both regular
	and irregular low-density parity-check (LDPC) codes in Bliss scheme-based
	storage systems are presented, to evaluate the performance of LDPC
	codes in terms of noise threshold. Firstly, with the assumption of
	fixed error propagation factor and independently and identically
	distributed (i.i.d.) messages, density evolution algorithms of regular
	LDPC codes are derived to compare performance among LDPC codes with
	the same code rate but different bit/check node degrees and determine
	the optimum scaling factor of scaled min-sum decoding. Secondly,
	with the looser assumption of fixed error propagation and independently
	distributed messages, density evolution algorithms of irregular LDPC
	codes are derived for performance evaluation of LDPC codes with arbitrary
	degree distributions. Comparisons with simulations show density evolution
	is a proper performance evaluation method for LDPC codes in Bliss
	scheme-based storage systems},
  doi = {10.1109/TCE.2006.1706484},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\01706484.pdf:PDF},
  issn = {0098-3063},
  keywords = {Bliss scheme-based storage systems;LDPC codes;density evolution;fixed
	error propagation factor;independently and identically distributed
	messages;low-density parity-check;scaled min-sum decoding;decoding;digital
	storage;parity check codes;},
  timestamp = {2011.11.24}
}

@INPROCEEDINGS{1197141,
  author = {Juntan Zhang and Fossorier, M.},
  title = {Shuffled belief propagation decoding},
  booktitle = {Signals, Systems and Computers, 2002. Conference Record of the Thirty-Sixth
	Asilomar Conference on},
  year = {2002},
  volume = {1},
  pages = { 8 - 15 vol.1},
  month = nov.,
  abstract = { In this paper, we propose a shuffled version of the belief propagation
	(BP) algorithm for the decoding of low-density parity-check (LDPC)
	codes. We show that when the Tanner graph of the code is acyclic
	and connected, the proposed scheme is optimal in the sense of MAP
	decoding and converges faster (or at least no slower) than the standard
	BP algorithm. Interestingly, this new version keeps the computational
	advantages of the forward-backward implementations of BP decoding.
	Both serial and parallel implementations are considered. We show
	by simulation that the new schedule offers better performance/complexity
	trade-offs.},
  doi = {10.1109/ACSSC.2002.1197141},
  file = {:PDF\\Shuffled_Belief_Propagation_Decoding.pdf:PDF},
  issn = {1058-6393 },
  keywords = { BP algorithm; LDPC codes; MAP decoding; Tanner graph; computational
	complexity; forward-backward implementations; low-density parity-check
	codes; maximum a posteriori probability; parallel implementation;
	serial implementation; shuffled belief propagation decoding; computational
	complexity; graph theory; maximum likelihood decoding; maximum likelihood
	estimation; parity check codes;}
}

@INPROCEEDINGS{Zhao:2011:DMB:1973009.1973104,
  author = {Zhao, Weisheng and Torres, Lionel and Guillemenet, Yoann and Cargnini,
	Lu\'{\i}s Vit\'{o}rio and Lakys, Yahya and Klein, Jacques-Olivier
	and Ravelosona, Dafine and Sassatelli, Gilles and Chappert, Claude},
  title = {Design of MRAM based logic circuits and its applications},
  booktitle = {Proceedings of the 21st edition of the great lakes symposium on Great
	lakes symposium on VLSI},
  year = {2011},
  series = {GLSVLSI '11},
  pages = {431--436},
  address = {New York, NY, USA},
  publisher = {ACM},
  acmid = {1973104},
  doi = {http://doi.acm.org/10.1145/1973009.1973104},
  file = {:PDF\\p431-zhao.pdf:PDF},
  isbn = {978-1-4503-0667-6},
  keywords = {3d integration, flip-flop, fpga, full-adder, magnetic logic, memory-in-logic,
	mram, non-volatile},
  location = {Lausanne, Switzerland},
  numpages = {6},
  timestamp = {2011.06.03},
  url = {http://doi.acm.org/10.1145/1973009.1973104}
}

@INPROCEEDINGS{5624881,
  author = {Xiongxin Zhao and Zhixiang Chen and Xiao Peng and Dajiang Zhou and
	Goto, S.},
  title = {A BER performance-aware early termination scheme for layered LDPC
	decoder},
  booktitle = {Signal Processing Systems (SIPS), 2010 IEEE Workshop on},
  year = {2010},
  pages = {416 -419},
  month = {oct},
  abstract = {This paper presents a novel early termination scheme for layered LDPC
	decoder. By solving the bit error rate (BER) performance degradation
	which will occur when other early termination schemes are applied
	in layered LDPC decoder, the proposed method achieves very fast termination
	speed without BER performance loss. It is the best solution for BER
	performance-aware layered LDPC decoders, such as satellite video
	broadcasting applications.},
  doi = {10.1109/SIPS.2010.5624881},
  file = {:PDF\\05624881.pdf:PDF},
  issn = {1520-6130},
  keywords = {BER performance-aware early termination scheme;bit error rate performance
	degradation;layered LDPC decoder;low-density parity-check;satellite
	video broadcasting;error statistics;parity check codes;},
  timestamp = {2011.04.26}
}

@INPROCEEDINGS{6039973,
  author = {Zhaoxia Zheng and Xiong Yin and Yan Hong and Yi Dan},
  title = {Efficient Degree Optimization for High-Rate Structured QC-LDPC},
  booktitle = {Wireless Communications, Networking and Mobile Computing (WiCOM),
	2011 7th International Conference on},
  year = {2011},
  pages = {1 -4},
  month = {sept.},
  abstract = {In order to efficiently optimize degree distribution of high-rate
	structured QC-LDPC (SQC-LDPC), based on analysis to the special structure
	of SQC-LDPC, this paper modifies previous degree distribution optimization
	method. At first, it proposes a new set of global optimization constraints
	for Differential Evolution (DE) algorithm. Furthermore, when Gaussian
	Approximate (GA) is used to compute the threshold of a given degree
	distribution pair, binary search rather than usually used fixed step
	search is adopted to speed up degree distribution optimization. Simulation
	results indicate that proposed degree distribution optimization method
	can obtain good degree distribution pairs of high-rate SQC-LDPC with
	less iteration times, and that high-rate SQC-LDPC designed by good
	degree distribution pairs embraces good performance.},
  doi = {10.1109/wicom.2011.6039973},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\06039973.pdf:PDF},
  issn = {2161-9646},
  keywords = {DE algorithm;GA;Gaussian approximation;binary search;degree distribution
	optimization method;differential evolution algorithm;fixed step search;global
	optimization constraint;high-rate SQC-LDPC;high-rate structured QC-LDPC;iteration
	time;Gaussian processes;approximation theory;cyclic codes;evolutionary
	computation;parity check codes;radio networks;search problems;},
  timestamp = {2012.01.11}
}

@ARTICLE{4939224,
  author = {Bo Zhou and Jingyu Kang and Ying Tai and Shu Lin and Zhi Ding},
  title = {High Performance Non-Binary Quasi-Cyclic LDPC Codes on Euclidean
	Geometries LDPC Codes on Euclidean Geometries},
  journal = {Communications, IEEE Transactions on},
  year = {2009},
  volume = {57},
  pages = {1298 -1311},
  number = {5},
  month = {may },
  abstract = {This paper presents algebraic methods for constructing high performance
	and efficiently encodable non-binary quasi-cyclic LDPC codes based
	on flats of finite Euclidean geometries and array masking. Codes
	constructed based on these methods perform very well over the AWGN
	channel. With iterative decoding using a fast Fourier transform based
	sum-product algorithm, they achieve significantly large coding gains
	over Reed-Solomon codes of the same lengths and rates decoded with
	either algebraic hard-decision Berlekamp-Massey algorithm or algebraic
	soft-decision Kotter-Vardy algorithm. Due to their quasi-cyclic structure,
	these non-binary LDPC codes on Euclidean geometries can be encoded
	using simple shift-registers with linear complexity. Structured non-binary
	LDPC codes have a great potential to replace Reed-Solomon codes for
	some applications in either communication or storage systems for
	combating mixed types of noise and interferences.},
  doi = {10.1109/TCOMM.2009.05.070240},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\04939224.pdf:PDF},
  issn = {0090-6778},
  keywords = {AWGN channel;Reed-Solomon codes;algebraic method;algebraic soft-decision
	Kotter-Vardy algorithm;array masking;encodable nonbinary quasicyclic
	LDPC code;fast Fourier transform;finite Euclidean geometries;hard-decision
	Berlekamp-Massey algorithm;iterative decoding;linear complexity;sum-product
	algorithm;AWGN channels;Reed-Solomon codes;algebraic geometric codes;channel
	coding;cyclic codes;fast Fourier transforms;iterative decoding;parity
	check codes;},
  timestamp = {2011.12.06}
}

@BOOK{miyake,
  title = {入門代数学},
  publisher = {培風館},
  year = {1999},
  author = {三宅 敏恒}
}

@ARTICLE{三浦晋示:1999-08-25,
  author = {三浦 晋示 and 岩垂 好裕 and 今井 秀樹},
  title = {代数幾何符号の数理 (代数曲線とその応用論文小特集)},
  journal = {電子情報通信学会論文誌. A, 基礎・境界},
  year = {1999-08-25},
  volume = {82},
  pages = {1223-1238},
  number = {8},
  file = {:PDF\\代数幾何符号の数理.pdf:PDF},
  issn = {09135707},
  publisher = {社団法人電子情報通信学会},
  url = {http://ci.nii.ac.jp/naid/110003313391/}
}

@ARTICLE{nikkei_ele20090330,
  author = {今井 拓司},
  title = {HDD対フラッシュ　携帯機器を巡り競合から共存へ},
  journal = {日経エレクトロニクス},
  year = {2009},
  pages = {102-109},
  month = {March}
}

@BOOK{functions-th,
  title = {物理と関数論},
  publisher = {岩波書店},
  year = {1983},
  author = {今村 勤},
  timestamp = {2011.09.18}
}

@ARTICLE{How_to_use_SSD,
  author = {佐伯　真也 and 大石　基之},
  title = {どう付き合うかSSD},
  journal = {日経エレクトロニクス},
  year = {2009},
  pages = {29-51},
  month = {April}
}

@BOOK{uchida,
  title = {有限体と符号理論},
  publisher = {サイエンス社},
  year = {2000},
  author = {内田　興二},
  month = {January}
}

@ARTICLE{wadayama:2001-12-13,
  author = {和田山 正},
  title = {低密度パリティ検査符号とその復号法について},
  journal = {映像情報メディア学会技術報告},
  year = {2001-12-13},
  volume = {25},
  pages = {39-46},
  number = {81},
  issn = {13426893},
  publisher = {社団法人映像情報メディア学会},
  url = {http://ci.nii.ac.jp/naid/110003689689/}
}

@BOOK{wadayama-book,
  title = {低密度パリティ検査符号とその復号法},
  publisher = {トリケップス},
  year = {2002},
  author = {和田山　正},
  isbn = {978-4886572227},
  yomi = {Tadashi Wadayama}
}

@BOOK{978-4-339-02446-3,
  title = {代数系と符号理論入門},
  publisher = {コロナ社},
  year = {2010},
  author = {坂庭 好一 and 渋谷 智治},
  month = {April}
}

@BOOK{ogiso-AG2002,
  title = {代数曲線論},
  publisher = {朝倉書店},
  year = {2002},
  author = {小木曽 啓示},
  timestamp = {2012.06.11}
}

@ARTICLE{yamanishi_fermat_1989,
  author = {山西 健司},
  title = {Fermat符号の構成と性能について},
  journal = {電子情報通信学会論文誌 A 基礎・境界},
  year = {1989-03},
  volume = {72},
  pages = {p597-607},
  number = {3},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\yamanishi_fermat_1989.pdf:PDF},
  issn = {09135707},
  publisher = {電子情報通信学会基礎・境界ソサイエティ},
  timestamp = {2011.07.15},
  url = {http://ci.nii.ac.jp/naid/40004637388/}
}

@ARTICLE{yamanishi_modular_1988,
  author = {山西 健司},
  title = {2元Modular符号の新しい漸近的性能評価について},
  journal = {電子情報通信学会論文誌 A 基礎・境界},
  year = {1988-12},
  volume = {71},
  pages = {2172-2182},
  number = {12},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\yamanishi_modular_1988.pdf:PDF},
  issn = {09135707},
  publisher = {電子情報通信学会基礎・境界ソサイエティ},
  timestamp = {2011.07.15},
  url = {http://ci.nii.ac.jp/naid/40004637257/}
}

@ARTICLE{yamanishi_elliptic_1988,
  author = {山西 健司},
  title = {だ円符号および超だ円符号による高性能符号の導出について},
  journal = {電子情報通信学会論文誌 A 基礎・境界},
  year = {1988-10},
  volume = {71},
  pages = {1936-1946},
  number = {10},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\yamanishi_elliptic_1988.pdf:PDF},
  issn = {09135707},
  publisher = {電子情報通信学会基礎・境界ソサイエティ},
  timestamp = {2011.07.15},
  url = {http://ci.nii.ac.jp/naid/40004637293/}
}

@BOOKLET{hirasawa,
  title = {符号理論},
  author = {平澤 茂一},
  year = {2008}
}

@BOOK{mikoshiba1991,
  title = {半導体の物理[改訂版]},
  publisher = {培風館},
  year = {1991},
  author = {御子柴　宣夫},
  timestamp = {2011.09.22}
}

@BOOKLET{uematsu2007,
  title = {代数系と符号理論-講義ノート-東京工業大学OCW},
  author = {植松 友彦},
  year = {2007},
  file = {:PDF\\代数系と符号理論-東京工業大学OCW2007.pdf:PDF}
}

@BOOKLET{uematsu2004,
  title = {代数系と符号理論-講義ノート-東京工業大学OCW},
  author = {植松 友彦},
  year = {2004},
  file = {:PDF\\代数系と符号理論-東京工業大学OCW2004.pdf:PDF}
}

@BOOK{ECC_ohm,
  title = {誤り訂正符号とその応用},
  publisher = {オーム社},
  year = {1996},
  author = {横山 克哉ほか}
}

@INBOOK{statisticalcomputing,
  chapter = {平均場近似・EM法・変分ベイズ法},
  pages = {121-189},
  title = {計算統計 I-統計科学のフロンティア11},
  publisher = {岩波書店},
  year = {2003},
  author = {樺島 祥介 and 上田 修功}
}

@BOOK{asymptotic_analysis,
  title = {漸近解析},
  publisher = {岩波書店},
  year = {1995},
  author = {江沢 洋},
  timestamp = {2011.12.16}
}

@ARTICLE{j83-a_11_1309,
  author = {沼上　幸夫 and 藤澤　匡哉 and 阪田　省二郎},
  title = {Reed-Solomon符号のリスト復号のための高速補間法},
  journal = {電子情報通信学会論文誌 A},
  year = {2000},
  volume = {83-A},
  pages = {1309-1317},
  number = {11},
  file = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\j83-a_11_1309.pdf:PDF},
  timestamp = {2014.05.16}
}

@ARTICLE{watanabe2009-08-01,
  author = {渡辺 重佳},
  title = {半導体メモリの過去40年の歴史と将来展望(シリコン材料・デバイス,<特集>エレクトロニクスソサイエティ和文論文誌500号記念論文)},
  journal = {電子情報通信学会論文誌. C, エレクトロニクス},
  year = {2009-08-01},
  volume = {92},
  pages = {467-476},
  number = {8},
  file = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\watanabe2009-08-01.pdf:PDF},
  issn = {13452827},
  publisher = {社団法人電子情報通信学会},
  timestamp = {2011.09.14},
  url = {http://ci.nii.ac.jp/naid/110007359668/}
}

@BOOK{igi-kawai-qm-I,
  title = {量子力学I},
  publisher = {講談社},
  year = {1994},
  author = {猪木　慶治 and 川合　光},
  timestamp = {2011.09.18}
}

@BOOK{igi-kawai-qm-II,
  title = {量子力学II},
  publisher = {講談社},
  year = {1994},
  author = {猪木 慶治 and 川合　光},
  timestamp = {2011.09.18}
}

@BOOK{amari_inf_theory,
  title = {情報理論},
  publisher = {筑摩書房},
  year = {2011},
  author = {甘利 俊一},
  timestamp = {2012.04.04}
}

@BOOK{bayesiannetwork,
  title = {ベイジアンネットワークの統計推論の数理},
  publisher = {コロナ社},
  year = {2009},
  author = {田中　和之}
}

@BOOK{hazamaAG2005,
  title = {代数幾何学POD版},
  publisher = {森北出版},
  year = {2005},
  author = {硲　文夫},
  month = {December},
  timestamp = {2011.09.23}
}

@BOOK{learn_euler,
  title = {オイラーに学ぶ},
  publisher = {日本評論社},
  year = {2007},
  author = {野海　正俊},
  isbn = {978-4535784888},
  timestamp = {2011.12.16},
  url = {http://www.nippyo.co.jp/book/3187.html}
}

@ARTICLE{2level-ECC,
  author = {金子 晴彦 and 松坂 拓哉 and 藤原 英一},
  title = {フラッシュメモリを用いた大容量SSDのための2段階誤り制御符号},
  journal = {第7回情報科学技術フォーラム},
  year = {2008},
  pages = {59-62},
  month = {aug}
}

@BOOK{MC_in_Condensed_Matter,
  title = {The Monte Carlo Method in Condensed Matter Physics},
  publisher = {Springer Verlag},
  year = {1995},
  editor = {Kurt Binder},
  abstract = {Alongside experimental and theoretical work, computer simulation now
	forms one of the major tools of research in physics. The Monte Carlo
	method is the most important simulation method in the area of condensed
	matter physics. This book, written by foremost experts in the field,
	describes the state of the art of simulation methods in solid state
	physics. It also reviews selected applications in areas of particular
	current interest like simulations of growth processes far from equilibrium,
	interfacial phenomena, quantum and classical fluids, polymers, quantum
	problems on lattices, and random systems. A new chapter on recent
	developments in the Monte Carlo simulation of condensed matter has
	been attached.},
  isbn = {978-3-540-60174-6},
  timestamp = {2012.03.22},
  url = {http://www.springer.com/physics/theoretical%2C+mathematical+%26+computational+physics/book/978-3-540-60174-6}
}

@BOOK{adv_in_comp_sim,
  title = {Advances in Computer Simulation},
  publisher = {Springer Verlag},
  year = {1996},
  editor = {Janos Kertesz and Imre Kondor},
  month = {July},
  abstract = {Computer simulation has become a basic tool in many branches of physics
	such as statistical physics, particle physics, or materials science.
	The application of efficient algorithms is at least as important
	as good hardware in large-scale computation. This volume contains
	didactic lectures on such techniques based on physical insight. The
	emphasis is on Monte Carlo methods (introduction, cluster algorithms,
	reweighting and multihistogram techniques, umbrella sampling), efficient
	data analysis and optimization methods, but aspects of supercomputing,
	the solution of stochastic differential equations, and molecular
	dynamics are also discussed. The book addresses graduate students
	and researchers in theoretical and computational physics.},
  isbn = {978-3-540-63942-8},
  keywords = {Monte Carlo methods - computer simulation - molecular dynamics - optimization
	- stochastic differential equations},
  timestamp = {2012.03.22},
  url = {http://www.springer.com/physics/theoretical%2C+mathematical+%26+computational+physics/book/978-3-540-63942-8}
}

